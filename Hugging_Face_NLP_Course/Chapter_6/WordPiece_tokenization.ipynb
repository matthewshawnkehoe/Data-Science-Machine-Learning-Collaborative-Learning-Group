{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITWg_xOckyQB"
      },
      "source": [
        "# WordPiece tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t22FGV90kyQC"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZorN5RHJkyQE"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also log into Hugging Face."
      ],
      "metadata": {
        "id": "Q3-nu5gBlf-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "e4khjGdflgyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4ec11ec7f4844eb7b9c4402438ce820b",
            "c879e85f66c747249c3b7c5a41ece97e",
            "49fd61cfd2ac4ceabe63ab4f48fea685",
            "0eecbd7de63749a5bc0e85e54d56a3c8",
            "67b3913106b749ab8a78ddf9575fe6fe",
            "331bfa62d8bf40788c6e245902a702ba",
            "056f813a29f445c28342523fc0783cf0",
            "6136c045aa4e4516a890d4bae4a95550",
            "373124618a834b408aff776f20765b8a",
            "d958b75933764e66a44d46c425817854",
            "b54cd73f315d4b199581c10698a554c0",
            "a35bbb9f3f6a4c76a1a79e70822f093f",
            "cd85c72ff13240ed9de99c74a6d2faa7",
            "ebd66f74d88140768938ecfb03a3b789",
            "2ce73d4f38934f0bbe8d62ce9f1e5f90",
            "877eaf845b48403ca6511d34d45e8e11",
            "fa905472d45c40f48c5cf2e67d62dd62",
            "7c6ef4d051734172b0306ab83bcb5324",
            "4885d089e6a04782977c30f14773a9ee",
            "24cac8948af14eb1b1c16f0b8b1d4693"
          ]
        },
        "outputId": "d9538898-59fb-4137-8858-42d640dbc889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ec11ec7f4844eb7b9c4402438ce820b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPiece is the <font color='blue'>tokenization algorithm</font> Google developed to <font color='blue'>pretrain BERT</font>. It has since been reused in quite a few <font color='blue'>Transformer models</font> based on BERT, such as <font color='blue'>DistilBERT</font>, <font color='blue'>MobileBERT</font>, <font color='blue'>Funnel Transformers</font>, and <font color='blue'>MPNET</font>. It's very similar to BPE in terms of the training, but the actual <font color='blue'>tokenization</font> is <font color='blue'>done differently</font>.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "üí° This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "Sd5ARm5SlUeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training algorithm\n",
        "\n",
        "<Tip warning={true}>\n",
        "\n",
        "‚ö†Ô∏è Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Like BPE, WordPiece starts from a <font color='blue'>small vocabulary</font> including the <font color='blue'>special tokens</font> used by the <font color='blue'>model</font> and the <font color='blue'>initial alphabet</font>. Since it identifies <font color='blue'>subwords</font> by <font color='blue'>adding</font> a <font color='blue'>prefix</font> (like `##` for BERT), each <font color='blue'>word</font> is initially <font color='blue'>split</font> by <font color='blue'>adding that prefix</font> to <font color='blue'>all</font> the <font color='blue'>characters</font> inside the <font color='blue'>word</font>. So, for instance, `\"word\"` gets split like this:\n",
        "\n",
        "```\n",
        "w ##o ##r ##d\n",
        "```\n",
        "\n",
        "Thus, the <font color='blue'>initial alphabet</font> contains <font color='blue'>all</font> the <font color='blue'>characters</font> present at the <font color='blue'>beginning</font> of a word and the characters present <font color='blue'>inside a word</font> preceded by the WordPiece prefix.\n",
        "\n",
        "Then, again like BPE, WordPiece learns <font color='blue'>merge rules</font>. The <font color='blue'>main difference</font> is the <font color='blue'>way</font> the <font color='blue'>pair to be merged</font> is <font color='blue'>selected</font>. Instead of selecting the most frequent pair, WordPiece <font color='blue'>computes a score</font> for <font color='blue'>each pair</font>, using the following formula:\n",
        "\n",
        "$$\\mathrm{score} = \\frac{\\mathrm{freq\\_of\\_pair}}{ \\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element}}$$\n",
        "\n",
        "By <font color='blue'>dividing</font> the <font color='blue'>frequency of the pair</font> by the <font color='blue'>product of the frequencies of each of its parts</font>, the algorithm <font color='blue'>prioritizes</font> the merging of pairs where the <font color='blue'>individual parts are less frequent</font> in the vocabulary. For instance, it <font color='blue'>won't</font> necessarily <font color='blue'>merge `(\"un\", \"##able\")`</font> even if that pair <font color='blue'>occurs very frequently</font> in the <font color='blue'>vocabulary</font>, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like <font color='blue'>`(\"hu\", \"##gging\")`</font> will probably be <font color='blue'>merged faster</font> (assuming the word \"hugging\" appears often in the vocabulary) since <font color='blue'>`\"hu\"`</font> and <font color='blue'>`\"##gging\"`</font> are likely to be <font color='blue'>less frequent</font> individually.\n",
        "\n",
        "Let's look at the same vocabulary we used in the BPE training example:\n",
        "\n",
        "```\n",
        "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
        "```\n",
        "\n",
        "The splits here will be:\n",
        "\n",
        "```\n",
        "(\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
        "```\n",
        "\n",
        "so the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget about special tokens for now). The most frequent pair is <font color='blue'>`(\"##u\", \"##g\")`</font> (present <font color='blue'>20</font> times), but the individual frequency of `\"##u\"` is very high, so its score is not the highest (it's <font color='blue'>1 / 36</font>). All pairs with a <font color='blue'>`\"##u\"`</font> actually have that <font color='blue'>same score (1 / 36)</font>, so the best score goes to the pair <font color='blue'>`(\"##g\", \"##s\")`</font> -- the only one without a `\"##u\"` -- at <font color='blue'>1 / 20</font>, and the <font color='blue'>first merge learned</font> is <font color='blue'>`(\"##g\", \"##s\") -> (\"##gs\")</font>`.\n",
        "\n",
        "Note that when we <font color='blue'>merge</font>, we <font color='blue'>remove</font> the <font color='blue'>`##` between</font> the <font color='blue'>two tokens</font>, so we <font color='blue'>add `\"##gs\"`</font> to the <font color='blue'>vocabulary</font> and <font color='blue'>apply the merge</font> in the words of the corpus:\n",
        "\n",
        "```\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
        "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
        "```\n",
        "\n",
        "At this point, <font color='blue'>`\"##u\"`</font> is in <font color='blue'>all</font> the <font color='blue'>possible pairs</font>, so they <font color='blue'>all</font> end up with the <font color='blue'>same score</font>. Let's say that in this case, the first pair is merged, so <font color='blue'>`(\"h\", \"##u\") -> \"hu\"`</font>. This takes us to:\n",
        "\n",
        "```\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
        "Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
        "```\n",
        "\n",
        "Then the next best score is shared by <font color='blue'>`(\"hu\", \"##g\")` and `(\"hu\", \"##gs\")`</font> (with <font color='blue'>1/15</font>, compared to <font color='blue'>1/21</font> for all the other pairs), so the <font color='blue'>first pair</font> with the <font color='blue'>biggest score</font> is <font color='blue'>merged</font>:\n",
        "\n",
        "```\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\n",
        "Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
        "```\n",
        "\n",
        "and we continue like this until we reach the desired vocabulary size.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "‚úèÔ∏è **Now your turn!** What will the next merge rule be?\n",
        "\n",
        "</Tip>\n",
        "\n",
        "The next merge rule will be <font color='blue'>(\"hu\", \"##gs\") -> \"hugs\"</font>.\n",
        "Among the remaining pairs, (\"hu\", \"##gs\") has the highest score of 1/15, compared to other pairs like (\"p\", \"##u\"), (\"##u\", \"##g\"), and (\"##u\", \"##n\") which all have lower scores of 1/21 due to the high individual frequency of \"##u\". The corpus becomes:\n",
        "\n",
        "```\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\", \"hugs\"]\n",
        "Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hugs\", 5)\n",
        "```"
      ],
      "metadata": {
        "id": "fF0ClJRElq0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization algorithm\n",
        "Tokenization differs in WordPiece and BPE in that <font color='blue'>WordPiece</font> only <font color='blue'>saves</font> the <font color='blue'>final vocabulary</font>, <font color='blue'>not</font> the <font color='blue'>merge rules</font> learned. Starting from the word to tokenize, <font color='blue'>WordPiece</font> finds the <font color='blue'>longest subword</font> that is <font color='blue'>in</font> the <font color='blue'>vocabulary</font>, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word <font color='blue'>`\"hugs\"`</font> the <font color='blue'>longest subword</font> starting from the beginning that is inside the vocabulary is <font color='blue'>`\"hug\"`</font>, so we <font color='blue'>split there</font> and get <font color='blue'>`[\"hug\", \"##s\"]`</font>. We then <font color='blue'>continue</font> with <font color='blue'> `\"##s\"`</font>, which is in the vocabulary, so the <font color='blue'>tokenization</font> of <font color='blue'>`\"hugs\"`</font> is <font color='blue'>`[\"hug\", \"##s\"]`</font>.\n",
        "\n",
        "With <font color='blue'>BPE</font>, we would have applied the merges learned in order and <font color='blue'>tokenized</font> this <font color='blue'>as `[\"hu\", \"##gs\"]`</font>, so the encoding is different.\n",
        "\n",
        "As <font color='blue'>another example</font>, let's see how the word <font color='blue'>`\"bugs\"`</font> would be <font color='blue'>tokenized</font>. <font color='blue'>`\"b\"`</font> is the <font color='blue'>longest subword</font> starting at the beginning of the word that is in the vocabulary, so we split there and get <font color='blue'>`[\"b\", \"##ugs\"]`</font>. Then <font color='blue'>`\"##u\"`</font> is the <font color='blue'>longest subword</font> starting at the <font color='blue'>beginning</font> of <font color='blue'>`\"##ugs\"`</font> that is <font color='blue'>in</font> the <font color='blue'>vocabulary</font>, so we split there and get <font color='blue'>`[\"b\", \"##u, \"##gs\"]`</font>. Finally, `\"##gs\"` is in the vocabulary, so this last list is the tokenization of `\"bugs\"`.\n",
        "\n",
        "When the tokenization gets to a stage where it's <font color='blue'>not possible</font> to find a <font color='blue'>subword</font> in the <font color='blue'>vocabulary</font>, the <font color='blue'>whole word</font> is <font color='blue'>tokenized</font> as <font color='blue'>unknown</font> -- so, for instance, <font color='blue'>`\"mug\"`</font> would be <font color='blue'>tokenized</font> as <font color='blue'>`[\"[UNK]\"]`</font>, as would <font color='blue'>`\"bum\"`</font> (even if we can begin with `\"b\"` and `\"##u\"`, `\"##m\"` is not the vocabulary, and the resulting tokenization will just be `[\"[UNK]\"]`, not `[\"b\", \"##u\", \"[UNK]\"]`). This is another difference from <font color='blue'>BPE</font>, which would <font color='blue'>only classify</font> the <font color='blue'>individual characters</font> not in the vocabulary as <font color='blue'>unknown</font>.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "‚úèÔ∏è **Now your turn!** How will the word `\"pugs\"` be tokenized?\n",
        "\n",
        "</Tip>\n",
        "\n",
        "For the word <font color='blue'>\"pugs\"</font>, we start by finding the <font color='blue'>longest subword</font> from the <font color='blue'>beginning</font> that is in the vocabulary. <font color='blue'>\"p\"</font> is the longest subword starting at the beginning of the word that is in the vocabulary, so we <font color='blue'>split there</font> and get <font color='blue'>[\"p\", \"##ugs\"]</font>. Then <font color='blue'>\"##u\"</font> is the <font color='blue'>longest subword</font> starting at the <font color='blue'>beginning</font> of <font color='blue'>\"##ugs\"</font> that is in the vocabulary, so we <font color='blue'>split there</font> and get <font color='blue'>[\"p\", \"##u\", \"##gs\"]</font>. Then, since <font color='blue'>\"##gs\"</font> is <font color='blue'>in</font> the <font color='blue'>vocabulary</font>, the tokenization of \"pugs\" is [\"p\", \"##u\", \"##gs\"]."
      ],
      "metadata": {
        "id": "9AFfwvi0mOMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing WordPiece\n",
        "\n",
        "Now let's take a look at an <font color='blue'>implementation</font> of the <font color='blue'>WordPiece algorithm</font>. Like with BPE, this is just pedagogical, and you won't able to use this on a big corpus.\n",
        "\n",
        "We will use the same corpus as in the BPE example:"
      ],
      "metadata": {
        "id": "sLoPdYuwmZps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBclYJwskyQF"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'></font>, we need to <font color='blue'>pre-tokenize</font> the <font color='blue'>corpus</font> into <font color='blue'>words</font>. Since we are replicating a WordPiece tokenizer (like BERT), we will use the `bert-base-cased` tokenizer for the pre-tokenization:"
      ],
      "metadata": {
        "id": "q9Ki0wJymd5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLIDWUcakyQG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we <font color='blue'>compute</font> the <font color='blue'>frequencies</font> of <font color='blue'>each word</font> in the corpus as we do the pre-tokenization:"
      ],
      "metadata": {
        "id": "11hud-JTmf2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFom1plIkyQH",
        "outputId": "7a6d9b7b-dd94-4c10-d3ad-d82f625747fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'This': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'Hugging': 1,\n",
              "             'Face': 1,\n",
              "             'Course': 1,\n",
              "             '.': 4,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'Hopefully': 1,\n",
              "             ',': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw before, the <font color='blue'>alphabet</font> is the <font color='blue'>unique set</font> composed of all the <font color='blue'>first letters</font> of <font color='blue'>words</font>, and <font color='blue'>all</font> the <font color='blue'>other letters</font> that appear in words <font color='blue'>prefixed</font> by <font color='blue'>`##`</font>:"
      ],
      "metadata": {
        "id": "8KpPYxaamkUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlObXhcSkyQI",
        "outputId": "a1806072-f0e2-4179-fa6a-ead4936c92e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['##a',\n",
            " '##b',\n",
            " '##c',\n",
            " '##d',\n",
            " '##e',\n",
            " '##f',\n",
            " '##g',\n",
            " '##h',\n",
            " '##i',\n",
            " '##k',\n",
            " '##l',\n",
            " '##m',\n",
            " '##n',\n",
            " '##o',\n",
            " '##p',\n",
            " '##r',\n",
            " '##s',\n",
            " '##t',\n",
            " '##u',\n",
            " '##v',\n",
            " '##w',\n",
            " '##y',\n",
            " '##z',\n",
            " ',',\n",
            " '.',\n",
            " 'C',\n",
            " 'F',\n",
            " 'H',\n",
            " 'T',\n",
            " 'a',\n",
            " 'b',\n",
            " 'c',\n",
            " 'g',\n",
            " 'h',\n",
            " 'i',\n",
            " 's',\n",
            " 't',\n",
            " 'u',\n",
            " 'w',\n",
            " 'y']\n"
          ]
        }
      ],
      "source": [
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "alphabet\n",
        "\n",
        "print('[' + ',\\n '.join(f\"'{element}'\" for element in alphabet) + ']')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also add the <font color='blue'>special tokens</font> used by the model at the <font color='blue'>beginning</font> of that <font color='blue'>vocabulary</font>. In the case of BERT, it's the list `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`:\n"
      ],
      "metadata": {
        "id": "nLGK9elkmncw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82n8aXhakyQI"
      },
      "outputs": [],
      "source": [
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to <font color='blue'>split</font> each <font color='blue'>word</font>, with all the <font color='blue'>letters</font> that are <font color='blue'>not</font> the <font color='blue'>first prefixed</font> by <font color='blue'>`##`</font>:"
      ],
      "metadata": {
        "id": "CoNj3U36mrLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhAb1fekyQK"
      },
      "outputs": [],
      "source": [
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are ready for training, let's write a <font color='blue'>function</font> that <font color='blue'>computes</font> the <font color='blue'>score</font> of <font color='blue'>each pair</font>. We'll need to use this at each step of the training:"
      ],
      "metadata": {
        "id": "0Y0WCbc1mvDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cykxmF5UkyQM"
      },
      "outputs": [],
      "source": [
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at a <font color='blue'>part</font> of <font color='blue'>this dictionary</font> after the initial splits:"
      ],
      "metadata": {
        "id": "XdnQseNlmxjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EIOwBIqkyQN",
        "outputId": "d60d3377-d9d4-4dc6-bedb-a2c390fab1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h'): 0.125\n",
            "('##h', '##i'): 0.03409090909090909\n",
            "('##i', '##s'): 0.02727272727272727\n",
            "('i', '##s'): 0.1\n",
            "('t', '##h'): 0.03571428571428571\n",
            "('##h', '##e'): 0.011904761904761904\n"
          ]
        }
      ],
      "source": [
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, finding the <font color='blue'>pair</font> with the <font color='blue'>best score</font> only takes a quick <font color='blue'>loop</font>:"
      ],
      "metadata": {
        "id": "m1YinFXgm0Kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOw4PNECkyQP",
        "outputId": "751336fa-0507-4c5a-b7c5-ac03a05d81ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', '##b') 0.2\n"
          ]
        }
      ],
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "print(best_pair, max_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the <font color='blue'>first merge</font> to learn is <font color='blue'>`('a', '##b') -> 'ab'`</font>, and we add `'ab'` to the vocabulary:\n"
      ],
      "metadata": {
        "id": "tZgUZr6bm2iQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5EvLx-MkyQQ"
      },
      "outputs": [],
      "source": [
        "vocab.append(\"ab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To continue, we need to apply that <font color='blue'>merge</font> in our <font color='blue'>`splits` dictionary</font>. Let's write another function for this:"
      ],
      "metadata": {
        "id": "jx_8LWE0m5TR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySEtQHeukyQR"
      },
      "outputs": [],
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can have a look at the <font color='blue'>result</font> of the <font color='blue'>first merge</font>:"
      ],
      "metadata": {
        "id": "3GdcCt37m9Ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLjEfTw2kyQR",
        "outputId": "e2cc5126-57f4-4e63-95a2-359ec697a6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything we need to <font color='blue'>loop</font> until we have <font color='blue'>learned all</font> the <font color='blue'>merges</font> we want. Let's aim for a vocab size of <font color='blue'>70</font>:"
      ],
      "metadata": {
        "id": "WW6l8Dy_m-aO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViozpKNkkyQR"
      },
      "outputs": [],
      "source": [
        "vocab_size = 70\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = (\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    vocab.append(new_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can then look at the generated vocabulary:"
      ],
      "metadata": {
        "id": "Z8ccnUmAnCmF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi2jIDB1kyQS",
        "outputId": "dd4a0f64-bb99-4c75-f47c-dac2fe50925e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]',\n",
            " '[UNK]',\n",
            " '[CLS]',\n",
            " '[SEP]',\n",
            " '[MASK]',\n",
            " '##a',\n",
            " '##b',\n",
            " '##c',\n",
            " '##d',\n",
            " '##e',\n",
            " '##f',\n",
            " '##g',\n",
            " '##h',\n",
            " '##i',\n",
            " '##k',\n",
            " '##l',\n",
            " '##m',\n",
            " '##n',\n",
            " '##o',\n",
            " '##p',\n",
            " '##r',\n",
            " '##s',\n",
            " '##t',\n",
            " '##u',\n",
            " '##v',\n",
            " '##w',\n",
            " '##y',\n",
            " '##z',\n",
            " ',',\n",
            " '.',\n",
            " 'C',\n",
            " 'F',\n",
            " 'H',\n",
            " 'T',\n",
            " 'a',\n",
            " 'b',\n",
            " 'c',\n",
            " 'g',\n",
            " 'h',\n",
            " 'i',\n",
            " 's',\n",
            " 't',\n",
            " 'u',\n",
            " 'w',\n",
            " 'y',\n",
            " 'ab',\n",
            " '##fu',\n",
            " 'Fa',\n",
            " 'Fac',\n",
            " '##ct',\n",
            " '##ful',\n",
            " '##full',\n",
            " '##fully',\n",
            " 'Th',\n",
            " 'ch',\n",
            " '##hm',\n",
            " 'cha',\n",
            " 'chap',\n",
            " 'chapt',\n",
            " '##thm',\n",
            " 'Hu',\n",
            " 'Hug',\n",
            " 'Hugg',\n",
            " 'sh',\n",
            " 'th',\n",
            " 'is',\n",
            " '##thms',\n",
            " '##za',\n",
            " '##zat',\n",
            " '##ut']\n"
          ]
        }
      ],
      "source": [
        "print('[' + ',\\n '.join(f\"'{element}'\" for element in vocab) + ']')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, compared to BPE, this tokenizer learns <font color='blue'>parts of words</font> as <font color='blue'>tokens</font> a bit <font color='blue'>faster</font>.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "üí° Using <font color='blue'>`train_new_from_iterator()`</font> on the <font color='blue'>same corpus won't result</font> in the exact <font color='blue'>same vocabulary</font>. This is because the ü§ó Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "To <font color='blue'>tokenize</font> a <font color='blue'>new text</font>, we <font color='blue'>pre-tokenize</font> it, <font color='blue'>split</font> it, then <font color='blue'>apply</font> the <font color='blue'>tokenization algorithm</font> on <font color='blue'>each word</font>. That is, we look for the <font color='blue'>biggest subword starting</font> at the <font color='blue'>beginning</font> of the <font color='blue'>first word</font> and <font color='blue'>split it</font>, then we <font color='blue'>repeat the process</font> on the second part, and so on for the <font color='blue'>rest</font> of <font color='blue'>that word and</font> the <font color='blue'>following words</font> in the text:\n"
      ],
      "metadata": {
        "id": "80R1jpWpnDsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekUwkaEKkyQT"
      },
      "outputs": [],
      "source": [
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it on <font color='blue'>one word</font> that's in the <font color='blue'>vocabulary</font>, and <font color='blue'>another</font> that <font color='blue'>isn't</font>:"
      ],
      "metadata": {
        "id": "2fjGjRilnIgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92wJnZIlkyQT",
        "outputId": "53134f57-4785-438e-cefb-6725e2882c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hugg', '##i', '##n', '##g']\n",
            "['[UNK]']\n"
          ]
        }
      ],
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write a <font color='blue'>function</font> that <font color='blue'>tokenizes</font> a <font color='blue'>text</font>:"
      ],
      "metadata": {
        "id": "omWUjVTXnMrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Iey5jsLkyQU"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try it on <font color='blue'>any text</font>:"
      ],
      "metadata": {
        "id": "RVuaczUdnP6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUDTDjJUkyQU",
        "outputId": "99a93794-da21-4cd7-b715-0c6a88d2f394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Th',\n",
              " '##i',\n",
              " '##s',\n",
              " 'is',\n",
              " 'th',\n",
              " '##e',\n",
              " 'Hugg',\n",
              " '##i',\n",
              " '##n',\n",
              " '##g',\n",
              " 'Fac',\n",
              " '##e',\n",
              " 'c',\n",
              " '##o',\n",
              " '##u',\n",
              " '##r',\n",
              " '##s',\n",
              " '##e',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it for the WordPiece algorithm! Now let's take a look at Unigram."
      ],
      "metadata": {
        "id": "VPhw9_OanQ8Z"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ec11ec7f4844eb7b9c4402438ce820b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_056f813a29f445c28342523fc0783cf0"
          }
        },
        "c879e85f66c747249c3b7c5a41ece97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6136c045aa4e4516a890d4bae4a95550",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_373124618a834b408aff776f20765b8a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "49fd61cfd2ac4ceabe63ab4f48fea685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d958b75933764e66a44d46c425817854",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b54cd73f315d4b199581c10698a554c0",
            "value": ""
          }
        },
        "0eecbd7de63749a5bc0e85e54d56a3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a35bbb9f3f6a4c76a1a79e70822f093f",
            "style": "IPY_MODEL_cd85c72ff13240ed9de99c74a6d2faa7",
            "value": true
          }
        },
        "67b3913106b749ab8a78ddf9575fe6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ebd66f74d88140768938ecfb03a3b789",
            "style": "IPY_MODEL_2ce73d4f38934f0bbe8d62ce9f1e5f90",
            "tooltip": ""
          }
        },
        "331bfa62d8bf40788c6e245902a702ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_877eaf845b48403ca6511d34d45e8e11",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fa905472d45c40f48c5cf2e67d62dd62",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "056f813a29f445c28342523fc0783cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6136c045aa4e4516a890d4bae4a95550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373124618a834b408aff776f20765b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d958b75933764e66a44d46c425817854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b54cd73f315d4b199581c10698a554c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a35bbb9f3f6a4c76a1a79e70822f093f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd85c72ff13240ed9de99c74a6d2faa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebd66f74d88140768938ecfb03a3b789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ce73d4f38934f0bbe8d62ce9f1e5f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "877eaf845b48403ca6511d34d45e8e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa905472d45c40f48c5cf2e67d62dd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6ef4d051734172b0306ab83bcb5324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4885d089e6a04782977c30f14773a9ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24cac8948af14eb1b1c16f0b8b1d4693",
            "value": "Connecting..."
          }
        },
        "4885d089e6a04782977c30f14773a9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24cac8948af14eb1b1c16f0b8b1d4693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}