{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woHSyMAEIxXY"
      },
      "source": [
        "# Fast tokenizers in the QA pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_efbKwPIxXa"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RLBQ800MIxXb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, log into Hugging face."
      ],
      "metadata": {
        "id": "UrSr_8VgJDfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "dfabfa3ee4374be793f1ac04dd87d30e",
            "a2486a5fba2944979913e204540f7fe4",
            "a8e2165ca77a4a5682718f3b851c2713",
            "ea277f91ec9f4d429932474ee5860ce5",
            "fd1fe7d68f614b2b847be08f73c15f4d",
            "269ecf76e96c44b09b34727208c0e780",
            "e5d9f4c7bdeb4c5fad3d97d92f93e9db",
            "9de488e58d404735812ebfc71be9d4df",
            "ca968987173c4987b8143061d8f8c671",
            "36234548c73e4398a206c48c54bf4cc2",
            "6764de3985fe4ac58ce680d02d775e0a",
            "68f823098e6f414b91c680237a27f28e",
            "14489a68beb443eba393140fa6fecf7c",
            "ad75c7577436430784df90af0fe6b3fb",
            "ee28f3c1ede84e5b87d02f032d319aa7",
            "0ae28d3f65954e67bd8b8d56ff531c9a",
            "2f9af6fbba024f3b83eb7e5010b1d5f3",
            "61e4dcf8c2a248dfb698e7a74fb43820",
            "4490fa92bb8e431d8a08eaa3870a8526",
            "3f95b47997994b1199954869cf4a7938"
          ]
        },
        "id": "LA9is9gJJHAz",
        "outputId": "923e9446-89c4-41a1-9195-dda4be60fbdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfabfa3ee4374be793f1ac04dd87d30e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now dive into the <font color='blue'>question-answering pipeline</font> and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the <font color='blue'>grouped entities</font> in the previous section. Then we will see how we can deal with <font color='blue'>very long contexts</font> that end up being <font color='blue'>truncated</font>. You can skip this section if you're not interested in the question answering task.\n"
      ],
      "metadata": {
        "id": "m7qXjBudJJLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the `question-answering` pipeline\n",
        "\n",
        "As we saw in [Chapter 1](https://huggingface.co/learn/llm-course/chapter1/3), we can use the <font color='blue'>question-answering pipeline</font> like this to get the <font color='blue'>answer</font> to a <font color='blue'>question</font>:"
      ],
      "metadata": {
        "id": "bJ5RTMUNJmMm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dj6OEtlIxXc",
        "outputId": "cef0d17d-645a-4103-a75d-97d1fd887fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.98026043176651,\n",
              " 'start': 78,\n",
              " 'end': 106,\n",
              " 'answer': 'Jax, PyTorch, and TensorFlow'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "context = \"\"\"\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the <font color='blue'>other pipelines</font>, which <font color='blue'>can't truncate</font> and <font color='blue'>split</font> texts that are <font color='blue'>longer</font> than the <font color='blue'>maximum length</font> accepted by the model (and thus may <font color='blue'>miss information</font> at the <font color='blue'>end</font> of a <font color='blue'>document</font>), this <font color='blue'>pipeline</font> can deal with <font color='blue'>very long contexts</font> and will <font color='blue'>return</font> the <font color='blue'>answer</font> to the <font color='blue'>question</font> even if it's at the end:\n"
      ],
      "metadata": {
        "id": "XffmRt5EJz9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlyDFu3NIxXd",
        "outputId": "95a2fbb0-90b3-4558-9574-8daa4596b40b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9714871048927307,\n",
              " 'start': 1892,\n",
              " 'end': 1919,\n",
              " 'answer': 'Jax, PyTorch and TensorFlow'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "long_context = \"\"\"\n",
        "ü§ó Transformers: State of the Art NLP\n",
        "\n",
        "ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
        "question answering, summarization, translation, text generation and more in over 100 languages.\n",
        "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
        "\n",
        "ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
        "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
        "can be modified to enable quick research experiments.\n",
        "\n",
        "Why should I use transformers?\n",
        "\n",
        "1. Easy-to-use state-of-the-art models:\n",
        "  - High performance on NLU and NLG tasks.\n",
        "  - Low barrier to entry for educators and practitioners.\n",
        "  - Few user-facing abstractions with just three classes to learn.\n",
        "  - A unified API for using all our pretrained models.\n",
        "  - Lower compute costs, smaller carbon footprint:\n",
        "\n",
        "2. Researchers can share trained models instead of always retraining.\n",
        "  - Practitioners can reduce compute time and production costs.\n",
        "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
        "\n",
        "3. Choose the right framework for every part of a model's lifetime:\n",
        "  - Train state-of-the-art models in 3 lines of code.\n",
        "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
        "  - Seamlessly pick the right framework for training, evaluation and production.\n",
        "\n",
        "4. Easily customize a model or an example to your needs:\n",
        "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
        "  - Model internals are exposed as consistently as possible.\n",
        "  - Model files can be used independently of the library for quick experiments.\n",
        "\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question_answerer(question=question, context=long_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it does all of this!"
      ],
      "metadata": {
        "id": "1CGBakWIKcem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a model for question answering\n",
        "\n",
        "Like with any other pipeline, we start by <font color='blue'>tokenizing our input</font> and then <font color='blue'>send</font> it through the <font color='blue'>model</font>. The <font color='blue'>checkpoint</font> used by <font color='blue'>default</font> for the `question-answering` pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) where the <font color='blue'>squad</font> in the <font color='blue'>name</font> comes from the <font color='blue'>dataset</font> on which the <font color='blue'>model</font> was <font color='blue'>fine-tuned</font>; we'll talk more about the SQuAD dataset in [Chapter 7](https://huggingface.co/learn/llm-course/chapter7/7):"
      ],
      "metadata": {
        "id": "JIzyZSqFKdYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X6cydrUIIxXe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we <font color='blue'>tokenize</font> the <font color='blue'>question</font> and the <font color='blue'>context</font> as a <font color='blue'>pair</font>, with the <font color='blue'>question first</font>."
      ],
      "metadata": {
        "id": "hkZkH4phKur0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg)"
      ],
      "metadata": {
        "id": "Mgg_TWfTKvyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models for question answering work a little differently from the models we've seen up to now. Using the <font color='blue'>picture</font> above as an <font color='blue'>example</font>, the model has been <font color='blue'>trained</font> to <font color='blue'>predict</font> the <font color='blue'>index</font> of the <font color='blue'>token starting</font> the <font color='blue'>answer</font> (here <font color='blue'>21</font>) and the <font color='blue'>index</font> of the <font color='blue'>token</font> where the <font color='blue'>answer ends</font> (here <font color='blue'>24</font>). This is why those models don't return one tensor of logits but <font color='blue'>two</font>: <font color='blue'>one</font> for the <font color='blue'>logits</font> corresponding to the <font color='blue'>start token</font> of the <font color='blue'>answer</font>, and <font color='blue'>one</font> for the <font color='blue'>logits</font> corresponding to the <font color='blue'>end token</font> of the <font color='blue'>answer</font>. Since in this case we have only one input containing <font color='blue'>67 tokens</font>, we get:\n"
      ],
      "metadata": {
        "id": "9kvfcAhfK8H0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwtmRh8qIxXf",
        "outputId": "69f7a0d9-fa6c-4f4f-bb8e-1788859806f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 67]) torch.Size([1, 67])\n"
          ]
        }
      ],
      "source": [
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert those <font color='blue'>logits</font> into <font color='blue'>probabilities</font>, we will apply a <font color='blue'>softmax function</font> -- but before that, we need to make sure we <font color='blue'>mask</font> the <font color='blue'>indices</font> that are <font color='blue'>not part</font> of the <font color='blue'>context</font>. Our input is `[CLS] question [SEP] context [SEP]`, so we need to mask the tokens of the <font color='blue'>question</font> as well as the <font color='blue'>`[SEP]` token</font>. We'll <font color='blue'>keep</font> the <font color='blue'>`[CLS]` token</font>, however, as some models use it to <font color='blue'>indicate</font> that the <font color='blue'>answer</font> is <font color='blue'>not in the context</font>.\n",
        "\n",
        "Since we will apply a <font color='blue'>softmax afterward</font>, we just need to <font color='blue'>replace</font> the <font color='blue'>logits</font> we want to <font color='blue'>mask</font> with a <font color='blue'>large negative number</font>. Here, we use `-10000`:"
      ],
      "metadata": {
        "id": "CTSCQ14PK_6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1yjrgjXFIxXf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "mask = torch.tensor(mask)[None]\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have properly <font color='blue'>masked</font> the <font color='blue'>logits</font> corresponding to <font color='blue'>positions</font> we <font color='blue'>don't want</font> to <font color='blue'>predict</font>, we can apply the <font color='blue'>softmax</font>:"
      ],
      "metadata": {
        "id": "_GOKH0T1LChy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zlKhSVQuIxXg"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we could take the <font color='blue'>argmax</font> of the <font color='blue'>start</font> and <font color='blue'>end probabilities</font> -- but we <font color='blue'>might end up</font> with a <font color='blue'>start index</font> that is <font color='blue'>greater</font> than the <font color='blue'>end index</font>, so we need to <font color='blue'>take</font> a few <font color='blue'>more precautions</font>. We will compute the <font color='blue'>probabilities</font> of each possible <font color='blue'>`start_index`</font> and <font color='blue'>`end_index`</font> where <font color='blue'>`start_index <= end_index`</font>, then take the <font color='blue'>tuple `(start_index, end_index)`</font> with the <font color='blue'>highest probability</font>.\n",
        "\n",
        "Assuming the <font color='blue'>events</font> \"The answer starts at `start_index`\" and \"The answer ends at `end_index`\" to be <font color='blue'>independent</font>, the <font color='blue'>probability</font> that the <font color='blue'>answer starts</font> at <font color='blue'>`start_index`</font> and <font color='blue'>ends</font> at <font color='blue'>`end_index`</font> is\n",
        "\n",
        "$$\\mathrm{start\\_probabilities}\\left[\\mathrm{start\\_index}\\right] \\times \\mathrm{end\\_probabilities}\\left[\\mathrm{end\\_index}\\right].$$\n",
        "\n",
        "So, to compute <font color='blue'>all the scores</font>, we just need to <font color='blue'>compute</font> all the products\n",
        "\n",
        "$$\\mathrm{start\\_probabilities}\\left[\\mathrm{start\\_index}\\right] \\times \\mathrm{end\\_probabilities}\\left[\\mathrm{end\\_index}\\right].$$\n",
        "\n",
        "where <font color='blue'>`start_index <= end_index`</font>. First let's compute <font color='blue'>all</font> the <font color='blue'>possible products</font>:"
      ],
      "metadata": {
        "id": "RdrBzGfiLH6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8cqNA-CMIxXh"
      },
      "outputs": [],
      "source": [
        "scores = start_probabilities[:, None] * end_probabilities[None, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we'll <font color='blue'>mask</font> the <font color='blue'>values</font> where <font color='blue'>`start_index > end_index`</font> by <font color='blue'>setting them</font> to <font color='blue'>`0`</font> (the other probabilities are all positive numbers). The <font color='blue'>`torch.triu()` function</font> returns the <font color='blue'>upper triangular part</font> of the <font color='blue'>2D tensor</font> passed as an argument, so it will <font color='blue'>do</font> that <font color='blue'>masking for us</font>:\n"
      ],
      "metadata": {
        "id": "RcJ6XlAJLNfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YuDqyHrIIxXh"
      },
      "outputs": [],
      "source": [
        "scores = torch.triu(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just have to <font color='blue'>get</font> the <font color='blue'>index</font> of the <font color='blue'>maximum</font>. Since <font color='blue'>PyTorch</font> will <font color='blue'>return</font> the <font color='blue'>index</font> in the <font color='blue'>flattened tensor</font>, we need to use the <font color='blue'>floor division `//`</font> and <font color='blue'>modulus `%`</font> operations to get the <font color='blue'>`start_index`</font> and <font color='blue'>`end_index`</font>:\n"
      ],
      "metadata": {
        "id": "nAuQCQ6bLOc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YI7oIrsIxXi",
        "outputId": "2baeca87-a32a-4dc0-efe9-1ecbd0e91d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 23, End: 35, Score: 0.980260\n"
          ]
        }
      ],
      "source": [
        "max_index = scores.argmax().item()\n",
        "start_index = max_index // scores.shape[1]\n",
        "end_index = max_index % scores.shape[1]\n",
        "print(f\"Start: {start_index}, End: {end_index}, Score: {scores[start_index, end_index].item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're <font color='blue'>not quite done yet</font>, but at least we already have the <font color='blue'>correct score</font> for the <font color='blue'>answer</font> (you can check this by comparing it to the first result in the previous section):"
      ],
      "metadata": {
        "id": "0GQ7WQVmLVbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Compute the start and end indices for the five most likely answers."
      ],
      "metadata": {
        "id": "pryfX0TGLoJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise - use torch.topk to returns the k largest elements of a given input tensor\n",
        "flat_scores = scores.flatten()\n",
        "top_5_flat_indices = torch.topk(flat_scores, k=5).indices\n",
        "start_indices = top_5_flat_indices // scores.shape[1]\n",
        "end_indices = top_5_flat_indices % scores.shape[1]\n",
        "top_5_scores = scores[start_indices, end_indices]\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. Start: {start_indices[i].item()}, End: {end_indices[i].item()}, Score: {top_5_scores[i].item():.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V2F7FsvWLiT",
        "outputId": "f1035ea1-e9ed-44d6-9e69-830d1dec5d14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Start: 23, End: 35, Score: 0.980260\n",
            "2. Start: 23, End: 36, Score: 0.008248\n",
            "3. Start: 16, End: 35, Score: 0.006841\n",
            "4. Start: 23, End: 29, Score: 0.001368\n",
            "5. Start: 25, End: 35, Score: 0.000381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the <font color='blue'>`start_index`</font> and <font color='blue'>`end_index`</font> of the <font color='blue'>answer</font> in terms of <font color='blue'>tokens</font>, so now we just need to <font color='blue'>convert to</font> the <font color='blue'>character indices</font> in the context. This is where the <font color='blue'>offsets</font> will be super <font color='blue'>useful</font>. We can grab them and use them like we did in the token classification task:\n"
      ],
      "metadata": {
        "id": "OaLUOq2VLrey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9FsSnY5EIxXi"
      },
      "outputs": [],
      "source": [
        "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "start_char, _ = offsets[start_index]\n",
        "_, end_char = offsets[end_index]\n",
        "answer = context[start_char:end_char]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just have to <font color='blue'>format everything</font> to <font color='blue'>get our result</font>:"
      ],
      "metadata": {
        "id": "2ojJAqe-Lybv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoEDjIhmIxXj",
        "outputId": "a16ff811-be77-4e85-b495-e1cd2a0c26fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jax, PyTorch, and TensorFlow\n",
            "Start: 78, End: 106, Score: 0.980260\n"
          ]
        }
      ],
      "source": [
        "result = {\n",
        "    \"answer\": answer,\n",
        "    \"start\": start_char,\n",
        "    \"end\": end_char,\n",
        "    \"score\": scores[start_index, end_index],\n",
        "}\n",
        "print(result['answer'])\n",
        "print(f\"Start: {result['start']}, End: {result['end']}, Score: {result['score'].item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! That's the <font color='blue'>same</font> as in our <font color='blue'>first example</font>!"
      ],
      "metadata": {
        "id": "cuOgwkldL6zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in `top_k=5` when calling it."
      ],
      "metadata": {
        "id": "9MtA_XiBL_jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise - use top_k=5 and extract answers\n",
        "offsets = tokenizer(question, context, return_offsets_mapping=True)[\"offset_mapping\"]\n",
        "top_5 = torch.topk(scores.flatten(), 5).indices\n",
        "\n",
        "for i, idx in enumerate(top_5, 1):\n",
        "    start_idx, end_idx = divmod(idx.item(), scores.shape[1])\n",
        "    start_char, end_char = offsets[start_idx][0], offsets[end_idx][1]\n",
        "    answer = context[start_char:end_char]\n",
        "    score = scores[start_idx, end_idx].item()\n",
        "    print(f\"{i}. '{answer}' (Score: {score:.6f})\")"
      ],
      "metadata": {
        "id": "EJ9INDY3bARn",
        "outputId": "f2706b0b-0943-4d18-acf8-d53aa3fa10a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 'Jax, PyTorch, and TensorFlow' (Score: 0.980260)\n",
            "2. 'Jax, PyTorch, and TensorFlow ‚Äî' (Score: 0.008248)\n",
            "3. 'three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow' (Score: 0.006841)\n",
            "4. 'Jax, PyTorch' (Score: 0.001368)\n",
            "5. 'PyTorch, and TensorFlow' (Score: 0.000381)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling long contexts\n",
        "\n",
        "If we try to <font color='blue'>tokenize</font> the <font color='blue'>question</font> and <font color='blue'>long context</font> we used as an <font color='blue'>example previously</font>, we'll get a <font color='blue'>number of tokens higher</font> than the <font color='blue'>maximum length</font> used in the <font color='blue'>question-answering pipeline</font> (which is <font color='blue'>384</font>):\n"
      ],
      "metadata": {
        "id": "i5amXUnqMDg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(question, long_context)\n",
        "print(f'The number of tokens is {len(inputs[\"input_ids\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MNR6p6XMUwj",
        "outputId": "43143eda-7324-49ef-d0dd-d7d30ce12d59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of tokens is 461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we'll need to <font color='blue'>truncate</font> our <font color='blue'>inputs</font> at that <font color='blue'>maximum length</font>. There are several ways we can do this, but we don't want to <font color='blue'>truncate</font> the question, only the <font color='blue'>context</font>. Since the <font color='blue'>context</font> is the <font color='blue'>second sentence</font>, we'll use the <font color='blue'>`\"only_second\"` truncation strategy.</font> The <font color='blue'>problem</font> that arises then is that the <font color='blue'>answer</font> to the question <font color='blue'>may not be in</font> the <font color='blue'>truncated context</font>. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:\n"
      ],
      "metadata": {
        "id": "2uxuoK9qMY7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml9_Ak5hIxXj",
        "outputId": "ca341c37-9211-4378-b017-6da8303b69c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] \n",
            "\n",
            "[UNK] Transformers: State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone.\n",
            "\n",
            "[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n",
            "\n",
            "Why should I use transformers?\n",
            "\n",
            "1. Easy-to-use state-of-the-art models\n",
            "- High performance on NLU and NLG tasks\n",
            "- Low barrier to entry for educators and practitioners\n",
            "- Few user-facing abstractions with just three classes to learn\n",
            "- A unified API for using all our pretrained models\n",
            "- Lower compute costs, smaller carbon footprint:\n",
            "\n",
            "2. Researchers can share trained models instead of always retraining\n",
            "- Practitioners can reduce compute time and production costs\n",
            "- Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages.\n",
            "\n",
            "3. Choose the right framework for every part of a model ' s lifetime\n",
            "- Train state-of-the-art models in 3 lines of code\n",
            "- Move a single model between TF2. 0 / PyTorch frameworks at will\n",
            "- Seamlessly pick the right framework for training, evaluation and production.\n",
            "\n",
            "4. Easily customize a model or an example to your needs\n",
            "- We provide examples for each architecture to reproduce the results published by its original authors\n",
            "- Model internal [SEP]\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
        "\n",
        "# Match the format provided at the beginning of the notebook\n",
        "text = tokenizer.decode(inputs[\"input_ids\"])\n",
        "text = text.replace(\" - \", \"-\").replace(\" , \", \", \").replace(\" .\", \".\").replace(\" :\", \":\")\n",
        "text = text.replace(\".-\", \"\\n- \")\n",
        "text = text.replace(\":-\", \"\\n- \")\n",
        "text = text.replace(\"[SEP] \", \"[SEP] \\n\\n\")\n",
        "text = text.replace(\"everyone. \", \"everyone.\\n\\n\")\n",
        "text = text.replace(\"experiments. Why\", \"experiments.\\n\\nWhy\")\n",
        "text = text.replace(\"transformers? 1.\", \"transformers?\\n\\n1.\")\n",
        "text = text.replace(\"footprint: 2.\", \"footprint:\\n\\n2.\")\n",
        "text = text.replace(\"languages. 3.\", \"languages.\\n\\n3.\")\n",
        "text = text.replace(\"production. 4.\", \"production.\\n\\n4.\")\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means the <font color='blue'>model</font> will have a <font color='blue'>hard time picking</font> the <font color='blue'>correct answer</font>. To fix this, the <font color='blue'>`question-answering` pipeline</font> allows us to <font color='blue'>split</font> the <font color='blue'>context</font> into <font color='blue'>smaller chunks</font>, <font color='blue'>specifying</font> the <font color='blue'>maximum length</font>. To make sure we don't split the context at exactly the wrong place to make it possible to find the answer, it also <font color='blue'>includes</font> some <font color='blue'>overlap between</font> the <font color='blue'>chunks</font>.\n",
        "\n",
        "We can have the <font color='blue'>tokenizer</font> (fast or slow) do this for us by <font color='blue'>adding `return_overflowing_tokens=True`</font>, and we can <font color='blue'>specify the overlap</font> we want with the <font color='blue'>`stride` argument</font>. Here is an example, using a smaller sentence:\n"
      ],
      "metadata": {
        "id": "xuIKahXCOOKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoXX-QNiIxXk",
        "outputId": "e9767991-7ded-4066-e951-c74d2da0fdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] This sentence is not [SEP]\n",
            "[CLS] is not too long [SEP]\n",
            "[CLS] too long but we [SEP]\n",
            "[CLS] but we are going [SEP]\n",
            "[CLS] are going to split [SEP]\n",
            "[CLS] to split it anyway [SEP]\n",
            "[CLS] it anyway. [SEP]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
        "inputs = tokenizer(\n",
        "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the <font color='blue'>sentence</font> has been <font color='blue'>split into chunks</font> in such a way that <font color='blue'>each entry</font> in `inputs[\"input_ids\"]` <font color='blue'>has at most 6 tokens</font>  (we would need to add padding to have the last entry be the same size as the others) and there is an <font color='blue'>overlap of 2 tokens</font> between <font color='blue'>each of the entries</font>.\n",
        "\n",
        "Let's take a <font color='blue'>closer look</font> at the <font color='blue'>result</font> of the <font color='blue'>tokenization</font>:"
      ],
      "metadata": {
        "id": "FMY4Wd5BOVT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOzI2eg9IxXk",
        "outputId": "b944a703-8faf-4175-d16e-99011c3d9f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
          ]
        }
      ],
      "source": [
        "print(inputs.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, we get <font color='blue'>input IDs</font> and an <font color='blue'>attention mask</font>. The last key, <font color='blue'>`overflow_to_sample_mapping`</font>, is a <font color='blue'>map</font> that <font color='blue'>tells us</font> which <font color='blue'>sentence</font> each of the <font color='blue'>results corresponds to</font> -- here we have <font color='blue'>7 results</font> that all come from the <font color='blue'>(only) sentence</font> we passed the tokenizer:\n"
      ],
      "metadata": {
        "id": "LVtUEpLlObX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3jfv0HIIxXk",
        "outputId": "9b74f73c-30d2-44bd-9874-d8a8e0eeb9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is <font color='blue'>more useful</font> when we <font color='blue'>tokenize several sentences together</font>. For instance, this returns:"
      ],
      "metadata": {
        "id": "W9Rg3MY6Onbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESMqyPqXIxXk",
        "outputId": "c6964985-3a13-4719-f731-c6465d9682ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"This sentence is not too long but we are going to split it anyway.\",\n",
        "    \"This sentence is shorter but will still get split.\",\n",
        "]\n",
        "inputs = tokenizer(\n",
        "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "which means the <font color='blue'>first sentence</font> is split into <font color='blue'>7 chunks</font> as before, and the <font color='blue'>next 4 chunks</font> come from the <font color='blue'>second sentence</font>.\n",
        "\n",
        "Now let's go back to our long context. By default the <font color='blue'>`question-answering` pipeline</font> uses a <font color='blue'>maximum length</font> of <font color='blue'>384</font>, as we mentioned earlier, and a <font color='blue'>stride</font> of <font color='blue'>128</font>, which <font color='blue'>correspond</font> to the <font color='blue'>way</font> the <font color='blue'>model was fine-tuned</font> (you can <font color='blue'>adjust</font> those <font color='blue'>parameters</font> by passing <font color='blue'>`max_seq_len`</font> and <font color='blue'>`stride`</font> arguments when calling the <font color='blue'>pipeline</font>). We will thus use those parameters when tokenizing. We'll also <font color='blue'>add padding</font> (to have <font color='blue'>samples</font> of the <font color='blue'>same length</font>, so we can build tensors) as well as <font color='blue'>ask for the offsets</font>:\n"
      ],
      "metadata": {
        "id": "8VKT9FJ1OzBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AjpUXDhIxXk",
        "outputId": "3f0508e4-f888-446c-8dca-1780d507d201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    long_context,\n",
        "    stride=128,\n",
        "    max_length=384,\n",
        "    padding=\"longest\",\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "print(inputs.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those <font color='blue'>inputs</font> will contain the <font color='blue'>input IDs</font> and <font color='blue'>attention masks</font> the model expects, as well as the <font color='blue'>offsets</font> and the <font color='blue'>`overflow_to_sample_mapping`</font> we just talked about. Since those two are <font color='blue'>not parameters used</font> by the <font color='blue'>model</font>, we'll <font color='blue'>pop them out</font> of the <font color='blue'>inputs`</font> (and we <font color='blue'>won't store the map</font>, since it's not useful here) before converting it to a tensor:\n"
      ],
      "metadata": {
        "id": "xOCAfzdtOhpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P56mKshOIxXl",
        "outputId": "5bf9564e-570c-43f8-ff53-133998b35d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 384])\n"
          ]
        }
      ],
      "source": [
        "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "offsets = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "inputs = inputs.convert_to_tensors(\"pt\")\n",
        "print(inputs[\"input_ids\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our <font color='blue'>long context</font> was <font color='blue'>split in two</font>, which means that <font color='blue'>after</font> it <font color='blue'>goes through</font> our <font color='blue'>model</font>, we will have <font color='blue'>two sets</font> of <font color='blue'>start</font> and <font color='blue'>end logits</font>:"
      ],
      "metadata": {
        "id": "_smn-11HO7qD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf6Mup9WIxXl",
        "outputId": "98422a6d-342c-4da0-c08b-60412da647d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 384]) torch.Size([2, 384])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Like before, we <font color='blue'>first mask</font> the <font color='blue'>tokens</font> that are <font color='blue'>not part</font> of the <font color='blue'>context before</font> taking the <font color='blue'>softmax</font>. We also <font color='blue'>mask</font> all the <font color='blue'>padding tokens</font> (as flagged by the attention mask):"
      ],
      "metadata": {
        "id": "9QzIgYnFPBp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "G0waclDJIxXl"
      },
      "outputs": [],
      "source": [
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "# Mask all the [PAD] tokens\n",
        "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can use the <font color='blue'>softmax</font> to <font color='blue'>convert</font> our <font color='blue'>logits</font> to <font color='blue'>probabilities</font>:"
      ],
      "metadata": {
        "id": "AVQNJ_hFPEmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "msCsfrCiIxXl"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is <font color='blue'>similar</font> to what we did for the <font color='blue'>small context</font>, but we <font color='blue'>repeat it</font> for each of our <font color='blue'>two chunks</font>. We <font color='blue'>attribute</font> a <font color='blue'>score</font> to <font color='blue'>all possible spans</font> of <font color='blue'>answer</font>, then take the <font color='blue'>span</font> with the <font color='blue'>best score</font>:"
      ],
      "metadata": {
        "id": "DELCWrBGPHDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiMbr_ZgIxXm",
        "outputId": "396db4f9-f561-4a3d-e51c-24ed81a8c906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 18, 0.3386705815792084), (173, 184, 0.9714869856834412)]\n"
          ]
        }
      ],
      "source": [
        "candidates = []\n",
        "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
        "    scores = start_probs[:, None] * end_probs[None, :]\n",
        "    idx = torch.triu(scores).argmax().item()\n",
        "\n",
        "    start_idx = idx // scores.shape[1]\n",
        "    end_idx = idx % scores.shape[1]\n",
        "    score = scores[start_idx, end_idx].item()\n",
        "    candidates.append((start_idx, end_idx, score))\n",
        "\n",
        "print(candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those <font color='blue'>two candidates correspond</font> to the <font color='blue'>best answers</font> the model was able to find in <font color='blue'>each chunk</font>. The model is way more confident the <font color='blue'>right answer</font> is in the <font color='blue'>second part</font> (which is a good sign!). Now we just have to <font color='blue'>map</font> those <font color='blue'>two token spans</font> to <font color='blue'>spans of characters</font> in the <font color='blue'>context</font> (we only need to map the second one to have our answer, but it's interesting to see what the model has picked in the first chunk)."
      ],
      "metadata": {
        "id": "CviM3q79PK0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚úèÔ∏è **Try it out!** Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk)."
      ],
      "metadata": {
        "id": "r0TjwWzEPQZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise - collect all candidates from all chunks\n",
        "all_candidates = []\n",
        "\n",
        "# Process each chunk separately (through stride/max_length splitting)\n",
        "for i, (s_logits, e_logits) in enumerate(zip(outputs.start_logits, outputs.end_logits)):\n",
        "    # Create mask to hide non-context tokens\n",
        "    # Here, sequence_ids: 0=question, 1=context, None=special tokens\n",
        "    mask = torch.tensor([j != 1 for j in inputs.sequence_ids(i)])  # Hide everything except context\n",
        "    mask[0] = False  # Keep [CLS] token visible for potential answers\n",
        "    mask |= inputs[\"attention_mask\"][i] == 0  # Mask padding tokens\n",
        "\n",
        "    # Need to clone as we will do in-place modification on tensors\n",
        "    s_logits, e_logits = s_logits.clone(), e_logits.clone()\n",
        "\n",
        "    # Apply mask through the same strategy as above\n",
        "    s_logits[mask] = e_logits[mask] = -10000\n",
        "\n",
        "    # Calculate probability matrix: start_prob[i] * end_prob[j] for all i,j combinations\n",
        "    # torch.triu keeps only upper triangle (start <= end positions)\n",
        "    scores = torch.triu(torch.softmax(s_logits, -1)[:, None] * torch.softmax(e_logits, -1)[None, :])\n",
        "\n",
        "    # Get top 10 scoring spans from this chunk\n",
        "    for idx in torch.topk(scores.flatten(), min(10, scores.nonzero().size(0))).indices:\n",
        "        # Convert flat index back to 2D coordinates (start_pos, end_pos)\n",
        "        start, end = divmod(idx.item(), scores.shape[1])\n",
        "\n",
        "        # Only keep candidates with positive scores that are in context\n",
        "        if (score := scores[start, end].item()) > 0.001:\n",
        "            # Verify that both start and end are in the context (sequence_id == 1)\n",
        "            if inputs.sequence_ids(i)[start] == 1 and inputs.sequence_ids(i)[end] == 1:\n",
        "              # Store: (score, chunk_index, start_token, end_token)\n",
        "              all_candidates.append((score, i, start, end))\n",
        "\n",
        "# Get the top 5 answers across all chunks\n",
        "top_5 = sorted(all_candidates, reverse=True)[:5]\n",
        "\n",
        "for rank, (score, chunk_idx, start_idx, end_idx) in enumerate(top_5, 1):\n",
        "    # Extract the actual tokens from this span\n",
        "    tokens = inputs[\"input_ids\"][chunk_idx][start_idx:end_idx+1]\n",
        "    # Decode tokens back to readable text\n",
        "    #answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    print(f\"{rank}. Score: {score:.6f}, Chunk: {chunk_idx}\")"
      ],
      "metadata": {
        "id": "Ky4894pnlLk_",
        "outputId": "79e81bcb-3dbf-4715-8ecb-9b32bf31aedf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Score: 0.971487, Chunk: 1\n",
            "2. Score: 0.149496, Chunk: 0\n",
            "3. Score: 0.015565, Chunk: 1\n",
            "4. Score: 0.013706, Chunk: 0\n",
            "5. Score: 0.010597, Chunk: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>`offsets`</font> we <font color='blue'>grabbed earlier</font> is actually a <font color='blue'>list of offsets</font>, with <font color='blue'>one list per chunk</font> of text:"
      ],
      "metadata": {
        "id": "WreQn8I3PU3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbB-Zs-QIxXm",
        "outputId": "4cdd8747-03e3-434c-f36e-1f4bc93b0b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ü§ó Transformers: State of the Art NLP\n",
            "   Score: 0.3387, Position: 0-37, Tokens: 0-18\n",
            "\n",
            "2. Jax, PyTorch and TensorFlow\n",
            "   Score: 0.9715, Position: 1892-1919, Tokens: 173-184\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (candidate, offset) in enumerate(zip(candidates, offsets), 1):\n",
        "    start_token, end_token, score = candidate\n",
        "    start_char, _ = offset[start_token]\n",
        "    _, end_char = offset[end_token]\n",
        "    answer = long_context[start_char:end_char].strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "    print(f\"{i}. {answer}\")\n",
        "    print(f\"   Score: {score:.4f}, Position: {start_char}-{end_char}, Tokens: {start_token}-{end_token}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we <font color='blue'>ignore</font> the <font color='blue'>first result</font>, we get the <font color='blue'>same result</font> as <font color='blue'>our pipeline</font> for this long context -- yay!"
      ],
      "metadata": {
        "id": "8UUsgsAcPZS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in `top_k=5` when calling it.\n"
      ],
      "metadata": {
        "id": "e2OWv-spPgDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise - Use the work done before\n",
        "for rank, (score, chunk_idx, start_idx, end_idx) in enumerate(top_5, 1):\n",
        "    # Extract the actual tokens from this span\n",
        "    tokens = inputs[\"input_ids\"][chunk_idx][start_idx:end_idx+1]\n",
        "    # Decode tokens back to readable text\n",
        "    answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    print(f\"{answer}, Score: {score:.6f}\")"
      ],
      "metadata": {
        "id": "VCqZ6tylqW33",
        "outputId": "8f5c2a3f-82df-4b4d-afcc-db9468bf7bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jax, PyTorch and TensorFlow, Score: 0.971487\n",
            "State of the Art NLP, Score: 0.149496\n",
            "Jax, PyTorch and TensorFlow ‚Äî, Score: 0.015565\n",
            "NLP, Score: 0.013706\n",
            "Transformers : State of the Art NLP, Score: 0.010597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: We compare with the pipeline results\n",
        "print(\"Verification using pipeline with top_k=5:\")\n",
        "print()\n",
        "pipeline_results = question_answerer(question=question, context=long_context, top_k=5)\n",
        "for i, result in enumerate(pipeline_results, 1):\n",
        "    print(f\"{i}. {result['answer']}, Score: {result['score']:.4f}\")"
      ],
      "metadata": {
        "id": "MIxEDpdJquXD",
        "outputId": "c87ea2be-fe42-461f-c048-3a865adb06eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification using pipeline with top_k=5:\n",
            "\n",
            "1. Jax, PyTorch and TensorFlow, Score: 0.9715\n",
            "2. State of the Art NLP, Score: 0.1495\n",
            "3. Jax, PyTorch and TensorFlow ‚Äî, Score: 0.0156\n",
            "4. NLP, Score: 0.0137\n",
            "5. Transformers: State of the Art NLP, Score: 0.0106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This concludes our <font color='blue'>deep dive</font> into the <font color='blue'>tokenizer's capabilities</font>. We will put all of this in <font color='blue'>practice again</font> in the <font color='blue'>next chapter</font>, when we show you how to <font color='blue'>fine-tune</font> a <font color='blue'>model</font> on a <font color='blue'>range</font> of common <font color='blue'>NLP tasks</font>.\n"
      ],
      "metadata": {
        "id": "iSr2D38UPhoi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fast tokenizers in the QA pipeline (PyTorch)",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dfabfa3ee4374be793f1ac04dd87d30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_e5d9f4c7bdeb4c5fad3d97d92f93e9db"
          }
        },
        "a2486a5fba2944979913e204540f7fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9de488e58d404735812ebfc71be9d4df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca968987173c4987b8143061d8f8c671",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a8e2165ca77a4a5682718f3b851c2713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_36234548c73e4398a206c48c54bf4cc2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6764de3985fe4ac58ce680d02d775e0a",
            "value": ""
          }
        },
        "ea277f91ec9f4d429932474ee5860ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_68f823098e6f414b91c680237a27f28e",
            "style": "IPY_MODEL_14489a68beb443eba393140fa6fecf7c",
            "value": true
          }
        },
        "fd1fe7d68f614b2b847be08f73c15f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ad75c7577436430784df90af0fe6b3fb",
            "style": "IPY_MODEL_ee28f3c1ede84e5b87d02f032d319aa7",
            "tooltip": ""
          }
        },
        "269ecf76e96c44b09b34727208c0e780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ae28d3f65954e67bd8b8d56ff531c9a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2f9af6fbba024f3b83eb7e5010b1d5f3",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e5d9f4c7bdeb4c5fad3d97d92f93e9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9de488e58d404735812ebfc71be9d4df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca968987173c4987b8143061d8f8c671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36234548c73e4398a206c48c54bf4cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6764de3985fe4ac58ce680d02d775e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68f823098e6f414b91c680237a27f28e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14489a68beb443eba393140fa6fecf7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad75c7577436430784df90af0fe6b3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee28f3c1ede84e5b87d02f032d319aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0ae28d3f65954e67bd8b8d56ff531c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9af6fbba024f3b83eb7e5010b1d5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61e4dcf8c2a248dfb698e7a74fb43820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4490fa92bb8e431d8a08eaa3870a8526",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3f95b47997994b1199954869cf4a7938",
            "value": "Connecting..."
          }
        },
        "4490fa92bb8e431d8a08eaa3870a8526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f95b47997994b1199954869cf4a7938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}