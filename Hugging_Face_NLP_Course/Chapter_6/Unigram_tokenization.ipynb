{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90OQ68Pwwr89"
      },
      "source": [
        "# Unigram tokenization\n",
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "_koMGBGp3T1C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also log into Hugging Face."
      ],
      "metadata": {
        "id": "Q1uHGJaq3VoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "31dfbf281f3c4de2bdeea2bc149b0660",
            "1c9a51238bd5498dbb1cb2ffd85eb564",
            "c5150fa6719d44df8c6f4d9f6dc436a9",
            "c6173a9738e94763aeecf824bf5bab74",
            "a0f2b57d9977474da50cf4e7d85ccfef",
            "f5bdaac00a4442a3b1fb46d1f3f98e29",
            "3d40fcab598143988f81ecc37f2840b7",
            "13a90d855f554f498320f4bb6678a50a",
            "7910cca465f040539a2d887e95e7ebc1",
            "4262a2f7c30e4227862c2b488dcb0773",
            "0162262f478e4d4a87984beb21263fb0",
            "904127ff299b4bab951c149938fe85e7",
            "d49d7de3377c4b1a9c02f9378d5e2c2b",
            "ea17b9ab08ea4a5495871d79b5bfbd50",
            "9f2fcc31113a4966a7a702e0363de186",
            "ce83492480d240c98a10cea0875f1a46",
            "99be669de5bb408fbad02ff081655be1",
            "5018984030b64a20ad38848816f38854",
            "8d4adddcf18345c7baadfeb0a5cfa89b",
            "f5a328cc8e1040cf8fd1240aa0244625"
          ]
        },
        "id": "3EbHpu4c3XqP",
        "outputId": "6c5c4546-95de-4851-8f56-85a4b678cefe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31dfbf281f3c4de2bdeea2bc149b0660"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>Unigram algorithm</font> is used in <font color='blue'>combination</font> with [SentencePiece](https://huggingface.co/papers/1808.06226), which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\n",
        "\n",
        "SentencePiece addresses the fact that <font color='blue'>not all languages</font> use <font color='blue'>spaces</font> to <font color='blue'>separate words</font>. Instead, SentencePiece treats the <font color='blue'>input</font> as a <font color='blue'>raw input stream</font> which includes the <font color='blue'>space</font> in the <font color='blue'>set of characters</font> to use. Then it can use the <font color='blue'>Unigram algorithm</font> to <font color='blue'>construct</font> the appropriate <font color='blue'>vocabulary</font>.\n",
        "\n",
        "üí° **Tip:** This section covers Unigram in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm."
      ],
      "metadata": {
        "id": "L3260qoE3OTP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkpQk7rGwr9A"
      },
      "source": [
        "## Training algorithm\n",
        "\n",
        "Compared to BPE and WordPiece, Unigram works in the other direction: it <font color='blue'>starts</font> from a <font color='blue'>big vocabulary</font> and <font color='blue'>removes tokens</font> from it until it <font color='blue'>reaches</font> the <font color='blue'>desired vocabulary size</font>. There are <font color='blue'>several options</font> to use to build that base vocabulary: we can take the <font color='blue'>most common substrings</font> in <font color='blue'>pre-tokenized words</font>, for instance, or <font color='blue'>apply BPE</font> on the <font color='blue'>initial corpus</font> with a large vocabulary size.\n",
        "\n",
        "At <font color='blue'>each step</font> of the training, the Unigram algorithm <font color='blue'>computes</font> a <font color='blue'>loss</font> over the <font color='blue'>corpus</font> given the current vocabulary. Then, for <font color='blue'>each symbol</font> in the <font color='blue'>vocabulary</font>, the algorithm <font color='blue'>computes</font> how much the <font color='blue'>overall loss</font> would <font color='blue'>increase</font> if the <font color='blue'>symbol</font> was <font color='blue'>removed</font>, and looks for the <font color='blue'>symbols</font> that would <font color='blue'>increase</font> it the <font color='blue'>least</font>. Those symbols have a <font color='blue'>lower effect</font> on the <font color='blue'>overall loss</font> over the <font color='blue'>corpus</font>, so in a sense they are \"less needed\" and are the best candidates for removal.\n",
        "\n",
        "This is all a very <font color='blue'>costly operation</font>, so we don't just remove the single symbol associated with the lowest loss increase, but the <font color='blue'>p</font> (p being a <font color='blue'>hyperparameter</font> you <font color='blue'>can control</font>, usually <font color='blue'>10</font> or <font color='blue'>20</font>) <font color='blue'>percent</font> of the <font color='blue'>symbols</font> associated with the <font color='blue'>lowest loss increase</font>. This process is then repeated until the vocabulary has reached the desired size.\n",
        "\n",
        "Note that we <font color='blue'>never remove</font> the <font color='blue'>base characters</font>, to make sure any word can be tokenized.\n",
        "\n",
        "Now, this is still a bit vague: the <font color='blue'>main part</font> of the <font color='blue'>algorithm</font> is to <font color='blue'>compute</font> a <font color='blue'>loss</font> over the <font color='blue'>corpus</font> and see how it <font color='blue'>changes</font> when we <font color='blue'>remove some tokens</font> from the <font color='blue'>vocabulary</font>, but we haven't explained how to do this yet. This step relies on the <font color='blue'>tokenization algorithm</font> of a <font color='blue'>Unigram model</font>, so we'll dive into this next.\n",
        "\n",
        "We'll <font color='blue'>reuse</font> the <font color='blue'>corpus</font> from the <font color='blue'>previous examples</font>:\n",
        "\n",
        "```\n",
        "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
        "```\n",
        "\n",
        "and for this example, we will take <font color='blue'>all strict substrings</font> for the initial vocabulary :\n",
        "\n",
        "```\n",
        "[\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GwRU5RGwr9A"
      },
      "source": [
        "## Tokenization algorithm\n",
        "\n",
        "A <font color='blue'>Unigram model</font> is a type of language model that considers <font color='blue'>each token</font> to be <font color='blue'>independent</font> of the <font color='blue'>tokens before it</font>. It's the simplest language model, in the sense that the <font color='blue'>probability of token X</font> given the <font color='blue'>previous context</font> is just the <font color='blue'>probability of token X</font>. So, if we used a Unigram language model to generate text, we would <font color='blue'>always predict</font> the <font color='blue'>most common token</font>.\n",
        "\n",
        "The <font color='blue'>probability</font> of a <font color='blue'>given token</font> is its <font color='blue'>frequency</font> (the number of times we find it) in the original corpus, <font color='blue'>divided</font> by the <font color='blue'>sum of all frequencies</font> of <font color='blue'>all tokens</font> in the <font color='blue'>vocabulary</font> (to make sure the probabilities sum up to 1). For instance, `\"ug\"` is present in `\"hug\"`, `\"pug\"`, and `\"hugs\"`, so it has a frequency of 20 in our corpus.\n",
        "\n",
        "Here are the <font color='blue'>frequencies</font> of <font color='blue'>all</font> the <font color='blue'>possible subwords</font> in the vocabulary:\n",
        "\n",
        "```\n",
        "(\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
        "(\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
        "```\n",
        "\n",
        "So, the <font color='blue'>sum</font> of <font color='blue'>all frequencies</font> is <font color='blue'>210</font>, and the probability of the subword `\"ug\"` is thus 20/210."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNMtvnwEwr9A"
      },
      "source": [
        "‚úèÔ∏è **Now your turn!** Write the code to compute the frequencies above and double-check that the results shown are correct, as well as the total sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLgCyQ5jwr9B",
        "outputId": "aaf3569c-1a8f-4637-e659-050c80023fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed frequencies:\n",
            "(\"b\", 4)\n",
            "(\"bu\", 4)\n",
            "(\"bun\", 4)\n",
            "(\"g\", 20)\n",
            "(\"gs\", 5)\n",
            "(\"h\", 15)\n",
            "(\"hu\", 15)\n",
            "(\"hug\", 15)\n",
            "(\"hugs\", 5)\n",
            "(\"n\", 16)\n",
            "(\"p\", 17)\n",
            "(\"pu\", 17)\n",
            "(\"pug\", 5)\n",
            "(\"pun\", 12)\n",
            "(\"s\", 5)\n",
            "(\"u\", 36)\n",
            "(\"ug\", 20)\n",
            "(\"ugs\", 5)\n",
            "(\"un\", 16)\n",
            "\n",
            "Total sum: 236\n",
            "\n",
            "Total sum of excluded:\n",
            "  \"pug\": 5 occurrences\n",
            "  \"pun\": 12 occurrences\n",
            "  \"bun\": 4 occurrences\n",
            "  \"hugs\": 5 occurrences\n",
            "\n",
            "Total sum calculated: 236-26 excluded = 210\n"
          ]
        }
      ],
      "source": [
        "# Your code here to compute the frequencies\n",
        "def compute_frequencies(corpus):\n",
        "    \"\"\"Compute all subword frequencies for Unigram model.\"\"\"\n",
        "    freq = {}\n",
        "    for word in corpus:\n",
        "        for i in range(len(word)):\n",
        "            for j in range(i + 1, len(word) + 1):\n",
        "                subword = word[i:j]\n",
        "                freq[subword] = freq.get(subword, 0) + 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "corpus = [\"hug\"]*10 + [\"pug\"]*5 + [\"pun\"]*12 + [\"bun\"]*4 + [\"hugs\"]*5\n",
        "frequencies = compute_frequencies(corpus)\n",
        "\n",
        "print(\"Computed frequencies:\")\n",
        "for subword in sorted(frequencies.keys()):\n",
        "    print(f'(\"{subword}\", {frequencies[subword]})')\n",
        "\n",
        "vocabulary = {\n",
        "    \"h\": 15, \"u\": 36, \"g\": 20, \"hu\": 15, \"ug\": 20, \"p\": 17,\n",
        "    \"pu\": 17, \"n\": 16, \"un\": 16, \"b\": 4, \"bu\": 4, \"s\": 5,\n",
        "    \"hug\": 15, \"gs\": 5, \"ugs\": 5\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"\\nTotal sum: {sum(frequencies.values())}\")\n",
        "excluded_total = 0\n",
        "print(f\"\\nTotal sum of excluded:\")\n",
        "for subword, freq in frequencies.items():\n",
        "    if subword not in vocabulary:\n",
        "        print(f'  \"{subword}\": {freq} occurrences')\n",
        "        excluded_total += freq\n",
        "print('\\nTotal sum calculated: 236-26 excluded = 210')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCZAdRWmwr9B"
      },
      "source": [
        "Now, to <font color='blue'>tokenize</font> a given word, we look at <font color='blue'>all</font> the <font color='blue'>possible segmentations</font> into tokens and <font color='blue'>compute</font> the <font color='blue'>probability of each</font> according to the Unigram model. Since <font color='blue'>all tokens</font> are considered <font color='blue'>independent</font>, this probability is just the <font color='blue'>product</font> of the <font color='blue'>probability of each token</font>. For instance, the tokenization `[\"p\", \"u\", \"g\"]` of `\"pug\"` has the probability:\n",
        "\n",
        "$$P([``p\", ``u\", ``g\"]) = P(``p\") \\times P(``u\") \\times P(``g\") = \\frac{5}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.000389$$\n",
        "\n",
        "Comparatively, the tokenization `[\"pu\", \"g\"]` has the probability:\n",
        "\n",
        "$$P(``pu\", \"g\"]) = P(``pu\") \\times P(``g\") = \\frac{5}{210} \\times \\frac{20}{210} = 0.0022676$$\n",
        "\n",
        "so that one is way more likely. In general, <font color='blue'>tokenizations</font> with the <font color='blue'>least tokens possible</font> will have the <font color='blue'>highest probability</font> (because of that division by 210 repeated for each token), which corresponds to what we <font color='blue'>want intuitively</font>: to <font color='blue'>split</font> a <font color='blue'>word</font> into the <font color='blue'>least number of tokens</font> possible.\n",
        "\n",
        "The <font color='blue'>tokenization</font> of a <font color='blue'>word</font> with the Unigram model is then the <font color='blue'>tokenization</font> with the <font color='blue'>highest probability</font>. In the example of `\"pug\"`, here are the <font color='blue'>probabilities</font> we would get for <font color='blue'>each possible segmentation</font>:\n",
        "\n",
        "```\n",
        "[\"p\", \"u\", \"g\"] : 0.000389\n",
        "[\"p\", \"ug\"] : 0.0022676\n",
        "[\"pu\", \"g\"] : 0.0022676\n",
        "```\n",
        "\n",
        "So, `\"pug\"` would be tokenized as `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]`, depending on <font color='blue'>which</font> of those <font color='blue'>segmentations</font> is <font color='blue'>encountered first</font> (note that in a larger corpus, equality cases like this will be rare)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JYGOJVhwr9C"
      },
      "source": [
        "In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it's going to be a bit harder. There is a <font color='blue'>classic algorithm</font> used for this, called the <font color='blue'>Viterbi algorithm</font>. Essentially, we can <font color='blue'>build a graph</font> to <font color='blue'>detect</font> the <font color='blue'>possible segmentations</font> of a <font color='blue'>given word</font> by saying there is a <font color='blue'>branch</font> from <font color='blue'>character _a_</font> to <font color='blue'>character _b_</font> if the <font color='blue'>subword</font> from <font color='blue'>_a_ to _b_</font> is <font color='blue'>in the vocabulary</font>, and attribute to that <font color='blue'>branch</font> the <font color='blue'>probability of the subword</font>.\n",
        "\n",
        "To <font color='blue'>find the path</font> in that <font color='blue'>graph</font> that is going to have the <font color='blue'>best score</font> the Viterbi algorithm determines, for <font color='blue'>each position</font> in the <font color='blue'>word</font>, the <font color='blue'>segmentation</font> with the <font color='blue'>best score</font> that <font color='blue'>ends</font> at <font color='blue'>that position</font>. Since we go from the <font color='blue'>beginning to the end</font>, that <font color='blue'>best score</font> can be found by <font color='blue'>looping</font> through <font color='blue'>all subwords ending</font> at the <font color='blue'>current position</font> and then <font color='blue'>using</font> the <font color='blue'>best tokenization score</font> from the <font color='blue'>position</font> this <font color='blue'>subword begins at</font>. Then, we just have to <font color='blue'>unroll the path taken</font> to <font color='blue'>arrive at the end</font>.\n",
        "\n",
        "Let's take a look at an example using our vocabulary and the word `\"unhug\"`. For <font color='blue'>each position</font>, the <font color='blue'>subwords</font> with the <font color='blue'>best scores</font> ending there are the following:\n",
        "\n",
        "```\n",
        "Character 0 (u): \"u\" (score 0.171429)\n",
        "Character 1 (n): \"un\" (score 0.076191)\n",
        "Character 2 (h): \"un\" \"h\" (score 0.005442)\n",
        "Character 3 (u): \"un\" \"hu\" (score 0.005442)\n",
        "Character 4 (g): \"un\" \"hug\" (score 0.005442)\n",
        "```\n",
        "\n",
        "Thus `\"unhug\"` would be tokenized as `[\"un\", \"hug\"]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiJnRUnPwr9C"
      },
      "source": [
        "‚úèÔ∏è **Now your turn!** Determine the tokenization of the word `\"huggun\"`, and its score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSiP4ZRJwr9C",
        "outputId": "e21234f6-b843-4405-ab7c-664a6a6532d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization of \"huggun\": ['hug', 'g', 'un']\n",
            "Score: 0.000518\n"
          ]
        }
      ],
      "source": [
        "# Your code here to determine the tokenization of \"huggun\"\n",
        "def unigram_tokenize(word, vocabulary):\n",
        "    \"\"\"Tokenize using Unigram model with Viterbi algorithm.\"\"\"\n",
        "    total = sum(vocabulary.values())\n",
        "    probs = {k: v/total for k, v in vocabulary.items()}\n",
        "\n",
        "    n = len(word)\n",
        "    best_score = [0.0] * (n + 1)\n",
        "    best_path = [[] for _ in range(n + 1)]\n",
        "    best_score[0] = 1.0\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(i):\n",
        "            subword = word[j:i]\n",
        "            if subword in probs:\n",
        "                score = best_score[j] * probs[subword]\n",
        "                if score > best_score[i]:\n",
        "                    best_score[i] = score\n",
        "                    best_path[i] = best_path[j] + [subword]\n",
        "\n",
        "    return best_path[n], best_score[n]\n",
        "\n",
        "vocab = {\"h\": 15, \"u\": 36, \"g\": 20, \"hu\": 15, \"ug\": 20, \"p\": 17,\n",
        "         \"pu\": 17, \"n\": 16, \"un\": 16, \"b\": 4, \"bu\": 4, \"s\": 5,\n",
        "         \"hug\": 15, \"gs\": 5, \"ugs\": 5}\n",
        "\n",
        "tokens, score = unigram_tokenize(\"huggun\", vocab)\n",
        "print(f'Tokenization of \"huggun\": {tokens}')\n",
        "print(f'Score: {score:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JitBHMrdwr9D"
      },
      "source": [
        "## Back to training\n",
        "\n",
        "Now that we have seen how the tokenization works, we can dive a little more deeply into the <font color='blue'>loss</font> used <font color='blue'>during training</font>. At any given stage, this <font color='blue'>loss</font> is computed by <font color='blue'>tokenizing every word</font> in the <font color='blue'>corpus</font>, using the <font color='blue'>current vocabulary</font> and the <font color='blue'>Unigram model</font> determined by the frequencies of each token in the corpus (as seen before).\n",
        "\n",
        "<font color='blue'>Each word</font> in the corpus has a <font color='blue'>score</font>, and the <font color='blue'>loss</font> is the <font color='blue'>negative log likelihood</font> of those scores -- that is, the sum for all the words in the corpus of all the `-log(P(word))`.\n",
        "\n",
        "Let's go back to our example with the following corpus:\n",
        "\n",
        "```\n",
        "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
        "```\n",
        "\n",
        "The <font color='blue'>tokenization</font> of <font color='blue'>each word</font> with their respective <font color='blue'>scores</font> is:\n",
        "\n",
        "```\n",
        "\"hug\": [\"hug\"] (score 0.071428)\n",
        "\"pug\": [\"pu\", \"g\"] (score 0.007710)\n",
        "\"pun\": [\"pu\", \"n\"] (score 0.006168)\n",
        "\"bun\": [\"bu\", \"n\"] (score 0.001451)\n",
        "\"hugs\": [\"hug\", \"s\"] (score 0.001701)\n",
        "```\n",
        "\n",
        "So the <font color='blue'>loss</font> is:\n",
        "\n",
        "```\n",
        "10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF9bDMkcwr9D"
      },
      "source": [
        "Now we need to compute how <font color='blue'>removing each token affects</font> the <font color='blue'>loss</font>. This is rather tedious, so we'll just do it for <font color='blue'>two tokens</font> here and save the whole process for when we have code to help us. In this (very) particular case, we had <font color='blue'>two equivalent tokenizations</font> of <font color='blue'>all</font> the <font color='blue'>words</font>: as we saw earlier, for example, `\"pug\"` could be tokenized `[\"p\", \"ug\"]` with the same score. Thus, <font color='blue'>removing</font> the <font color='blue'>`\"pu\"` token</font> from the <font color='blue'>vocabulary</font> will <font color='blue'>give</font> the <font color='blue'>exact same loss</font>.\n",
        "\n",
        "On the other hand,<font color='blue'>removing `\"hug\"`</font> will <font color='blue'>make</font> the <font color='blue'>loss worse</font>, because the tokenization of `\"hug\"` and `\"hugs\"` will become:\n",
        "\n",
        "```\n",
        "\"hug\": [\"hu\", \"g\"] (score 0.006802)\n",
        "\"hugs\": [\"hu\", \"gs\"] (score 0.001701)\n",
        "```\n",
        "\n",
        "These changes will cause the <font color='blue'>loss</font> to <font color='blue'>rise by</font>:\n",
        "\n",
        "```\n",
        "- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5\n",
        "```\n",
        "\n",
        "Therefore, the token `\"pu\"` will probably be removed from the vocabulary, but not `\"hug\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usf5z_EFwr9D"
      },
      "source": [
        "## Implementing Unigram\n",
        "\n",
        "Now let's implement everything we've seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better.\n",
        "\n",
        "We will use the <font color='blue'>same corpus</font> as <font color='blue'>before</font> as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sa-4GNgVwr9D"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPh5_-vhwr9D"
      },
      "source": [
        "This time, we will use `xlnet-base-cased` as our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ALDPwa8wwr9D"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QIVbwBfwr9E"
      },
      "source": [
        "Like for BPE and WordPiece, we begin by <font color='blue'>counting the number</font> of <font color='blue'>occurrences</font> of <font color='blue'>each word</font> in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqv0frIIwr9E",
        "outputId": "091b5716-4564-46ff-b2f5-5da2f5432053"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'‚ñÅThis': 3,\n",
              "             '‚ñÅis': 2,\n",
              "             '‚ñÅthe': 1,\n",
              "             '‚ñÅHugging': 1,\n",
              "             '‚ñÅFace': 1,\n",
              "             '‚ñÅCourse.': 1,\n",
              "             '‚ñÅchapter': 1,\n",
              "             '‚ñÅabout': 1,\n",
              "             '‚ñÅtokenization.': 1,\n",
              "             '‚ñÅsection': 1,\n",
              "             '‚ñÅshows': 1,\n",
              "             '‚ñÅseveral': 1,\n",
              "             '‚ñÅtokenizer': 1,\n",
              "             '‚ñÅalgorithms.': 1,\n",
              "             '‚ñÅHopefully,': 1,\n",
              "             '‚ñÅyou': 1,\n",
              "             '‚ñÅwill': 1,\n",
              "             '‚ñÅbe': 1,\n",
              "             '‚ñÅable': 1,\n",
              "             '‚ñÅto': 1,\n",
              "             '‚ñÅunderstand': 1,\n",
              "             '‚ñÅhow': 1,\n",
              "             '‚ñÅthey': 1,\n",
              "             '‚ñÅare': 1,\n",
              "             '‚ñÅtrained': 1,\n",
              "             '‚ñÅand': 1,\n",
              "             '‚ñÅgenerate': 1,\n",
              "             '‚ñÅtokens.': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noz1qBZrwr9E"
      },
      "source": [
        "Then, we need to <font color='blue'>initialize our vocabulary</font> to something <font color='blue'>larger</font> than the <font color='blue'>vocab size</font> we will want at the end. We have to include <font color='blue'>all</font> the <font color='blue'>basic characters</font> (otherwise we won't be able to tokenize every word), but for the <font color='blue'>bigger substrings</font> we'll only <font color='blue'>keep</font> the <font color='blue'>most common</font> ones, so we <font color='blue'>sort</font> them by <font color='blue'>frequency</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5w_I2aGwr9E",
        "outputId": "5f73bb2b-0a5c-4492-f5c1-5cc601510c9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('‚ñÅt', 7),\n",
              " ('is', 5),\n",
              " ('er', 5),\n",
              " ('‚ñÅa', 5),\n",
              " ('‚ñÅto', 4),\n",
              " ('to', 4),\n",
              " ('en', 4),\n",
              " ('‚ñÅT', 3),\n",
              " ('‚ñÅTh', 3),\n",
              " ('‚ñÅThi', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "char_freqs = defaultdict(int)\n",
        "subwords_freqs = defaultdict(int)\n",
        "for word, freq in word_freqs.items():\n",
        "    for i in range(len(word)):\n",
        "        char_freqs[word[i]] += freq\n",
        "        # Loop through the subwords of length at least 2\n",
        "        for j in range(i + 2, len(word) + 1):\n",
        "            subwords_freqs[word[i:j]] += freq\n",
        "\n",
        "# Sort subwords by frequency\n",
        "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_subwords[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_JrXnyzwwr9E"
      },
      "outputs": [],
      "source": [
        "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
        "token_freqs = {token: freq for token, freq in token_freqs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58TPBPhAwr9F"
      },
      "source": [
        "üí° **Tip:** SentencePiece uses a more efficient algorithm called <font color='blue'>Enhanced Suffix Array (ESA)</font> to create the initial vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYaEvEzwr9F"
      },
      "source": [
        "Next, we compute the <font color='blue'>sum of all frequencies</font>, to <font color='blue'>convert</font> the <font color='blue'>frequencies</font> into <font color='blue'>probabilities</font>. For our model we will <font color='blue'>store</font> the <font color='blue'>logarithms</font> of the <font color='blue'>probabilities</font>, because it's more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UhOyLHubwr9F"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "\n",
        "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WMy5YFwwr9F"
      },
      "source": [
        "Now the main function is the one that <font color='blue'>tokenizes words</font> using the <font color='blue'>Viterbi algorithm</font>. As we saw before, that algorithm computes the <font color='blue'>best segmentation</font> of <font color='blue'>each substring</font> of the <font color='blue'>word</font>, which we will store in a variable named `best_segmentations`. We will store <font color='blue'>one dictionary</font> per <font color='blue'>position</font> in the <font color='blue'>word</font> (from 0 to its total length), with <font color='blue'>two keys</font>: the <font color='blue'>index</font> of the <font color='blue'>start</font> of the <font color='blue'>last token</font> in the <font color='blue'>best segmentation</font>, and the <font color='blue'>score</font> of the <font color='blue'>best segmentation</font>. With the <font color='blue'>index</font> of the <font color='blue'>start</font> of the <font color='blue'>last token</font>, we will be able to <font color='blue'>retrieve</font> the <font color='blue'>full segmentation</font> once the <font color='blue'>list</font> is <font color='blue'>completely populated</font>.\n",
        "\n",
        "Populating the list is done with just <font color='blue'>two loops</font>: the <font color='blue'>main loop</font> goes over <font color='blue'>each start position</font>, and the <font color='blue'>second loop</font> tries <font color='blue'>all substrings beginning</font> at that <font color='blue'>start position</font>. If the <font color='blue'>substring</font> is in the <font color='blue'>vocabulary</font>, we have a <font color='blue'>new segmentation</font> of the <font color='blue'>word</font> up until that <font color='blue'>end position</font>, which we compare to what is in `best_segmentations`.\n",
        "\n",
        "Once the main loop is finished, we just <font color='blue'>start from the end </font>and <font color='blue'>hop</font> from <font color='blue'>one start position</font> to the <font color='blue'>next</font>, <font color='blue'>recording</font> the <font color='blue'>tokens</font> as we go, <font color='blue'>until</font> we <font color='blue'>reach</font> the <font color='blue'>start</font> of <font color='blue'>the word</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "L2czbdrrwr9F"
      },
      "outputs": [],
      "source": [
        "def encode_word(word, model):\n",
        "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
        "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
        "    ]\n",
        "    for start_idx in range(len(word)):\n",
        "        # This should be properly filled by the previous steps of the loop\n",
        "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
        "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
        "            token = word[start_idx:end_idx]\n",
        "            if token in model and best_score_at_start is not None:\n",
        "                score = model[token] + best_score_at_start\n",
        "                # If we have found a better segmentation ending at end_idx, we update\n",
        "                if (\n",
        "                    best_segmentations[end_idx][\"score\"] is None\n",
        "                    or best_segmentations[end_idx][\"score\"] > score\n",
        "                ):\n",
        "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
        "\n",
        "    segmentation = best_segmentations[-1]\n",
        "    if segmentation[\"score\"] is None:\n",
        "        # We did not find a tokenization of the word -> unknown\n",
        "        return [\"<unk>\"], None\n",
        "\n",
        "    score = segmentation[\"score\"]\n",
        "    start = segmentation[\"start\"]\n",
        "    end = len(word)\n",
        "    tokens = []\n",
        "    while start != 0:\n",
        "        tokens.insert(0, word[start:end])\n",
        "        next_start = best_segmentations[start][\"start\"]\n",
        "        end = start\n",
        "        start = next_start\n",
        "    tokens.insert(0, word[start:end])\n",
        "    return tokens, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLwX_BqYwr9F"
      },
      "source": [
        "We can already try our initial model on some words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBuyHEwgwr9F",
        "outputId": "6e0ac85c-12b2-4799-faa9-e79ef757a7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
            "(['This'], 6.288267030694535)\n"
          ]
        }
      ],
      "source": [
        "print(encode_word(\"Hopefully\", model))\n",
        "print(encode_word(\"This\", model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcdr6jpPwr9G"
      },
      "source": [
        "Now it's straightforward to compute the <font color='blue'>loss</font> of the <font color='blue'>model</font> on the corpus!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "o804_2gWwr9G"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model):\n",
        "    loss = 0\n",
        "    for word, freq in word_freqs.items():\n",
        "        _, word_loss = encode_word(word, model)\n",
        "        loss += freq * word_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV7sct06wr9H"
      },
      "source": [
        "We can check it works on the model we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw39xVJRwr9H",
        "outputId": "426245ea-0dcb-4c06-ba36-a8e1c1b084a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "413.10377642940875"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "compute_loss(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK6SBm7bwr9H"
      },
      "source": [
        "Computing the <font color='blue'>scores</font> for <font color='blue'>each token</font> is not very hard either; we just have to <font color='blue'>compute</font> the <font color='blue'>loss</font> for the <font color='blue'>models</font> obtained by deleting each token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Km3RO_o4wr9H"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def compute_scores(model):\n",
        "    scores = {}\n",
        "    model_loss = compute_loss(model)\n",
        "    for token, score in model.items():\n",
        "        # We always keep tokens of length 1\n",
        "        if len(token) == 1:\n",
        "            continue\n",
        "        model_without_token = copy.deepcopy(model)\n",
        "        _ = model_without_token.pop(token)\n",
        "        scores[token] = compute_loss(model_without_token) - model_loss\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIlI7gQ_wr9H"
      },
      "source": [
        "We can try it on a <font color='blue'>given token</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd9ycLO8wr9H",
        "outputId": "08efb395-98c9-4dbf-e6c6-085e7536566b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.376412403623874\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "scores = compute_scores(model)\n",
        "print(scores[\"ll\"])\n",
        "print(scores[\"his\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTyS0OPMwr9H"
      },
      "source": [
        "Since `\"ll\"` is used in the tokenization of `\"Hopefully\"`, and removing it will probably make us use the token `\"l\"` twice instead, we expect it will have a positive loss. `\"his\"` is only used inside the word `\"This\"`, which is tokenized as itself, so we expect it to have a zero loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSHMGmNqwr9H"
      },
      "source": [
        "üí° **Tip:** This approach is <font color='blue'>very inefficient</font>, so <font color='blue'>SentencePiece</font> uses an <font color='blue'>approximation</font> of the <font color='blue'>loss of the model</font> without token X: <font color='blue'>instead</font> of <font color='blue'>starting from scratch</font>, it just replaces token X by its segmentation in the vocabulary that is left. This way, all the scores can be computed at once at the same time as the model loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOFEfRXSwr9I"
      },
      "source": [
        "With all of this in place, the last thing we need to do is <font color='blue'>add</font> the <font color='blue'>special tokens</font> used by the model <font color='blue'>to the vocabulary</font>, then <font color='blue'>loop</font> until we have <font color='blue'>pruned enough tokens</font> from the <font color='blue'>vocabulary</font> to reach our desired size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "03BjkgAawr9I"
      },
      "outputs": [],
      "source": [
        "percent_to_remove = 0.1\n",
        "while len(model) > 100:\n",
        "    scores = compute_scores(model)\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
        "    # Remove percent_to_remove tokens with the lowest scores.\n",
        "    for i in range(int(len(model) * percent_to_remove)):\n",
        "        _ = token_freqs.pop(sorted_scores[i][0])\n",
        "\n",
        "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmZAPZh3wr9I"
      },
      "source": [
        "Then, to <font color='blue'>tokenize some text</font>, we just need to <font color='blue'>apply</font> the <font color='blue'>pre-tokenization</font> and then use our `encode_word()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2yyT3Stwr9I",
        "outputId": "b2092284-d6c6-4c0c-d9cf-89e906920a2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‚ñÅThis',\n",
              " '‚ñÅis',\n",
              " '‚ñÅthe',\n",
              " '‚ñÅHugging',\n",
              " '‚ñÅFace',\n",
              " '‚ñÅ',\n",
              " 'c',\n",
              " 'ou',\n",
              " 'r',\n",
              " 's',\n",
              " 'e',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "def tokenize(text, model):\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
        "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "tokenize(\"This is the Hugging Face course.\", model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfg7J1-Twr9I"
      },
      "source": [
        "**Tip:** The XLNetTokenizer uses SentencePiece which is why the `\"_\"` character is included. To decode with SentencePiece, concatenate all the tokens and replace `\"_\"` with a space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SQqKq8Twr9I"
      },
      "source": [
        "That's it for Unigram! Hopefully by now you're feeling like an expert in all things tokenizer. In the next section, we will delve into the building blocks of the ü§ó Tokenizers library, and show you how you can use them to build your own tokenizer."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31dfbf281f3c4de2bdeea2bc149b0660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_3d40fcab598143988f81ecc37f2840b7"
          }
        },
        "1c9a51238bd5498dbb1cb2ffd85eb564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a90d855f554f498320f4bb6678a50a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7910cca465f040539a2d887e95e7ebc1",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "c5150fa6719d44df8c6f4d9f6dc436a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4262a2f7c30e4227862c2b488dcb0773",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0162262f478e4d4a87984beb21263fb0",
            "value": ""
          }
        },
        "c6173a9738e94763aeecf824bf5bab74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_904127ff299b4bab951c149938fe85e7",
            "style": "IPY_MODEL_d49d7de3377c4b1a9c02f9378d5e2c2b",
            "value": true
          }
        },
        "a0f2b57d9977474da50cf4e7d85ccfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ea17b9ab08ea4a5495871d79b5bfbd50",
            "style": "IPY_MODEL_9f2fcc31113a4966a7a702e0363de186",
            "tooltip": ""
          }
        },
        "f5bdaac00a4442a3b1fb46d1f3f98e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce83492480d240c98a10cea0875f1a46",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_99be669de5bb408fbad02ff081655be1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3d40fcab598143988f81ecc37f2840b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "13a90d855f554f498320f4bb6678a50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7910cca465f040539a2d887e95e7ebc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4262a2f7c30e4227862c2b488dcb0773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0162262f478e4d4a87984beb21263fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "904127ff299b4bab951c149938fe85e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d49d7de3377c4b1a9c02f9378d5e2c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea17b9ab08ea4a5495871d79b5bfbd50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2fcc31113a4966a7a702e0363de186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ce83492480d240c98a10cea0875f1a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99be669de5bb408fbad02ff081655be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5018984030b64a20ad38848816f38854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4adddcf18345c7baadfeb0a5cfa89b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f5a328cc8e1040cf8fd1240aa0244625",
            "value": "Connecting..."
          }
        },
        "8d4adddcf18345c7baadfeb0a5cfa89b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a328cc8e1040cf8fd1240aa0244625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}