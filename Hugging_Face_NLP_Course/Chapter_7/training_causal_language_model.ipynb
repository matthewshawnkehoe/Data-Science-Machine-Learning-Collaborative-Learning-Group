{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vId2l9HeDqXO"
      },
      "source": [
        "# Training a causal language model from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfviSjtTKQew"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_koMGBGp3T1C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1uHGJaq3VoQ"
      },
      "source": [
        "Also log into Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "d328f9a6e29949388d1b90ecb4e230d8",
            "7c6e6ae38e57497e976667da061484af",
            "90cfb0064a9f440f812ca9f88fb2d546",
            "d66f0f9617634f109a452ea2bb843d46",
            "f3295472f6e34c4bbc0ffde2ec1db9ae",
            "35dce967953c48efa54cd68183234d99",
            "cb0d388468b84343aa0aebd83b1c6b96",
            "7cf1e0e525214e798a4ebdd0de74470b",
            "8f0009281b77480180310a2bffe57ea5",
            "f435dce012c8416bb29962f2fd3e6f06",
            "807094d4de41498daa9c203727fc4d14",
            "93853a1abfc94273960ec274b021e231",
            "2d044551d92a4fc0a08493c3d5d27cc5",
            "744b907a70f245b0bf0b2285c9f12ad5",
            "672c413f4c664e1fbd18a9aa341fb0d2",
            "19d973f8a2f442da918cab44a5355f74",
            "9697644ceeb2454f9bf75d0c44d175a3",
            "2b525051cfe64e18b364cc2f015b0b1a",
            "f32f61d2dc9f46429265cd447eb7df8b",
            "fb2970b4751a45ab956a0b1a19c2b442"
          ]
        },
        "id": "3EbHpu4c3XqP",
        "outputId": "7f75b67a-105f-42e9-c771-8f2cad79ed6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d328f9a6e29949388d1b90ecb4e230d8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up until now, we've mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in Chapter 1, this is commonly referred to as _transfer learning_, and it's a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we'll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub's Copilot, powered by OpenAI's Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2.\n",
        "\n",
        "In this section we will build a scaled-down version of a code generation model: we'll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the `matplotlib`, `seaborn`, `pandas`, and `scikit-learn` libraries. When using those frameworks it's common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us."
      ],
      "metadata": {
        "id": "HveSx-KYDzf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cOrUI9sDqXR"
      },
      "source": [
        "**Video Tutorial:** [Watch on YouTube](https://www.youtube.com/watch?v=Vpjb1lu0MDk)\n",
        "\n",
        "In Chapter 6 we created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, we'll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the `Trainer` API and ðŸ¤— Accelerate to train the model. Let's get to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VexgVRa9DqXS"
      },
      "source": [
        "**Demo:** This showcases the model that was trained and uploaded to the Hub using the code shown in this section. You can find it [here](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Note that since there is some randomization happening in the text generation, you will probably get a slightly different result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l7M6BsADqXT"
      },
      "source": [
        "## Gathering the data\n",
        "\n",
        "Python code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called `codeparrot`, the authors built a dataset that they then shared on the [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot).\n",
        "\n",
        "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let's start by filtering the `codeparrot` dataset for all files that include any of the libraries in this stack. Because of the dataset's size, we want to avoid downloading it; instead, we'll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, we'll use the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ENPavdh2DqXT"
      },
      "outputs": [],
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword in string:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44bUGpJDqXU"
      },
      "source": [
        "Let's test it on two examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KtWJ27BDqXV",
        "outputId": "7be643d1-1ac3-4f9c-a203-841148513f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False True\n"
          ]
        }
      ],
      "source": [
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "example_1 = \"import numpy as np\"\n",
        "example_2 = \"import pandas as pd\"\n",
        "\n",
        "print(\n",
        "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Koma-JDqXW"
      },
      "source": [
        "We can use this to create a function that will stream the dataset and filter the elements we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7O-e1IL4DqXX"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "    filtered_dict = defaultdict(list)\n",
        "    total = 0\n",
        "    for sample in tqdm(iter(dataset)):\n",
        "        total += 1\n",
        "        if any_keyword_in_string(sample[\"content\"], filters):\n",
        "            for k, v in sample.items():\n",
        "                filtered_dict[k].append(v)\n",
        "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "    return Dataset.from_dict(filtered_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR12Kk7XDqXX"
      },
      "source": [
        "Then we can simply apply this function to the streaming dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9hzNjkQDqXY"
      },
      "outputs": [],
      "source": [
        "# This cell will take a very long time to execute, so you should skip it and go to\n",
        "# the next one!\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# split = \"train\"  # \"valid\"\n",
        "# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "\n",
        "# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
        "# filtered_data = filter_streaming_dataset(data, filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRPGdQ5ZDqXY"
      },
      "source": [
        "Output:\n",
        "```\n",
        "3.26% of data after filtering.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwZct0MeDqXY"
      },
      "source": [
        "This leaves us with about 3% of the original dataset, which is still quite sizable -- the resulting dataset is 6 GB and consists of 600,000 Python scripts!\n",
        "\n",
        "Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you don't want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbjjHUXuDqXY",
        "outputId": "00795949-f56b-4eb8-ccaa-ffb7c4c2f92e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 606720\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 3322\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8nRdmzDqXZ"
      },
      "source": [
        "> **TIP:** Pretraining the language model will take a while. We suggest that you first run the training loop on a sample of the data by uncommenting the two partial lines above, and make sure that the training successfully completes and the models are stored. Nothing is more frustrating than a training run failing at the last step because you forgot to create a folder or because there's a typo at the end of the training loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGO52kSeDqXZ"
      },
      "source": [
        "Let's look at an example from the dataset. We'll just show the first 200 characters of each field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rScBxLKsDqXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e749ca-3ae8-4dbf-897e-7d75368880b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPO_NAME: kmike/scikit-learn\n",
            "PATH: sklearn/utils/__init__.py\n",
            "COPIES: 3\n",
            "SIZE: 10094\n",
            "CONTENT: \"\"\"\n",
            "The :mod:`sklearn.utils` module includes various utilites.\n",
            "\"\"\"\n",
            "\n",
            "from collections import Sequence\n",
            "\n",
            "import numpy as np\n",
            "from scipy.sparse import issparse\n",
            "import warnings\n",
            "\n",
            "from .murmurhash import murm\n",
            "LICENSE: bsd-3-clause\n"
          ]
        }
      ],
      "source": [
        "for key in raw_datasets[\"train\"][0]:\n",
        "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JicKix4VDqXa"
      },
      "source": [
        "We can see that the `content` field contains the code that we want our model to train on. Now that we have a dataset, we need to prepare the texts so they're in a format suitable for pretraining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRB0wJHdDqXa"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "**Video Tutorial:** [Watch on YouTube](https://www.youtube.com/watch?v=ma1TrR7gE7I)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlceiwXODqXa"
      },
      "source": [
        "The first step is to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. The benefit of this is that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, let's fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively.\n",
        "\n",
        "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we'll use the `return_overflowing_tokens` option to tokenize the whole input and split it into several chunks, as we did in [Chapter 6](/course/chapter6/4). We'll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we'll get rid of these pieces to avoid padding issues; we don't really need them as we have plenty of data anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv-m39RnDqXa"
      },
      "source": [
        "![Visualizing how text is split into several chunks by tokenizing with overflowing tokens.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoW3KadWDqXb"
      },
      "source": [
        "Let's see exactly how this works by looking at the first two examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f28F6g6iDqXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db9ea7a-2dd7-4500-ec2d-363e8ba030a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs length: 34\n",
            "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
            "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"content\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORnxymquDqXb"
      },
      "source": [
        "We can see that we get 34 examples in total from these two examples. Looking at the chunk lengths, we can see that the chunks at the ends of both documents have less than 128 tokens (117 and 41, respectively). These represent just a small fraction of the total chunks that we have, so we can safely throw them away. With the `overflow_to_sample_mapping` field, we can also reconstruct which chunks belonged to which input samples.\n",
        "\n",
        "With this operation we're using a handy feature of the `Dataset.map()` function in ðŸ¤— Datasets, which is that it does not require one-to-one maps; as we saw in [Section 3](/course/chapter7/3), we can create batches with more or fewer elements than the input batch. This is useful when doing operations like data augmentation or data filtering that change the number of elements. In our case, when tokenizing each element into chunks of the specified context size, we create many samples from each document. We just need to make sure to delete the existing columns, since they have a conflicting size. If we wanted to keep them, we could repeat them appropriately and return them within the `Dataset.map()` call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VHamJ-NKDqXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "2b793c04f5e34d49912eeae2772d504d",
            "ffae77f034aa45948e81f6080caddd39",
            "1851115875b64faa8a6bca63243e4f9d",
            "030f0a63e7c7459ea8c8da43204e20af",
            "3b9442b0784445ddbded8b548ac713e4",
            "d4842a09a1394cd99580cef44d77dde8",
            "3964099e07ee4fc1abb5d5c44190180f",
            "89d697f9877d463a865e764b5c8fec51",
            "22e59bbdbb9e4f6898a7073ba85370ec",
            "46acbc9b78d146aeaa49ad90263465a5",
            "d715e14177af49afa84138476239c99e",
            "b7550a9c2d744762b9b3e15d1769f4f2",
            "6dd6d1930da1474e88e64ddc5282577d",
            "22c528937e2f442e9e30243de379ce18",
            "223e30f4edb84db1b821db5ecfee9ec2",
            "ebbd3c5c746c496c8d02f0668a1c548e",
            "d05e07604ce2404abf7f9111286e53a1",
            "37524eb832c749ff9191238e8b9fb1e6",
            "b9aacb5e38bd45da881264c0e8a7bad7",
            "34dd4b683cd748648f679d4dde0f2793",
            "e3c492308da24090b68a4ee0b60cab58",
            "db6a1f68c47f4b888d684e50132651d0"
          ]
        },
        "outputId": "54f68e60-afec-4a3e-f7ca-b3eda5c82e52"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b793c04f5e34d49912eeae2772d504d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/606720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3322 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7550a9c2d744762b9b3e15d1769f4f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 16702061\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 93164\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGeJ6K02DqXd"
      },
      "source": [
        "Now we have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAI's GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists.\n",
        "\n",
        "Now that we have the dataset ready, let's set up the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV9CLl42DqXd"
      },
      "source": [
        "## Initializing a new model\n",
        "\n",
        "**Video Tutorial:** [Watch on YouTube](https://www.youtube.com/watch?v=3AGJoa38IIA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5O-i4RDqXd"
      },
      "source": [
        "Our first step is to freshly initialize a GPT-2 model. We'll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the `bos` and `eos` (beginning and end of sequence) token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k1IGV0h1DqXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7175c874d5d34d48a28f5e0cde0bd323",
            "b950669688c041f19c8f88e7b7107ce9",
            "0c69680418374541b9e8c4043e1f545c",
            "57a1c7575f8b4bdcaf9e1ec724381121",
            "232a2bccc5254bfb92214dd18eb80c69",
            "1f3b2c179a7346d4a742301213d633ca",
            "5dc8ee4e7b144da1918acfc86e98d3ef",
            "b6e8a259d74d4d9091a9d72ff8f5c75f",
            "ed2aa67c3f494eaa9e951aa1b39324d0",
            "74e19e35e32943819de8778f3d6bc070",
            "2aa6db6342d14790b7653a5c59dcd8e7"
          ]
        },
        "outputId": "cb76074a-f2c0-4e6f-b7e8-cfa34891a631"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7175c874d5d34d48a28f5e0cde0bd323"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dsg1bxuDqXd"
      },
      "source": [
        "With that configuration, we can load a new model. Note that this is the first time we don't use the `from_pretrained()` function, since we're actually initializing a model ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WJjwrvRhDqXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f6242d-af74-4460-f1d4-201e99bfed31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 size: 124.2M parameters\n"
          ]
        }
      ],
      "source": [
        "model = GPT2LMHeadModel(config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75QOwkz_DqXe"
      },
      "source": [
        "Our model has 124M parameters that we'll have to tune. Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels -- in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don't need to duplicate the `input_ids`.\n",
        "\n",
        "Note that `DataCollatorForLanguageModeling` supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument `mlm=False`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wObjT2gPDqXe"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QYKKlbuDqXe"
      },
      "source": [
        "Let's have a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aLufAFa8DqXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3d6690-1be7-40df-b451-c61ad9cd4942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([5, 128])\n",
            "attention_mask shape: torch.Size([5, 128])\n",
            "labels shape: torch.Size([5, 128])\n"
          ]
        }
      ],
      "source": [
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5nFNPdSDqXf"
      },
      "source": [
        "We can see the data collator has stacked the examples and applied padding (all of our examples are of the same length, so no actual padding is needed in this case), and all is ready for training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfMNi2tEDqXf"
      },
      "source": [
        "## Training with the `Trainer` API\n",
        "\n",
        "Training is now super easy -- we just need to define a few training arguments and we're good to go. We'll use cosine learning rate schedule with some warmup and an effective batch size of 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). Gradient accumulation is used when a single batch does not fit in memory, and incrementally builds up the gradient through several forward/backward passes. We'll see this in action when we create the training loop with ðŸ¤— Accelerate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OO9E_L9mDqXf"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"codeparrot-ds\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=5_000,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    report_to=\"none\"  # Don't rely on wandb\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVhmOPaFDqXg"
      },
      "source": [
        "Now we can just train our model by calling the `train()` method on our trainer. This will take a while -- grab a coffee!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwVZw2-4DqXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "91e498d1-90d5-4281-84a4-d0d71c4d7045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3486' max='65243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3486/65243 31:18 < 9:15:04, 1.85 it/s, Epoch 0.05/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmG7CJmFDqXg"
      },
      "source": [
        "Note that during the training you can see the loss reported. Once training is finished, we can push the model and tokenizer to the Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3dJbDqSDqXg"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebMwPMhWDqXh"
      },
      "source": [
        "> **TIP:** If you want to pause training and resume later, use the `trainer.train(resume_from_checkpoint=True)` argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_53gvv9sDqXh"
      },
      "source": [
        "## Code generation with a pipeline\n",
        "\n",
        "**Video Tutorial:** [Watch on YouTube](https://www.youtube.com/watch?v=J-E6i01Ujbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXkJt_9DqXh"
      },
      "source": [
        "Now is the moment of truth: let's see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let's take a look at how well it works on some prompts. To do that we'll wrap the model in a text generation `pipeline`, and we'll put it on the GPU for fast generations if there is one available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH7abCWYDqXh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoCD0PiEDqXh"
      },
      "source": [
        "Let's start with the simple task of creating a scatter plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0eDxPutDqXi"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create scatter plot with x, y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCychw-DqXi"
      },
      "source": [
        "The result looks correct. Does it also work for a `pandas` operation? Let's see if we can create a `DataFrame` from two arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rkQQDAbDqXi"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create dataframe from x and y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOKkCzTgDqXj"
      },
      "source": [
        "Nice, that's the correct answer -- although it then inserts the column `x` again. Since the number of generated tokens is limited, the following `for` loop is cut off. Let's see if we can do something a bit more complex and have the model help us use the `groupby` operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0wATeTVDqXj"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# dataframe with profession, income and name\n",
        "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
        "\n",
        "# calculate the mean income per profession\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV7YYa6fDqXj"
      },
      "source": [
        "Not bad; that's the right way to do it. Finally, let's see if we can also use it for a `scikit-learn` and set up a Random Forest model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhOl_rmrDqXk"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\n",
        "# import random forest regressor from scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# fit random forest model with 300 estimators on X, y:\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yQR5mkODqXl"
      },
      "source": [
        "Looking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however, as we'll see next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-jg9OwpDqXl"
      },
      "source": [
        "## Training with ðŸ¤— Accelerate\n",
        "\n",
        "**Video Tutorial:** [Watch on YouTube](https://www.youtube.com/watch?v=SJqC51CDDUg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAqLoiFFDqXl"
      },
      "source": [
        "We've seen how to train a model with the `Trainer`, which can allow for some customization. Sometimes, however, we want full control over the training loop, or we want to make some exotic changes. In this case ðŸ¤— Accelerate is a great choice, and in this section we'll go through the steps to use it to train our model. To make things more interesting, we'll also add a twist to the training loop.\n",
        "\n",
        "Since we're mainly interested in sensible autocompletion for the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these samples by using the list of keywords that we used to filter the dataset as the list of words we want to overweight in the loss. Since we might also want to do this for other libraries or domains, let's write a function that will create this `keytoken_ids` list for us. The first things we need are the tokenizer and the list of keywords we care about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa-FtQk_DqXl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"huggingface-course/codeparrot-ds\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-798q70tDqXl"
      },
      "outputs": [],
      "source": [
        "keytoken_ids = []\n",
        "for keyword in [\n",
        "    \"plt\",\n",
        "    \"pd\",\n",
        "    \"sk\",\n",
        "    \"fit\",\n",
        "    \"predict\",\n",
        "    \" plt\",\n",
        "    \" pd\",\n",
        "    \" sk\",\n",
        "    \"testtest\",\n",
        "]:\n",
        "    ids = tokenizer([keyword]).input_ids[0]\n",
        "    if len(ids) == 1:\n",
        "        keytoken_ids.append(ids[0])\n",
        "    else:\n",
        "        print(f\"Keyword has not single token: {keyword}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBEe2LKODqXm"
      },
      "source": [
        "Great, that seems to work nicely! We included the `testtest` token to double-check that the function filters out keywords that comprise multiple tokens. Now let's wrap this in a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyxMzEjfDqXm"
      },
      "outputs": [],
      "source": [
        "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    # Calculate per-token loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    # Resize and average loss per sample\n",
        "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
        "    # Calculate and scale weighting\n",
        "    weights = torch.stack([(shift_labels == kt).float() for kt in keytoken_ids]).sum(\n",
        "        axis=[0, 2]\n",
        "    )\n",
        "    weights = alpha * (1.0 + weights)\n",
        "    # Calculate weighted average\n",
        "    weighted_loss = (loss_per_sample * weights).mean()\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwK4NMgADqXm"
      },
      "source": [
        "Before we jump into defining our training loop with this fantastic new loss function, we need to prepare a few things:\n",
        "\n",
        "- We need dataloaders to load the data in batches.\n",
        "- We need to set up the weight decay parameters.\n",
        "- From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function.\n",
        "\n",
        "Let's start with the dataloaders. We only need to set the format of the dataset to `\"torch\"`, and then we can pass it to a PyTorch `DataLoader` with the appropriate batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqOp0OV8DqXm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQ_WJ8IDqXn"
      },
      "source": [
        "Next, we group the parameters so that the optimizer knows which ones will get an extra weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; here's how we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sariu9YrDqXn"
      },
      "outputs": [],
      "source": [
        "weight_decay = 0.1\n",
        "\n",
        "\n",
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "    params_with_wd, params_without_wd = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(nd in n for nd in no_decay):\n",
        "            params_without_wd.append(p)\n",
        "        else:\n",
        "            params_with_wd.append(p)\n",
        "    return [\n",
        "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
        "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8tvdqtDqXn"
      },
      "source": [
        "Since we want to evaluate the model regularly on the validation set during training, let's write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0ByOjBUDqXn"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
        "\n",
        "        losses.append(accelerator.gather(outputs.loss))\n",
        "    loss = torch.mean(torch.cat(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    return loss.item(), perplexity.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEa87yZDqXn"
      },
      "source": [
        "With the `evaluate()` function we can report loss and [perplexity](/course/chapter7/3) at regular intervals. Next, we redefine our model to make sure we train from scratch again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHo0Ot98DqXn"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw0ZPlp1DqXn"
      },
      "source": [
        "We can then define our optimizer, using the function from before to split the parameters for weight decay:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIuwun6oDqXn"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AaqtWYrDqXn"
      },
      "source": [
        "Now let's prepare the model, optimizer, and dataloaders so we can start training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAk_lA6dDqXp"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(fp16=True)\n",
        "\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4iq7WSvDqXp"
      },
      "source": [
        "> **TIP:** ðŸš¨ If you're training on a TPU, you'll need to move all the code starting at the cell above into a dedicated training function. See [Chapter 3](/course/chapter3) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TkdByacDqXp"
      },
      "source": [
        "Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq1x3mouDqXp"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 1\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1cNUZqfDqXq"
      },
      "source": [
        "Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren't logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA3mld10DqXq"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"codeparrot-ds-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73YrVCLPDqXq"
      },
      "source": [
        "Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9fkgGzQDqXr"
      },
      "outputs": [],
      "source": [
        "output_dir = \"codeparrot-ds-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGj0IKdMDqXr"
      },
      "source": [
        "We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.\n",
        "\n",
        "Before we train, let's run a quick test to see if the evaluation function works properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwwzSl0kDqXr"
      },
      "outputs": [],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYEn6KQbDqXr"
      },
      "source": [
        "Those are very high values for loss and perplexity, but that's not surprising as we haven't trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new `evaluate()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkumPUp1DqXs"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "gradient_accumulation_steps = 8\n",
        "eval_steps = 5_000\n",
        "\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for epoch in range(num_train_epochs):\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
        "    ):\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
        "        if step % 100 == 0:\n",
        "            accelerator.print(\n",
        "                {\n",
        "                    \"samples\": step * samples_per_step,\n",
        "                    \"steps\": completed_steps,\n",
        "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
        "                }\n",
        "            )\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "        if step % gradient_accumulation_steps == 0:\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
        "            eval_loss, perplexity = evaluate()\n",
        "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
        "            model.train()\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "            if accelerator.is_main_process:\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                repo.push_to_hub(\n",
        "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMACiAibDqXs"
      },
      "source": [
        "And that's it -- you now have your own custom training loop for causal language models such as GPT-2 that you can further customize to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BUatZhoDqXt"
      },
      "source": [
        "> **TIP:** âœï¸ **Try it out!** Either create your own custom loss function tailored to your use case, or add another custom step into the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function for Syntax"
      ],
      "metadata": {
        "id": "i3Tf1qulr0Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syntax_aware_loss(inputs, logits, tokenizer, alpha=2.0):\n",
        "    \"\"\"\n",
        "    Custom loss function that weights tokens differently based on their syntactic importance.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input token IDs\n",
        "        logits: Model logits\n",
        "        tokenizer: Tokenizer to decode tokens\n",
        "        alpha: Weight multiplier for important tokens (default: 2.0)\n",
        "\n",
        "    Returns:\n",
        "        Weighted loss value\n",
        "    \"\"\"\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "\n",
        "    # Calculate per-token loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "    # Reshape loss to [batch_size, seq_len]\n",
        "    loss = loss.view(shift_logits.size(0), shift_logits.size(1))\n",
        "\n",
        "    weights = torch.ones_like(loss)\n",
        "\n",
        "    for i in range(inputs.size(0)):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[i].tolist())\n",
        "\n",
        "        for j in range(len(tokens) - 1):\n",
        "            # Weight tokens by method calls\n",
        "            if tokens[j] == '.':\n",
        "                weights[i, j] *= alpha\n",
        "\n",
        "            # Weight tokens by function arguments\n",
        "            if tokens[j] == '(':\n",
        "                weights[i, j] *= alpha\n",
        "\n",
        "            # Weight tokens by assignments\n",
        "            if '=' in tokens[j] and '==' not in tokens[j]:\n",
        "                weights[i, max(0, j-1):min(len(tokens)-1, j+2)] *= alpha\n",
        "\n",
        "            # Weight import-related tokens\n",
        "            if tokens[j] in ['import', 'from']:\n",
        "                weights[i, j:min(len(tokens)-1, j+3)] *= alpha\n",
        "\n",
        "    # Calculate weighted loss\n",
        "    weighted_loss = (loss * weights).sum() / weights.sum()\n",
        "\n",
        "    return weighted_loss"
      ],
      "metadata": {
        "id": "lTVtrcfOr7fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function for Imbalanced Token Distribution"
      ],
      "metadata": {
        "id": "vSrz8twGqcKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(inputs, logits, gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Focal Loss implementation for language modeling.\n",
        "    Helps model focus on hard-to-predict tokens.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input token IDs\n",
        "        logits: Model logits\n",
        "        gamma: Focusing parameter (higher = more focus on hard examples)\n",
        "        alpha: Weighting factor\n",
        "\n",
        "    Returns:\n",
        "        Focal loss value\n",
        "    \"\"\"\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "\n",
        "    # Calculate cross entropy loss\n",
        "    ce_loss = torch.nn.functional.cross_entropy(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1),\n",
        "        reduction='none'\n",
        "    )\n",
        "\n",
        "    probs = torch.softmax(shift_logits, dim=-1)\n",
        "    # Get probability of correct class\n",
        "    pt = probs.view(-1, probs.size(-1)).gather(1, shift_labels.view(-1, 1)).squeeze(1)\n",
        "\n",
        "    focal_weight = (1 - pt) ** gamma\n",
        "    focal_loss = alpha * focal_weight * ce_loss\n",
        "\n",
        "    return focal_loss.mean()"
      ],
      "metadata": {
        "id": "4yL0Ms5OqGic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamically adjust gradient clipping based on loss magnitude."
      ],
      "metadata": {
        "id": "nl-r33TxqnP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_gradient_clip(model, loss, base_clip_value=1.0, loss_threshold=5.0):\n",
        "    \"\"\"\n",
        "    Dynamically adjust gradient clipping based on loss magnitude so that\n",
        "    higher losses get more aggressive clipping.\n",
        "\n",
        "    Args:\n",
        "        model: The model being trained\n",
        "        loss: Current loss value\n",
        "        base_clip_value: Base clipping value\n",
        "        loss_threshold: Loss value above which to increase clipping\n",
        "\n",
        "    Returns:\n",
        "        Actual clip value used\n",
        "    \"\"\"\n",
        "    if loss.item() > loss_threshold:\n",
        "        clip_value = base_clip_value * 0.5\n",
        "    else:\n",
        "        clip_value = base_clip_value\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "    return clip_value"
      ],
      "metadata": {
        "id": "JgEswIHDqxab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **TIP:** âœï¸ **Try it out!** When running long training experiments it's a good idea to log important metrics using tools such as TensorBoard or Weights & Biases. Add proper logging to the training loop so you can always check how the training is going."
      ],
      "metadata": {
        "id": "OfGNzA7jqmI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorBoard Logging"
      ],
      "metadata": {
        "id": "94I2E-Xbq75R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboard\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "log_dir = f\"runs/codeparrot_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
        "print(f\"To view: tensorboard --logdir={log_dir}\")"
      ],
      "metadata": {
        "id": "9Subs1GJqmms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics_tensorboard(writer, metrics, step, prefix=\"train\"):\n",
        "    \"\"\"\n",
        "    Log metrics to TensorBoard.\n",
        "\n",
        "    Args:\n",
        "        writer: TensorBoard SummaryWriter\n",
        "        metrics: Dictionary of metrics to log\n",
        "        step: Current training step\n",
        "        prefix: Metric prefix (train/eval)\n",
        "    \"\"\"\n",
        "    for key, value in metrics.items():\n",
        "        writer.add_scalar(f\"{prefix}/{key}\", value, step)\n",
        "\n",
        "\n",
        "def log_gradients_tensorboard(writer, model, step):\n",
        "    \"\"\"\n",
        "    Log gradient statistics to TensorBoard.\n",
        "    \"\"\"\n",
        "    total_norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "\n",
        "            # Log individual layer gradients\n",
        "            writer.add_scalar(f\"gradients/{name}\", param_norm, step)\n",
        "\n",
        "    total_norm = total_norm ** 0.5\n",
        "    writer.add_scalar(\"gradients/total_norm\", total_norm, step)\n",
        "    return total_norm"
      ],
      "metadata": {
        "id": "VddPS3iHrD-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights & Biases Logging"
      ],
      "metadata": {
        "id": "2NK0QnrNrK6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.init(\n",
        "    project=\"codeparrot-training\",\n",
        "    name=\"codeparrot-ds-custom\",\n",
        "    config={\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"epochs\": 1,\n",
        "        \"batch_size\": 32,\n",
        "        \"gradient_accumulation_steps\": 8,\n",
        "        \"context_length\": 128,\n",
        "        \"weight_decay\": 0.1,\n",
        "        \"warmup_steps\": 1_000,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Watch model (logs gradients and parameters)\n",
        "wandb.watch(model, log=\"all\", log_freq=100)"
      ],
      "metadata": {
        "id": "O9UY1Ec0rLQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics_wandb(metrics, step, prefix=\"train\"):\n",
        "    \"\"\"\n",
        "    Log metrics to Weights & Biases.\n",
        "\n",
        "    Args:\n",
        "        metrics: Dictionary of metrics to log\n",
        "        step: Current training step\n",
        "        prefix: Metric prefix (train/eval)\n",
        "    \"\"\"\n",
        "    log_dict = {f\"{prefix}/{k}\": v for k, v in metrics.items()}\n",
        "    log_dict[\"step\"] = step\n",
        "    wandb.log(log_dict)"
      ],
      "metadata": {
        "id": "yQthJRnGrNf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop and Logging adding\n",
        "\n",
        "- Custom loss functions\n",
        "- TensorBoard/W&B logging\n",
        "- Gradient monitoring\n",
        "- Learning rate tracking\n",
        "- Token accuracy calculation"
      ],
      "metadata": {
        "id": "qY7Nic67rQra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "gradient_accumulation_steps = 8\n",
        "eval_steps = 5_000\n",
        "log_steps = 100\n",
        "use_wandb = True  # Set to False to use TensorBoard instead\n",
        "\n",
        "samples_per_step = (\n",
        "    accelerator.state.num_processes *\n",
        "    args.per_device_train_batch_size *\n",
        "    gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=1),\n",
        "        total=len(train_dataloader),\n",
        "        desc=f\"Epoch {epoch + 1}\"\n",
        "    ):\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "\n",
        "        # Choose loss function\n",
        "        # Option 1: Key token weighted loss\n",
        "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
        "\n",
        "        # Option 2: Syntax-aware loss\n",
        "        # loss = syntax_aware_loss(batch[\"input_ids\"], logits, tokenizer, alpha=2.0)\n",
        "\n",
        "        # Option 3: Focal loss\n",
        "        # loss = focal_loss(batch[\"input_ids\"], logits, gamma=2.0)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate token accuracy\n",
        "        with torch.no_grad():\n",
        "            predictions = torch.argmax(logits[:, :-1, :], dim=-1)\n",
        "            targets = batch[\"input_ids\"][:, 1:]\n",
        "            accuracy = (predictions == targets).float().mean().item()\n",
        "\n",
        "        if step % log_steps == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            metrics = {\n",
        "                \"loss\": loss.item() * gradient_accumulation_steps,\n",
        "                \"learning_rate\": current_lr,\n",
        "                \"accuracy\": accuracy,\n",
        "                \"samples\": step * samples_per_step,\n",
        "                \"epoch\": epoch,\n",
        "            }\n",
        "\n",
        "            if use_wandb:\n",
        "                log_metrics_wandb(metrics, completed_steps, prefix=\"train\")\n",
        "            else:\n",
        "                log_metrics_tensorboard(writer, metrics, completed_steps, prefix=\"train\")\n",
        "\n",
        "            accelerator.print(\n",
        "                f\"Step {step} | Loss: {metrics['loss']:.4f} | \"\n",
        "                f\"Accuracy: {accuracy:.4f} | LR: {current_lr:.2e}\"\n",
        "            )\n",
        "\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        # Optimizer step with gradient accumulation\n",
        "        if step % gradient_accumulation_steps == 0:\n",
        "            # Log gradients before clipping\n",
        "            if step % (log_steps * gradient_accumulation_steps) == 0:\n",
        "                if use_wandb:\n",
        "                    pass\n",
        "                else:\n",
        "                    grad_norm = log_gradients_tensorboard(writer, model, completed_steps)\n",
        "\n",
        "            # Gradient clipping\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "\n",
        "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
        "            eval_loss, perplexity = evaluate()\n",
        "\n",
        "            eval_metrics = {\n",
        "                \"loss\": eval_loss,\n",
        "                \"perplexity\": perplexity,\n",
        "            }\n",
        "\n",
        "            if use_wandb:\n",
        "                log_metrics_wandb(eval_metrics, completed_steps, prefix=\"eval\")\n",
        "            else:\n",
        "                log_metrics_tensorboard(writer, eval_metrics, completed_steps, prefix=\"eval\")\n",
        "\n",
        "            accelerator.print(\n",
        "                f\"\\n{'='*50}\\n\"\n",
        "                f\"Evaluation | Loss: {eval_loss:.4f} | Perplexity: {perplexity:.2f}\\n\"\n",
        "                f\"{'='*50}\\n\"\n",
        "            )\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            # Save\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "\n",
        "            if accelerator.is_main_process:\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                repo.push_to_hub(\n",
        "                    commit_message=f\"Training in progress step {step}\",\n",
        "                    blocking=False\n",
        "                )\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    accelerator.print(\n",
        "        f\"\\nEpoch {epoch + 1} completed | Average Loss: {avg_epoch_loss:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.log({\"epoch_avg_loss\": avg_epoch_loss, \"epoch\": epoch + 1})\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.finish()\n",
        "else:\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "Q6u-Po6YrViP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing Your Logs. For TensorBoard:"
      ],
      "metadata": {
        "id": "2ojAE-Axs2qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=runs/"
      ],
      "metadata": {
        "id": "lVLnCB3XtOwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then open your browser to `http://localhost:6006`"
      ],
      "metadata": {
        "id": "7pHEHN45tMHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Weights & Biases: the training run will be automatically synced to your W&B account. Go to [wandb.ai](https://wandb.ai) to view interactive dashboards with:\n",
        "- Real-time training metrics\n",
        "- System metrics (GPU/CPU usage, memory)\n",
        "- Model architecture visualization\n",
        "- Hyperparameter comparison across runs\n",
        "- Code versioning"
      ],
      "metadata": {
        "id": "km4-swh_tTUB"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d328f9a6e29949388d1b90ecb4e230d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cb0d388468b84343aa0aebd83b1c6b96"
          }
        },
        "7c6e6ae38e57497e976667da061484af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cf1e0e525214e798a4ebdd0de74470b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8f0009281b77480180310a2bffe57ea5",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "90cfb0064a9f440f812ca9f88fb2d546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f435dce012c8416bb29962f2fd3e6f06",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_807094d4de41498daa9c203727fc4d14",
            "value": ""
          }
        },
        "d66f0f9617634f109a452ea2bb843d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_93853a1abfc94273960ec274b021e231",
            "style": "IPY_MODEL_2d044551d92a4fc0a08493c3d5d27cc5",
            "value": true
          }
        },
        "f3295472f6e34c4bbc0ffde2ec1db9ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_744b907a70f245b0bf0b2285c9f12ad5",
            "style": "IPY_MODEL_672c413f4c664e1fbd18a9aa341fb0d2",
            "tooltip": ""
          }
        },
        "35dce967953c48efa54cd68183234d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d973f8a2f442da918cab44a5355f74",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9697644ceeb2454f9bf75d0c44d175a3",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cb0d388468b84343aa0aebd83b1c6b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7cf1e0e525214e798a4ebdd0de74470b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f0009281b77480180310a2bffe57ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f435dce012c8416bb29962f2fd3e6f06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807094d4de41498daa9c203727fc4d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93853a1abfc94273960ec274b021e231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d044551d92a4fc0a08493c3d5d27cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "744b907a70f245b0bf0b2285c9f12ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672c413f4c664e1fbd18a9aa341fb0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "19d973f8a2f442da918cab44a5355f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9697644ceeb2454f9bf75d0c44d175a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b525051cfe64e18b364cc2f015b0b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f32f61d2dc9f46429265cd447eb7df8b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb2970b4751a45ab956a0b1a19c2b442",
            "value": "Connecting..."
          }
        },
        "f32f61d2dc9f46429265cd447eb7df8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2970b4751a45ab956a0b1a19c2b442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b793c04f5e34d49912eeae2772d504d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffae77f034aa45948e81f6080caddd39",
              "IPY_MODEL_1851115875b64faa8a6bca63243e4f9d",
              "IPY_MODEL_030f0a63e7c7459ea8c8da43204e20af"
            ],
            "layout": "IPY_MODEL_3b9442b0784445ddbded8b548ac713e4"
          }
        },
        "ffae77f034aa45948e81f6080caddd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4842a09a1394cd99580cef44d77dde8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3964099e07ee4fc1abb5d5c44190180f",
            "value": "Map:â€‡100%"
          }
        },
        "1851115875b64faa8a6bca63243e4f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d697f9877d463a865e764b5c8fec51",
            "max": 606720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22e59bbdbb9e4f6898a7073ba85370ec",
            "value": 606720
          }
        },
        "030f0a63e7c7459ea8c8da43204e20af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46acbc9b78d146aeaa49ad90263465a5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d715e14177af49afa84138476239c99e",
            "value": "â€‡606720/606720â€‡[35:46&lt;00:00,â€‡279.13â€‡examples/s]"
          }
        },
        "3b9442b0784445ddbded8b548ac713e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4842a09a1394cd99580cef44d77dde8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3964099e07ee4fc1abb5d5c44190180f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89d697f9877d463a865e764b5c8fec51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e59bbdbb9e4f6898a7073ba85370ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46acbc9b78d146aeaa49ad90263465a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d715e14177af49afa84138476239c99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7550a9c2d744762b9b3e15d1769f4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dd6d1930da1474e88e64ddc5282577d",
              "IPY_MODEL_22c528937e2f442e9e30243de379ce18",
              "IPY_MODEL_223e30f4edb84db1b821db5ecfee9ec2"
            ],
            "layout": "IPY_MODEL_ebbd3c5c746c496c8d02f0668a1c548e"
          }
        },
        "6dd6d1930da1474e88e64ddc5282577d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05e07604ce2404abf7f9111286e53a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_37524eb832c749ff9191238e8b9fb1e6",
            "value": "Map:â€‡100%"
          }
        },
        "22c528937e2f442e9e30243de379ce18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9aacb5e38bd45da881264c0e8a7bad7",
            "max": 3322,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34dd4b683cd748648f679d4dde0f2793",
            "value": 3322
          }
        },
        "223e30f4edb84db1b821db5ecfee9ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c492308da24090b68a4ee0b60cab58",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db6a1f68c47f4b888d684e50132651d0",
            "value": "â€‡3322/3322â€‡[00:12&lt;00:00,â€‡262.66â€‡examples/s]"
          }
        },
        "ebbd3c5c746c496c8d02f0668a1c548e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d05e07604ce2404abf7f9111286e53a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37524eb832c749ff9191238e8b9fb1e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9aacb5e38bd45da881264c0e8a7bad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34dd4b683cd748648f679d4dde0f2793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3c492308da24090b68a4ee0b60cab58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6a1f68c47f4b888d684e50132651d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7175c874d5d34d48a28f5e0cde0bd323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b950669688c041f19c8f88e7b7107ce9",
              "IPY_MODEL_0c69680418374541b9e8c4043e1f545c",
              "IPY_MODEL_57a1c7575f8b4bdcaf9e1ec724381121"
            ],
            "layout": "IPY_MODEL_232a2bccc5254bfb92214dd18eb80c69"
          }
        },
        "b950669688c041f19c8f88e7b7107ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3b2c179a7346d4a742301213d633ca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5dc8ee4e7b144da1918acfc86e98d3ef",
            "value": "config.json:â€‡100%"
          }
        },
        "0c69680418374541b9e8c4043e1f545c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e8a259d74d4d9091a9d72ff8f5c75f",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed2aa67c3f494eaa9e951aa1b39324d0",
            "value": 665
          }
        },
        "57a1c7575f8b4bdcaf9e1ec724381121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74e19e35e32943819de8778f3d6bc070",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2aa6db6342d14790b7653a5c59dcd8e7",
            "value": "â€‡665/665â€‡[00:00&lt;00:00,â€‡83.9kB/s]"
          }
        },
        "232a2bccc5254bfb92214dd18eb80c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3b2c179a7346d4a742301213d633ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dc8ee4e7b144da1918acfc86e98d3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e8a259d74d4d9091a9d72ff8f5c75f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2aa67c3f494eaa9e951aa1b39324d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74e19e35e32943819de8778f3d6bc070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa6db6342d14790b7653a5c59dcd8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}