{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed9724aa",
      "metadata": {
        "id": "ed9724aa"
      },
      "source": [
        "# Chat Templates\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Chat templates are essential for structuring interactions between language models and users. Whether you're building a simple chatbot or a complex AI agent, understanding how to properly format your conversations is crucial for getting the best results from your model. In this guide, we'll explore what chat templates are, why they matter, and how to use them effectively.\n",
        "\n",
        "**üí° TIP:** Chat templates are crucial for:\n",
        "- Maintaining consistent conversation structure\n",
        "- Ensuring proper role identification\n",
        "- Managing context across multiple turns\n",
        "- Supporting advanced features like tool use\n",
        "\n",
        "## Model Types and Templates\n",
        "\n",
        "### Base Models vs Instruct Models\n",
        "A base model is trained on raw text data to predict the next token, while an instruct model is fine-tuned specifically to follow instructions and engage in conversations. For example, [`SmolLM2-135M`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) is a base model, while [`SmolLM2-135M-Instruct`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) is its instruction-tuned variant.  \n",
        "\n",
        "Instruction tuned models are trained to follow a specific conversational structure, making them more suitable for chatbot applications. Moreover, instruct models can handle complex interactions, including tool use, multimodal inputs, and function calling.\n",
        "\n",
        "To make a base model behave like an instruct model, we need to format our prompts in a consistent way that the model can understand. This is where chat templates come in. ChatML is one such template format that structures conversations with clear role indicators (system, user, assistant). Here's a guide on [ChatML](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/e2c3f7557efbdec707ae3a336371d169783f1da1/tokenizer_config.json#L146).\n",
        "\n",
        "**‚ö†Ô∏è WARNING:** When using an instruct model, always verify you're using the correct chat template format. Using the wrong template can result in poor model performance or unexpected behavior. The easiest way to ensure this is to check the model tokenizer configuration on the Hub. For example, the `SmolLM2-135M-Instruct` model uses <a href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/e2c3f7557efbdec707ae3a336371d169783f1da1/tokenizer_config.json#L146\">this configuration</a>.\n",
        "\n",
        "### Common Template Formats\n",
        "\n",
        "Before diving into specific implementations, it's important to understand how different models expect their conversations to be formatted. Let's explore some common template formats using a simple example conversation:\n",
        "\n",
        "We'll use the following conversation structure for all examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "92c3e10c",
      "metadata": {
        "id": "92c3e10c"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e667dfb",
      "metadata": {
        "id": "7e667dfb"
      },
      "source": [
        "This is the ChatML template used in models like SmolLM2 and Qwen 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b4aca7",
      "metadata": {
        "id": "e8b4aca7"
      },
      "source": [
        "```sh\n",
        "<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Hello!<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Hi! How can I help you today?<|im_end|>\n",
        "<|im_start|>user\n",
        "What's the weather?<|im_start|>assistant\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97372c70",
      "metadata": {
        "id": "97372c70"
      },
      "source": [
        "This is using the `mistral` template format:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c505e4a",
      "metadata": {
        "id": "4c505e4a"
      },
      "source": [
        "```sh\n",
        "<s>[INST] You are a helpful assistant. [/INST]\n",
        "Hi! How can I help you today?</s>\n",
        "[INST] Hello! [/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f319bb",
      "metadata": {
        "id": "42f319bb"
      },
      "source": [
        "Key differences between these formats include:\n",
        "1. **System Message Handling**:\n",
        "   - Llama 2 wraps system messages in `<<SYS>>` tags\n",
        "   - Llama 3 uses `<|system|>` tags with `</s>` endings\n",
        "   - Mistral includes system message in the first instruction\n",
        "   - Qwen uses explicit `system` role with `<|im_start|>` tags\n",
        "   - ChatGPT uses `SYSTEM:` prefix\n",
        "\n",
        "2. **Message Boundaries**:\n",
        "   - Llama 2 uses `[INST]` and `[/INST]` tags\n",
        "   - Llama 3 uses role-specific tags (`<|system|>`, `<|user|>`, `<|assistant|>`) with `</s>` endings\n",
        "   - Mistral uses `[INST]` and `[/INST]` with `<s>` and `</s>`\n",
        "   - Qwen uses role-specific start/end tokens\n",
        "\n",
        "3. **Special Tokens**:\n",
        "   - Llama 2 uses `<s>` and `</s>` for conversation boundaries\n",
        "   - Llama 3 uses `</s>` to end each message\n",
        "   - Mistral uses `<s>` and `</s>` for turn boundaries\n",
        "   - Qwen uses role-specific start/end tokens\n",
        "\n",
        "Understanding these differences is key to working with various models. Let's look at how the transformers library helps us handle these variations automatically:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# These models have chat templates defined\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
        "smol_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
        "]\n",
        "\n",
        "mistral_chat = mistral_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "qwen_chat = qwen_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "smol_chat = smol_tokenizer.apply_chat_template(messages, tokenize=False)"
      ],
      "metadata": {
        "id": "Uw6BC2bQUSp1"
      },
      "id": "Uw6BC2bQUSp1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "459d2f2b",
      "metadata": {
        "id": "459d2f2b"
      },
      "source": [
        "```sh\n",
        "<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Hello!<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Hi! How can I help you today?<|im_end|>\n",
        "<|im_start|>user\n",
        "What's the weather?<|im_start|>assistant\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72750bef",
      "metadata": {
        "id": "72750bef"
      },
      "source": [
        "Mistral template:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928e4e22",
      "metadata": {
        "id": "928e4e22"
      },
      "source": [
        "```sh\n",
        "<s>[INST] You are a helpful assistant. [/INST]\n",
        "Hi! How can I help you today?</s>\n",
        "[INST] Hello! [/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ac9a2c",
      "metadata": {
        "id": "f0ac9a2c"
      },
      "source": [
        "</details>\n",
        "\n",
        "### Advanced Features\n",
        "Chat templates can handle more complex scenarios beyond just conversational interactions, including:\n",
        "\n",
        "1. **Tool Use**: When models need to interact with external tools or APIs\n",
        "2. **Multimodal Inputs**: For handling images, audio, or other media types\n",
        "3. **Function Calling**: For structured function execution\n",
        "4. **Multi-turn Context**: For maintaining conversation history\n",
        "\n",
        "**üí° TIP:** When implementing advanced features:\n",
        "- Test thoroughly with your specific model. Vision and tool use template are particularly diverse.\n",
        "- Monitor token usage carefully between each feature and model.\n",
        "- Document the expected format for each feature\n",
        "\n",
        "For multimodal conversations, chat templates can include image references or base64-encoded images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f8dbba9f",
      "metadata": {
        "id": "f8dbba9f"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful vision assistant that can analyze images.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "            {\"type\": \"image\", \"image_url\": \"https://example.com/image.jpg\"},\n",
        "        ],\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f00d115",
      "metadata": {
        "id": "5f00d115"
      },
      "source": [
        "Here's an example of a chat template with tool use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a9fb8e5f",
      "metadata": {
        "id": "a9fb8e5f"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"},\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Let me help you with that.\",\n",
        "        \"tool_calls\": [\n",
        "            {\n",
        "                \"tool\": \"calculator\",\n",
        "                \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456},\n",
        "            },\n",
        "            {\"tool\": \"weather_api\", \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}},\n",
        "        ],\n",
        "    },\n",
        "    {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"},\n",
        "    {\n",
        "        \"role\": \"tool\",\n",
        "        \"tool_name\": \"weather_api\",\n",
        "        \"content\": \"{'condition': 'rain', 'temperature': 15}\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d5856f",
      "metadata": {
        "id": "33d5856f"
      },
      "source": [
        "## Best Practices\n",
        "\n",
        "### General Guidelines\n",
        "When working with chat templates, follow these key practices:\n",
        "\n",
        "1. **Consistent Formatting**: Always use the same template format throughout your application\n",
        "2. **Clear Role Definition**: Clearly specify roles (system, user, assistant, tool) for each message\n",
        "3. **Context Management**: Be mindful of token limits when maintaining conversation history\n",
        "4. **Error Handling**: Include proper error handling for tool calls and multimodal inputs\n",
        "5. **Validation**: Validate message structure before sending to the model\n",
        "\n",
        "**‚ö†Ô∏è WARNING:**\n",
        "\n",
        "Common pitfalls to avoid:\n",
        "- Mixing different template formats in the same application\n",
        "- Exceeding token limits with long conversation histories\n",
        "- Not properly escaping special characters in messages\n",
        "- Forgetting to validate input message structure\n",
        "- Ignoring model-specific template requirements\n",
        "\n",
        "## Hands-on Exercise\n",
        "\n",
        "Let's practice implementing chat templates with a real-world example.\n",
        "\n",
        "**üí° TIP:** Follow these steps to convert the `HuggingFaceTB/smoltalk` dataset into chatml format:\n",
        "\n",
        "1. Load the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'all')"
      ],
      "metadata": {
        "id": "2DZiGFcHYl16"
      },
      "id": "2DZiGFcHYl16",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"\\nColumn names: {dataset['train'].column_names}\")\n",
        "print(f\"\\nFirst training example:\")\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jyFn3vMZhWL",
        "outputId": "586377a6-d315-4d79-d863-88fb47054c6d"
      },
      "id": "2jyFn3vMZhWL",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['messages', 'category', 'difficulty', 'quality', 'reward_model_score', 'conversation_tokens'],\n",
            "        num_rows: 409537\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['messages', 'category', 'difficulty', 'quality', 'reward_model_score', 'conversation_tokens'],\n",
            "        num_rows: 21555\n",
            "    })\n",
            "})\n",
            "\n",
            "Column names: ['messages', 'category', 'difficulty', 'quality', 'reward_model_score', 'conversation_tokens']\n",
            "\n",
            "First training example:\n",
            "{'messages': [{'content': 'Jim has won a vacation at a lake resort and can choose to travel to either lake A or lake B. Lake A has a 20% chance of rain spoiling the vacation while lake B has a 40% chance of rain. The cost of travel to lake A is $200 higher than that to lake B. What are some factors Jim might consider in making a decision on which lake to choose.', 'role': 'user'}, {'content': \"To make an informed decision, Jim should weigh the potential risks and costs associated with each option. One key factor to consider is the likelihood of rain spoiling the vacation. Lake A has a lower chance of rain, at 20%, which increases the likelihood of a pleasant vacation. On the other hand, lake B has a higher chance of rain, at 40%, which may lead to a less enjoyable experience.\\n\\nAnother crucial factor is the cost difference between traveling to lake A and lake B. With a $200 higher cost for lake A, Jim needs to consider whether the lower risk of rain is worth the additional expense. This decision might depend on his budget and how much he values the potential for better weather.\\n\\nAdditionally, Jim may want to consider other factors that could influence his decision, such as the scenic beauty, recreational activities, and amenities offered by each lake. For instance, if lake A offers more exciting activities or stunning scenery, Jim might be willing to pay the extra $200 to travel there, despite the lower risk of rain. Conversely, if lake B has similar attractions at a lower cost, Jim might be willing to take the higher risk of rain to save money.\\n\\nJim's personal preferences and priorities also play a significant role in this decision. If he is particularly sensitive to weather conditions or has a strong desire for a sunny vacation, he may be more inclined to choose lake A. On the other hand, if he is on a tight budget or more flexible with his plans, lake B might be the more appealing option.\\n\\nUltimately, Jim's decision will depend on his individual circumstances, priorities, and risk tolerance. By carefully weighing the pros and cons of each option, he can make an informed decision that suits his needs and preferences.\", 'role': 'assistant'}, {'content': 'I was given a logical reasoning question but I do not know what the question is asking for. It states: The detective suspects one of five people of a murder: Alex, Ben, Chris, David, and Emily. He has the following clues: (1) The killer was a man, (2) Chris and David were together when the murder occurred and (3) Ben has a rock-solid alibi. Using these clues what can be concluded about the murderer.', 'role': 'user'}, {'content': \"Let's break down the information provided. The first clue states that the killer was a man, which immediately narrows down the list of suspects to Alex, Ben, Chris, and David, since Emily is a woman and can be ruled out.\\n\\nThe second clue tells us that Chris and David were together when the murder occurred. This information is useful in that it doesn't necessarily exonerate either of them, as they could have committed the crime together. However, it does tell us that if one of them is guilty, the other would likely be guilty as well, or at least be aware of the other's guilt.\\n\\nThe third clue states that Ben has a rock-solid alibi, which means he has a reliable and verifiable account of his whereabouts at the time of the murder, making it highly unlikely that he was the killer. With this information, we can safely rule out Ben as a suspect.\\n\\nGiven these clues, we can conclude that the murderer is likely either Alex, or Chris and/or David. The fact that Chris and David were together means that we can't rule out either of them individually, but we can say that if one of them is guilty, the other is likely involved as well. The only other suspect is Alex, who hasn't been ruled out by any of the clues. Therefore, the murderer is likely one of these three options: Alex, or Chris and/or David.\", 'role': 'assistant'}, {'content': \"I'm currently studying for the Law School Admission Test (LSAT) and I was hoping you could help me to identify the flaw in a practice argument I came across. The argument is: The company's profits have been declining for the past two years, but an analyst believes the company will return to profitability soon. She points to the fact that the company is the leading seller in its market, and that this is typically a very profitable position to be in. The analyst concludes that the company's dominant market share will cause its profits to increase. \\n\\nCan you identify what's wrong with this argument, specifically what logical fallacy it's committing?\", 'role': 'user'}, {'content': 'The argument presented by the analyst is flawed because it assumes that being the leading seller in the market will automatically lead to increased profits, without considering other factors that may be contributing to the company\\'s decline. This argument is committing the logical fallacy of \"appeal to irrelevant evidence\" or \"ignoring a common cause.\"\\n\\nThe analyst is relying on a general rule that being the leading seller in a market is typically profitable, but she is ignoring the specific circumstances of the company\\'s decline. There may be other factors at play that are causing the company\\'s profits to decline, such as increased competition, changing market trends, or internal inefficiencies, that are not addressed by the analyst\\'s argument.\\n\\nFurthermore, the argument is also committing the logical fallacy of \"post hoc ergo propter hoc\" or \"correlation does not imply causation.\" The analyst is assuming that the company\\'s dominant market share will cause its profits to increase, but she is not providing any evidence of a causal link between the two. It is possible that the company\\'s dominant market share is not the cause of its future profitability, but rather a coincidence or a result of other factors.\\n\\nThe analyst\\'s argument would be stronger if she provided more nuanced analysis of the company\\'s specific situation, including the reasons for its decline and the steps it is taking to address those issues. Simply relying on the company\\'s market share as evidence of future profitability is not sufficient to support her conclusion.', 'role': 'assistant'}], 'category': 'reasoning', 'difficulty': 'medium', 'quality': 'good', 'reward_model_score': 0.15092703700065613, 'conversation_tokens': 1283}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a processing function:"
      ],
      "metadata": {
        "id": "t_8repymYyp7"
      },
      "id": "t_8repymYyp7"
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_chatml(example):\n",
        "      return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": example[\"source\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"messages\"]},\n",
        "        ]\n",
        "     }"
      ],
      "metadata": {
        "id": "FJzusmWhY2O3"
      },
      "id": "FJzusmWhY2O3",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Apply the chat template using your chosen model's tokenizer"
      ],
      "metadata": {
        "id": "yxmx53E-ZO5u"
      },
      "id": "yxmx53E-ZO5u"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SmolLM2 tokenizer with the ChatML format\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "\n",
        "example_messages = dataset['train'][0]['messages']\n",
        "\n",
        "print(\"Original message:\")\n",
        "print()\n",
        "for msg in example_messages:\n",
        "    print(f\"  {msg['role']}: {msg['content'][:1100]}\")\n",
        "\n",
        "\n",
        "# Use the chat template\n",
        "formatted_chat = tokenizer.apply_chat_template(\n",
        "    example_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print()\n",
        "print(\"Formatted chat with ChatML template:\")\n",
        "print()\n",
        "print(formatted_chat)\n",
        "\n",
        "# Examine the tokenization\n",
        "formatted_chat_tokenized = tokenizer.apply_chat_template(\n",
        "    example_messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(f\"Tokenized shape: {formatted_chat_tokenized.shape}\")\n",
        "print(f\"Number of tokens: {formatted_chat_tokenized.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-r9wJg_ZrkN",
        "outputId": "9ef23f57-45af-43f3-b846-ff7934b0a6cf"
      },
      "id": "x-r9wJg_ZrkN",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original message:\n",
            "\n",
            "  user: The function \\( g(x) \\) satisfies the functional equation\n",
            "\\[ g(x + y) = g(x) + g(y) \\]\n",
            "for all real numbers \\( x \\) and \\( y \\), and it is given that \\( g(3) = 4 \\). Find \\( g(10) \\).\n",
            "  assistant: Given the functional equation and the specific value \\( g(3) = 4 \\), we can find \\( g(1) \\) by using the equation multiple times:\n",
            "\\[\n",
            "g(3) = g(2) + g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(2) = g(1) + g(1) = 2g(1)\n",
            "\\]\n",
            "Thus,\n",
            "\\[\n",
            "4 = 2g(1) + g(1) = 3g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(1) = \\frac{4}{3}\n",
            "\\]\n",
            "Now we can find \\( g(10) \\) using \\( g(1) \\):\n",
            "\\[\n",
            "g(10) = 10g(1) = 10 \\times \\frac{4}{3} = \\frac{40}{3}\n",
            "\\]\n",
            "Hence, the value of \\( g(10) \\) is \\(\\boxed{\\frac{40}{3}}\\).\n",
            "\n",
            "Formatted chat with ChatML template:\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "The function \\( g(x) \\) satisfies the functional equation\n",
            "\\[ g(x + y) = g(x) + g(y) \\]\n",
            "for all real numbers \\( x \\) and \\( y \\), and it is given that \\( g(3) = 4 \\). Find \\( g(10) \\).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Given the functional equation and the specific value \\( g(3) = 4 \\), we can find \\( g(1) \\) by using the equation multiple times:\n",
            "\\[\n",
            "g(3) = g(2) + g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(2) = g(1) + g(1) = 2g(1)\n",
            "\\]\n",
            "Thus,\n",
            "\\[\n",
            "4 = 2g(1) + g(1) = 3g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(1) = \\frac{4}{3}\n",
            "\\]\n",
            "Now we can find \\( g(10) \\) using \\( g(1) \\):\n",
            "\\[\n",
            "g(10) = 10g(1) = 10 \\times \\frac{4}{3} = \\frac{40}{3}\n",
            "\\]\n",
            "Hence, the value of \\( g(10) \\) is \\(\\boxed{\\frac{40}{3}}\\).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Tokenized shape: torch.Size([1, 343])\n",
            "Number of tokens: 343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply chat template to the entire dataset\n",
        "def apply_chat_template(example):\n",
        "    \"\"\"Apply tokenizer's chat template to messages\"\"\"\n",
        "    example[\"formatted_chat\"] = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return example\n",
        "\n",
        "# Process a subset\n",
        "dataset_formatted = dataset['train'].select(range(10)).map(apply_chat_template)\n",
        "\n",
        "print(\"Examples made by the chat template\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print()\n",
        "    print(dataset_formatted[i]['formatted_chat'][:1000] + \"...\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "826108bdc1de47cd86f28e0e884038c4",
            "dd21f77953694e05aeb1ef4d768dcc98",
            "e5b4b34b339847c2a191c1c307541810",
            "84080c2e7848433baca0bb5f615ccd36",
            "963b1dd5fa894f8696acc251a17cab35",
            "ceb4b7d3e60d45f6b42ba66b2140db19",
            "42b1a9363aed4af5bb591462cfa7f2d9",
            "952cfa94bc794899a3638aa1a60e469b",
            "080b464cecf74f8096c0e7530caeeeff",
            "5a644cd93ba44b7893baaa47c23b855f",
            "c9eb43a033aa4e9da1f38876686ba14a"
          ]
        },
        "id": "QQaV4CDRf4IU",
        "outputId": "8f06ee03-e29d-429a-cf4a-4d5e6d4f59a8"
      },
      "id": "QQaV4CDRf4IU",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "826108bdc1de47cd86f28e0e884038c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples made by the chat template\n",
            "\n",
            "Example 1:\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "The function \\( g(x) \\) satisfies the functional equation\n",
            "\\[ g(x + y) = g(x) + g(y) \\]\n",
            "for all real numbers \\( x \\) and \\( y \\), and it is given that \\( g(3) = 4 \\). Find \\( g(10) \\).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Given the functional equation and the specific value \\( g(3) = 4 \\), we can find \\( g(1) \\) by using the equation multiple times:\n",
            "\\[\n",
            "g(3) = g(2) + g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(2) = g(1) + g(1) = 2g(1)\n",
            "\\]\n",
            "Thus,\n",
            "\\[\n",
            "4 = 2g(1) + g(1) = 3g(1)\n",
            "\\]\n",
            "\\[\n",
            "g(1) = \\frac{4}{3}\n",
            "\\]\n",
            "Now we can find \\( g(10) \\) using \\( g(1) \\):\n",
            "\\[\n",
            "g(10) = 10g(1) = 10 \\times \\frac{4}{3} = \\frac{40}{3}\n",
            "\\]\n",
            "Hence, the value of \\( g(10) \\) is \\(\\boxed{\\frac{40}{3}}\\).<|im_end|>\n",
            "...\n",
            "\n",
            "\n",
            "Example 2:\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "Ben twice chooses a random integer between 1 and 60, inclusive. What is the probability that at least one of the numbers Ben chooses is a multiple of 4?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "First, we find the number of multiples of 4 between 1 and 60. The smallest multiple of 4 is $4 \\times 1 = 4$ and the largest multiple of 4 within 60 is $4 \\times 15 = 60$. Hence, there are 15 multiples of 4.\n",
            "\n",
            "The total number of integers between 1 to 60 is 60. Therefore, the number of integers that are not multiples of 4 is $60 - 15 = 45$.\n",
            "\n",
            "The probability that a single number chosen is not a multiple of 4 is $\\frac{45}{60} = \\frac{3}{4}$. If Ben chooses two numbers independently, the probability that neither number is a multiple of 4 is $\\left(\\frac{3}{4}\\right)^2 = \\frac{9}{16}$.\n",
            "\n",
            "Thus, the probability that at least one of the numbers is a multiple of 4 is $1 - \\frac{9}{16} = \\frac{7}{16}$...\n",
            "\n",
            "\n",
            "Example 3:\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "Find all values of $x$ that satisfy the equation $|2x-5| = 3x + 1$.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "We can solve the equation $|2x-5| = 3x + 1$ by considering two cases for the absolute value.\n",
            "\n",
            "**Case 1**: When $2x - 5 \\geq 0$, which simplifies to $x \\geq \\frac{5}{2}$.\n",
            "\\[\n",
            "2x - 5 = 3x + 1\n",
            "\\Rightarrow -x = 6\n",
            "\\Rightarrow x = -6\n",
            "\\]\n",
            "However, $x = -6$ does not satisfy $x \\geq \\frac{5}{2}$. Thus, no solutions exist in this case.\n",
            "\n",
            "**Case 2**: When $2x - 5 < 0$, which implies $x < \\frac{5}{2}$.\n",
            "\\[\n",
            "-(2x-5) = 3x + 1\n",
            "\\Rightarrow -2x + 5 = 3x + 1\n",
            "\\Rightarrow 5x = 4\n",
            "\\Rightarrow x = \\frac{4}{5}\n",
            "\\]\n",
            "We then check if $x = \\frac{4}{5}$ meets $x < \\frac{5}{2}$ which is true.\n",
            "\n",
            "Substituting $x = \\frac{4}{5}$ back into the original equation to verify,\n",
            "\\[\n",
            "|2(\\frac{4}{5}) - 5| = 3(\\frac{4}{5}) + 1\n",
            "\\Rightarrow |-\\frac{1}{5}| = \\frac{12}{5} + 1\n",
            "\\Rightarrow \\frac{1}{5} = \\frac{17}{5}\n",
            "\\]\n",
            "This claim is...\n",
            "\n",
            "\n",
            "Example 4:\n",
            "\n",
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "How can you help me? I'm writing an essay and need assistance in revising my work for better structure and clarity.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "I can help you refine your essay by reviewing your text and providing suggestions to improve its structure, clarity, and overall flow. I can also assist in identifying areas where the writing may be unclear or difficult to understand and offer alternatives to make your points more concise and impactful.\n",
            "\n",
            "Please go ahead and share the essay you're working on, and let me know what specific areas you're concerned about or would like me to focus on. Is it the introduction, transitions between paragraphs, supporting evidence, or the conclusion? Or would you like a general review of the entire piece?<|im_end|>\n",
            "<|im_start|>user\n",
            "What does constructive criticism mean?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Constructive criticism refers to feed...\n",
            "\n",
            "\n",
            "Example 5:\n",
            "\n",
            "<|im_start|>system\n",
            "Extract and present the main key point of the input text in one very short sentence, including essential details like dates or locations if necessary.<|im_end|>\n",
            "<|im_start|>user\n",
            "Hi Michael,\n",
            "\n",
            "I hope you're doing well! I wanted to follow up on our conversation from the Math Educators Forum about collaborating on decimal operation worksheets. I'm excited to work together and combine our strengths to create something great for our students.\n",
            "\n",
            "I was thinking we could meet in person to discuss our plans and goals for the project. I live in Oakville and you're in Pinecrest, right? There's a great coffee shop called \"The Bean Counter\" that's about halfway between us. Would you be available to meet there next Saturday, March 14th, at 10 AM? Let me know if that works for you or if you have any other suggestions.\n",
            "\n",
            "Looking forward to working together!\n",
            "\n",
            "Best,\n",
            "Sarah<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Sarah is proposing a meeting at \"The Bean Counter\" in Oakville on March 14th at 10 AM...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [Hugging Face Chat Templating Guide](https://huggingface.co/docs/transformers/main/en/chat_templating)\n",
        "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "- [Chat Templates Examples Repository](https://github.com/chujiezheng/chat_templates)"
      ],
      "metadata": {
        "id": "zZ2QBhuPYe-B"
      },
      "id": "zZ2QBhuPYe-B"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "826108bdc1de47cd86f28e0e884038c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd21f77953694e05aeb1ef4d768dcc98",
              "IPY_MODEL_e5b4b34b339847c2a191c1c307541810",
              "IPY_MODEL_84080c2e7848433baca0bb5f615ccd36"
            ],
            "layout": "IPY_MODEL_963b1dd5fa894f8696acc251a17cab35"
          }
        },
        "dd21f77953694e05aeb1ef4d768dcc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb4b7d3e60d45f6b42ba66b2140db19",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42b1a9363aed4af5bb591462cfa7f2d9",
            "value": "Map:‚Äá100%"
          }
        },
        "e5b4b34b339847c2a191c1c307541810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952cfa94bc794899a3638aa1a60e469b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_080b464cecf74f8096c0e7530caeeeff",
            "value": 10
          }
        },
        "84080c2e7848433baca0bb5f615ccd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a644cd93ba44b7893baaa47c23b855f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c9eb43a033aa4e9da1f38876686ba14a",
            "value": "‚Äá10/10‚Äá[00:00&lt;00:00,‚Äá155.00‚Äáexamples/s]"
          }
        },
        "963b1dd5fa894f8696acc251a17cab35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb4b7d3e60d45f6b42ba66b2140db19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42b1a9363aed4af5bb591462cfa7f2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "952cfa94bc794899a3638aa1a60e469b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "080b464cecf74f8096c0e7530caeeeff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a644cd93ba44b7893baaa47c23b855f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9eb43a033aa4e9da1f38876686ba14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}