{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv06Bwk5Gdpt"
      },
      "source": [
        "# Transformers, what can they do?\n",
        "\n",
        "Course Website: https://huggingface.co/learn/nlp-course/chapter1/3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKHD9syKGdpu"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MdDsXO7IGdpv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, log into Hugging face"
      ],
      "metadata": {
        "id": "3gVSG6i1z5-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "255e6020b54e4bfb875363444cc6b731",
            "f0488974d0f24a68bb4706804aa1ad51",
            "a338e4ba4dd24b4d906a6ed1add8f32f",
            "8325c5becf1e40eda64cbe26421309ca",
            "7ca9241475e040898e7a5aaaaba1a590",
            "5b3449f2f26043fb85fc808ab2c64283",
            "27bcaca28b9f4bb2a9f7311bf3618727",
            "365599dd3f8e4bdabdcdb26ed1cf6e61",
            "9b5933f8b80841c8897e1feaa49ee9bf",
            "188eff481b7e41158ffc561f3bd23bf2",
            "74ae48c193e746c989887f1997c7a105",
            "fae2692fadb548d68a037d06a225168f",
            "a9e2a45e916e417e9207684f2222b535",
            "5eab90bc386f4795b243ca8c922cf5be",
            "77b75a9e95c24e0f9bc75b24029d82f2",
            "dc1740a880c24898ac64dea6d1421bdd",
            "bb4468a4b77746a28f1dd207b85e6023",
            "63c7d1604c9646efbea78e93d4cb179f",
            "1bf9790904274b84ac0ab7219568c0aa",
            "d03020caff9a4664a25a8f5badf5c595",
            "2c59dc85322844769590720972f98b7e",
            "a7a448eece6d445596d61bb7a1e91fe3",
            "7cc8a18396a1454bad22d3442b7e1980",
            "bfd869dd5d9f4b2c89e45ff23fe09dfc",
            "edab985f152143b896c1760ed19aa500",
            "a14f75c5377c4e5094b168dd6e1c17a2",
            "19ac98650eac473c89d910deab19658d",
            "2e7af7b23d794895800b1cd5e7eb19c6",
            "4b898e7ecfdc4dedb31636f697836850",
            "315beb8a2f8d438884636747eec2e5ae",
            "c1bbca3ff15947a5918f499d7b285131",
            "8b8461d61ad5424c9b33262da9b6bb2e"
          ]
        },
        "id": "gNbWnYWkz6yP",
        "outputId": "027c61ec-a2a9-49df-b606-50e254cfdb41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "255e6020b54e4bfb875363444cc6b731"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers are everywhere!\n",
        "\n",
        "<font color='blue'>Transformer models</font> are used to solve <font color='blue'>all kinds of NLP tasks</font>, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n",
        "\n",
        "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\n",
        "\n",
        "The [ü§ó Transformers library](https://github.com/huggingface/transformers) provides the functionality to <font color='blue'>create</font> and <font color='blue'>use</font> those <font color='blue'>shared models</font>. The [Model Hub](https://huggingface.co/models) contains <font color='blue'>thousands</font> of <font color='blue'>pretrained models</font> that anyone can <font color='blue'>download</font> and <font color='blue'>use</font>. You can also upload your own models to the Hub!\n",
        "\n",
        "Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.\n",
        "\n",
        "###  Working with pipelines"
      ],
      "metadata": {
        "id": "mQn-pQbmKjR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most basic object in the ü§ó Transformers library is the <font color='blue'>pipeline() function</font>. It <font color='blue'>connects</font> a <font color='blue'>model</font> with its necessary <font color='blue'>preprocessing</font> and <font color='blue'>postprocessing steps</font>, allowing us to directly input any text and get an intelligible answer:"
      ],
      "metadata": {
        "id": "ipBZO3tPJQ9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "liyJbURzGdpw",
        "outputId": "188d1479-4c47-4724-a500-4d3e91945c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can even pass <font color='blue'>several sentences</font>!"
      ],
      "metadata": {
        "id": "oIo-ykTnJWQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "veLTiZLfGdpx",
        "outputId": "9014c1b3-c2ea-4deb-d4ce-0d08b588de56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n",
              " {'label': 'POSITIVE', 'score': 0.9995144605636597},\n",
              " {'label': 'POSITIVE', 'score': 0.8833526968955994},\n",
              " {'label': 'NEGATIVE', 'score': 0.9764426350593567},\n",
              " {'label': 'POSITIVE', 'score': 0.8040932416915894}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\",\n",
        "     \"I hate this so much!\",\n",
        "     \"I like to eat chocolate and peanut butter.\",\n",
        "     \"Do you like green eggs and ham?\",\n",
        "     \"What is better than running?\",\n",
        "     \"I don't like walking without socks.\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, this pipeline <font color='blue'>selects</font> a <font color='blue'>particular pretrained model</font> that has been <font color='blue'>fine-tuned</font> for <font color='blue'>sentiment analysis</font> in English. The model is <font color='blue'>downloaded</font> and <font color='blue'>cached</font> when you create the <font color='blue'>classifier</font> object. If you <font color='blue'>rerun</font> the <font color='blue'>command</font>, the <font color='blue'>cached model</font> will be <font color='blue'>used</font> instead and there is no need to download the model again.\n",
        "\n",
        "There are <font color='blue'>three main steps</font> involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is <font color='blue'>preprocessed</font> into a format the model can understand.\n",
        "2. The <font color='blue'>preprocessed inputs</font> are <font color='blue'>passed</font> to the <font color='blue'>model</font>.\n",
        "3. The <font color='blue'>predictions</font> of the model are <font color='blue'>post-processed</font>, so you can make sense of them.\n",
        "\n",
        "Some of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines) are:\n",
        "\n",
        "1. feature-extraction (get the vector representation of a text)\n",
        "2. fill-mask\n",
        "3. ner (named entity recognition)\n",
        "4. question-answering\n",
        "5. sentiment-analysis\n",
        "6. summarization\n",
        "7. text-generation\n",
        "8. translation\n",
        "9. zero-shot-classification\n",
        "\n",
        "Let's have a look at a few of these!"
      ],
      "metadata": {
        "id": "_eSlbmcBJXRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot classification\n",
        "\n",
        "We'll start by tackling a <font color='blue'>more challenging</font> task where we need to <font color='blue'>classify texts</font> that <font color='blue'>haven't been labelled</font>. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the <font color='blue'>zero-shot-classification</font> pipeline is very powerful: it allows you to specify <font color='blue'>which labels to use for the classification</font>, so you <font color='blue'>don't have to rely on the labels of the pretrained model</font>. You've already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like."
      ],
      "metadata": {
        "id": "UrpiTvoZKoA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T8VTH1yOGdpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3764c7e0-59ad-4661-84ba-51e30cd8b1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445968627929688, 0.1119757816195488, 0.04342734441161156]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "classifier(\n",
        "    \"My favoite sport involves running shoes and long distances.\",\n",
        "    candidate_labels=[\"health\", \"education\", \"politics\"],\n",
        ")"
      ],
      "metadata": {
        "id": "7_X7n2kcPMQh",
        "outputId": "1733bbc2-3e44-4f40-96a6-b99479297eab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'My favoite sport involves running shoes and long distances.',\n",
              " 'labels': ['health', 'education', 'politics'],\n",
              " 'scores': [0.7021063566207886, 0.18310600519180298, 0.11478759348392487]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This <font color='blue'>pipeline</font> is called <font color='blue'>zero-shot</font> because you <font color='blue'>don't need</font> to <font color='blue'>fine-tune the model</font> on your data to use it. It can directly return probability scores for any list of labels you want!"
      ],
      "metadata": {
        "id": "6xl7fJx1LGFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** <font color='blue'>Play</font> around with your own <font color='blue'>sequences</font> and <font color='blue'>labels</font> and see how the model behaves."
      ],
      "metadata": {
        "id": "ci8PS6wUcCvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "classifier(\n",
        "    \"The best running shoes hold up for at least 600 miles.\",\n",
        "    candidate_labels=[\"Brooks\", \"Adidas\", \"Kona\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5kX08N3cERN",
        "outputId": "3c17645e-bf87-49b0-c1d5-16240601cd03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'The best running shoes hold up for at least 600 miles.',\n",
              " 'labels': ['Brooks', 'Kona', 'Adidas'],\n",
              " 'scores': [0.39663437008857727, 0.31448471546173096, 0.2888808846473694]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation\n",
        "\n",
        "Now let's see how to <font color='blue'>use</font> a <font color='blue'>pipeline</font> to <font color='blue'>generate some text</font>. The main idea here is that you <font color='blue'>provide a prompt</font> and the <font color='blue'>model will auto-complete</font> it by <font color='blue'>generating</font> the remaining <font color='blue'>text</font>. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it's normal if you don't get the same results as shown below."
      ],
      "metadata": {
        "id": "QYyZgPZjL8Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "generator(\"In this course, we will teach you how to\",\n",
        "          max_length=20, # Length of sentence\n",
        "          num_return_sequences=4) # 4 sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0plh2vRSK6_",
        "outputId": "6ee3a3a4-e973-450f-c243-47be0f319637"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"In this course, we will teach you how to implement the Racket program into the Raspberry Pi's\"},\n",
              " {'generated_text': 'In this course, we will teach you how to create a virtual machine like Linux, which uses the'},\n",
              " {'generated_text': 'In this course, we will teach you how to read and write a complex code, which I shall'},\n",
              " {'generated_text': 'In this course, we will teach you how to use both of them in order to get rid of'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Use the <font color='blue'>num_return_sequences</font> and <font color='blue'>max_length</font> arguments to generate <font color='blue'>two sentences</font> of <font color='blue'>15 words</font> each."
      ],
      "metadata": {
        "id": "QeoD5jQbcc7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
        "generator(\"We can explain machine learning by\",\n",
        "          max_length=15, # Length of sentence\n",
        "          num_return_sequences=2) # 2 sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vwPFPLUceNB",
        "outputId": "e0586b4d-7309-4809-d9c9-4e69a701cbfb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'We can explain machine learning by seeing the same thing:\\n\\n\\n\\n'},\n",
              " {'generated_text': 'We can explain machine learning by studying neural networks during a task, but it'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using any model from the Hub in a pipeline\n",
        "\n",
        "The previous examples used the <font color='blue'>default model</font> for the task at hand, but you can also <font color='blue'>choose</font> a <font color='blue'>particular model</font> from the <font color='blue'>Hub</font> to use in a pipeline for a specific task ‚Äî say, text generation. Go to the [Model Hub](https://huggingface.co/models) and <font color='blue'>click</font> on the <font color='blue'>corresponding tag</font> on the <font color='blue'>left</font> to display only the supported models for that task. You should get to a page like [this one](https://huggingface.co/models?pipeline_tag=text-generation).\n",
        "\n",
        "Let's try the [distilgpt2](https://huggingface.co/distilgpt2) model! Here's how to load it in the same pipeline as before:"
      ],
      "metadata": {
        "id": "zHEN1L01MKqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "uZeB-CYvMjnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b214be-f56f-49d5-abc4-719f69aba6aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to set your course:\\n\\n\\n\\n\\nCreate and set the new variable to use to manage the'},\n",
              " {'generated_text': 'In this course, we will teach you how to develop a unique brand of custom-built machine parts to the user experience, from to the user experience'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And similarly for the <font color='blue'>gpt2-large</font> model:"
      ],
      "metadata": {
        "id": "50N7jsVI4xVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
        "\n",
        "generator(\n",
        "    \"Boys like to do the following activities in the summer\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=5,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO0WWdoSTbLl",
        "outputId": "6b1e7aed-759b-455d-d448-a03ffc44d13b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Boys like to do the following activities in the summer holidays;\\n\\n‚Ä¢ to get up, to cook, to play football (with a friend'},\n",
              " {'generated_text': 'Boys like to do the following activities in the summer of every year (all activities are done as often but as little, if at all, as'},\n",
              " {'generated_text': 'Boys like to do the following activities in the summer:\\n\\n- Take a break from summer vacation time to go to the beach, where there'},\n",
              " {'generated_text': \"Boys like to do the following activities in the summer when it's hot:\\n\\nVisit beach playgrounds\\n\\nVisit the lake\\n\\nPlay\"},\n",
              " {'generated_text': 'Boys like to do the following activities in the summer ‚Äì for reasons that are totally self-evident. (If the weather is too warm to'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can <font color='blue'>refine</font> your <font color='blue'>search</font> for a model by <font color='blue'>clicking</font> on the <font color='blue'>language tags</font>, and pick a model that will generate text in another language. The <font color='blue'>Model Hub</font> even contains <font color='blue'>checkpoints</font> for <font color='blue'>multilingual models</font> that support several languages.\n",
        "\n",
        "Once you select a model by clicking on it, you'll see that there is a <font color='blue'>widget</font> enabling you to <font color='blue'>try it directly online</font>. This way you can quickly test the model's capabilities before downloading it."
      ],
      "metadata": {
        "id": "8uiPLTlWPuGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Use the filters to find a text generation model for another language. Feel free to play with the widget and use it in a pipeline!\n",
        "\n",
        "I will try text generation for the <font color='blue'>Japanese language</font>:"
      ],
      "metadata": {
        "id": "57sWoUt8dHXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"ai-forever/mGPT\")\n",
        "\n",
        "japanese_text = \"„ÉÜ„Ç£„Éº„É≥„Ç®„Ç§„Ç∏„É£„Éº„ÅØÂÜ¨„Å´„Åì„Çå„Çâ„ÅÆ„Çπ„Éù„Éº„ÉÑ„ÇíÊ•Ω„Åó„ÇÄ„Åì„Å®„ÅåÂ•Ω„Åç„Åß„Åô„ÄÇ\"\n",
        "\n",
        "generated_text = generator(\n",
        "    japanese_text,\n",
        "    max_length=30,\n",
        "    num_return_sequences=1,\n",
        ")"
      ],
      "metadata": {
        "id": "3DKoh9LudIcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated text (in Japanese) is:"
      ],
      "metadata": {
        "id": "T1yrQ0r45f7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh8qbjKcpdkg",
        "outputId": "34f4cf19-a70e-467b-8a5d-188903995830"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': '„ÉÜ„Ç£„Éº„É≥„Ç®„Ç§„Ç∏„É£„Éº„ÅØÂÜ¨„Å´„Åì„Çå„Çâ„ÅÆ„Çπ„Éù„Éº„ÉÑ„ÇíÊ•Ω„Åó„ÇÄ„Åì„Å®„ÅåÂ•Ω„Åç„Åß„Åô„ÄÇ ÁßÅ„ÅØ„ÄÅÁßÅ„ÅÆÊÅØÂ≠ê„Åå„ÄÅÁßÅ„ÅÆÊØçË¶™„Åå„ÄÅ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Inference API\n",
        "\n",
        "All the models can be <font color='blue'>tested</font> directly through your <font color='blue'>browser</font> using the <font color='blue'>Inference API</font>, which is available on the Hugging Face [website](https://huggingface.co/). You can play with the model directly on this page by inputting custom text and watching the model process the input data.\n",
        "\n",
        "The Inference API that powers the widget is <font color='blue'>also available </font>as a <font color='blue'>paid product</font>, which comes in handy if you need it for your workflows. See the [pricing page](https://huggingface.co/pricing) for more details."
      ],
      "metadata": {
        "id": "aIycTXs7PyZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mask filling\n",
        "\n",
        "The next pipeline you'll try is <font color='blue'>fill-mask</font>. The idea of this task is to <font color='blue'>fill</font> in the <font color='blue'>blanks</font> in a given text:"
      ],
      "metadata": {
        "id": "CtQT59tPP_CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")\n",
        "unmasker(\"This course will teach you all about [MASK] models.\", top_k=2)"
      ],
      "metadata": {
        "id": "4ytkDHZxQFYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a58d05-c476-405d-b912-6ac7037eda1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.09235832095146179,\n",
              "  'token': 8045,\n",
              "  'token_str': 'mathematical',\n",
              "  'sequence': 'this course will teach you all about mathematical models.'},\n",
              " {'score': 0.027659853920340538,\n",
              "  'token': 2535,\n",
              "  'token_str': 'role',\n",
              "  'sequence': 'this course will teach you all about role models.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Search for the <font color='blue'>bert-base-cased</font> model on the <font color='blue'>Hub</font> and <font color='blue'>identify</font> its <font color='blue'>mask word</font> in the <font color='blue'>Inference API widget</font>. What does this model predict for the sentence in our pipeline example above?"
      ],
      "metadata": {
        "id": "CYTNgUejd8rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
        "unmasker(\"This course will teach you all about [MASK] models.\", top_k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6J3pAz6VX0T",
        "outputId": "ffc581b9-b620-41a3-ee6b-03b7a565ff2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2596302330493927,\n",
              "  'token': 1648,\n",
              "  'token_str': 'role',\n",
              "  'sequence': 'This course will teach you all about role models.'},\n",
              " {'score': 0.09427274763584137,\n",
              "  'token': 1103,\n",
              "  'token_str': 'the',\n",
              "  'sequence': 'This course will teach you all about the models.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>top_k argument</font> controls how many <font color='blue'>possibilities</font> you want to be displayed. Note that here the <font color='blue'>model</font> fills in the special [MASK} word, which is often referred to as a <font color='blue'>mask token</font>. <font color='blue'>Other mask-filling models</font> might have <font color='blue'>different mask tokens</font>, so it's always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget."
      ],
      "metadata": {
        "id": "wA_vY6biQK6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named entity recognition\n",
        "\n",
        "Named entity recognition (<font color='blue'>NER</font>) is a task where the model has to <font color='blue'>find</font> which <font color='blue'>parts</font> of the <font color='blue'>input text correspond</font> to entities such as <font color='blue'>persons</font>, <font color='blue'>locations</font>, or <font color='blue'>organizations</font>. Let's look at an example:"
      ],
      "metadata": {
        "id": "hnHpmSbGQXv2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ttYVjU17Gdp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3735a56-2d3b-49b8-ff2e-dea67f099ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9981694,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9796019,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9932106,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True, model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also examine the same example with the <font color='blue'>RoBERTa</font> model:"
      ],
      "metadata": {
        "id": "bWkeA4BP7ykV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True, model=\"xlm-roberta-large-finetuned-conll03-english\")\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVi2vhH8ZN5Q",
        "outputId": "708350d2-4794-48bb-c6e9-f403c8e39d2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9998966,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9998043,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.99908185,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the model <font color='blue'>correctly identified</font> that <font color='blue'>Sylvain</font> is a <font color='blue'>person</font> (PER), <font color='blue'>Hugging Face</font> an <font color='blue'>organization</font> (ORG), and <font color='blue'>Brooklyn</font> a <font color='blue'>location</font> (LOC).\n",
        "\n",
        "We pass the option <font color='blue'>grouped_entities=True</font> in the pipeline creation function to tell the pipeline to <font color='blue'>regroup together</font> the <font color='blue'>parts</font> of the sentence that correspond to the <font color='blue'>same entity</font>: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the <font color='blue'>preprocessing</font> even <font color='blue'>splits</font> some <font color='blue'>words</font> into <font color='blue'>smaller parts</font>. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces."
      ],
      "metadata": {
        "id": "XW15rGnsQgm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** <font color='blue'>Search</font> the <font color='blue'>Model Hub</font> for a model able to do <font color='blue'>part-of-speech tagging</font> (usually abbreviated as POS) in English. What does this model predict for the sentence in the example above?"
      ],
      "metadata": {
        "id": "G1NxmjM6efet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True, model=\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHoc-RNseerG",
        "outputId": "3339faa3-a9d2-4545-a020-bf80521c138f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PRP$',\n",
              "  'score': 0.99944586,\n",
              "  'word': 'My',\n",
              "  'start': 0,\n",
              "  'end': 2},\n",
              " {'entity_group': 'NN',\n",
              "  'score': 0.9995615,\n",
              "  'word': 'name',\n",
              "  'start': 3,\n",
              "  'end': 7},\n",
              " {'entity_group': 'VBZ',\n",
              "  'score': 0.9995523,\n",
              "  'word': 'is',\n",
              "  'start': 8,\n",
              "  'end': 10},\n",
              " {'entity_group': 'NNP',\n",
              "  'score': 0.9963742,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'CC',\n",
              "  'score': 0.9996537,\n",
              "  'word': 'and',\n",
              "  'start': 19,\n",
              "  'end': 22},\n",
              " {'entity_group': 'PRP',\n",
              "  'score': 0.99956113,\n",
              "  'word': 'I',\n",
              "  'start': 23,\n",
              "  'end': 24},\n",
              " {'entity_group': 'VBP',\n",
              "  'score': 0.9976095,\n",
              "  'word': 'work',\n",
              "  'start': 25,\n",
              "  'end': 29},\n",
              " {'entity_group': 'IN',\n",
              "  'score': 0.999801,\n",
              "  'word': 'at',\n",
              "  'start': 30,\n",
              "  'end': 32},\n",
              " {'entity_group': 'NNP',\n",
              "  'score': 0.96980053,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'IN',\n",
              "  'score': 0.99977857,\n",
              "  'word': 'in',\n",
              "  'start': 46,\n",
              "  'end': 48},\n",
              " {'entity_group': 'NNP',\n",
              "  'score': 0.99542207,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57},\n",
              " {'entity_group': '.',\n",
              "  'score': 0.9999286,\n",
              "  'word': '.',\n",
              "  'start': 57,\n",
              "  'end': 58}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question answering\n",
        "\n",
        "The <font color='blue'>question-answering</font> pipeline <font color='blue'>answers questions</font> using information from a given context.\n",
        "\n",
        "Let's firs try this with <font color='blue'>DistilBERT</font>:"
      ],
      "metadata": {
        "id": "4_bIA2-lQnHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VDA1D_DvGdp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2360a19c-7da4-4175-8777-342ba4c332b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.6949753165245056, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then with <font color='blue'>BERT large model (uncased)</font>:"
      ],
      "metadata": {
        "id": "0nRm-htC9MKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_hPtDxraCVY",
        "outputId": "4246b440-d190-4ac6-900f-de2e16a534c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.5150135159492493,\n",
              " 'start': 33,\n",
              " 'end': 57,\n",
              " 'answer': 'Hugging Face in Brooklyn'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this pipeline works by <font color='blue'>extracting information</font> from the <font color='blue'>provided context</font>; it does not generate the answer."
      ],
      "metadata": {
        "id": "VYw_ZghoQrPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "\n",
        "<font color='blue'>Summarization</font> is the task of <font color='blue'>reducing</font> a <font color='blue'>text</font> into a shorter text <font color='blue'>while keeping</font> all (or <font color='blue'>most</font>) of the <font color='blue'>important aspects</font> referenced in the text. Here's an example:"
      ],
      "metadata": {
        "id": "PikxDEXkQwER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HlZu9DBIGdp1",
        "outputId": "537518f3-4380-46b1-c8c1-e4be995de5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as India and Europe continue to encourage and advance the teaching of engineering .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", max_length=60)\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with <font color='blue'>text generation</font>, you can specify a <font color='blue'>max_length</font> or a <font color='blue'>min_length</font> for the <font color='blue'>esult</font>."
      ],
      "metadata": {
        "id": "_2myvIG4Q6Fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation\n",
        "\n",
        "For translation, you can use a <font color='blue'>default model</font> if you <font color='blue'>provide</font> a <font color='blue'>language pair</font> in the <font color='blue'>task name</font> (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the [Model Hub](https://huggingface.co/models). Here we'll try translating from <font color='blue'>French to English</font>:"
      ],
      "metadata": {
        "id": "aWG9MHrmQ85k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "49LifT-6Gdp3",
        "outputId": "86004795-9384-4806-b7e1-6fe39dda8abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with <font color='blue'>text generation</font> and <font color='blue'>summarization</font>, you can specify a <font color='blue'>max_length</font> or a <font color='blue'>min_length</font> for the <font color='blue'>result</font>.\n",
        "\n",
        "The <font color='blue'>pipelines</font> shown so far are mostly for <font color='blue'>demonstrative purposes</font>. They were programmed for <font color='blue'>specific tasks</font> and <font color='blue'>cannot perform variations</font> of them. In the next chapter, you'll learn what's inside a `pipeline()` function and how to customize its behavior."
      ],
      "metadata": {
        "id": "VwPIw9zTRG5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è **Try it out!** Search for translation models in other languages and try to translate the previous sentence into a few different languages."
      ],
      "metadata": {
        "id": "nJ6RKZstfT_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll try translating from <font color='blue'>French to Spanish</font>:"
      ],
      "metadata": {
        "id": "qKlWp0bp-3n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# French to Spanish\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-es\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "id": "3S8uXpWcRIky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e514584-c666-4a83-c420-dc53b2eaf749"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'Este curso es producido por Hugging Face.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then from <font color='blue'>French to German</font>:"
      ],
      "metadata": {
        "id": "cHHJe22x--1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# French to German\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-de\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_uL5BmHg8Uv",
        "outputId": "a77f6d5b-8bbd-4201-df20-ec23e4298655"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'Dieser Kurs wird von Hugging Face erstellt.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "255e6020b54e4bfb875363444cc6b731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c59dc85322844769590720972f98b7e",
              "IPY_MODEL_a7a448eece6d445596d61bb7a1e91fe3",
              "IPY_MODEL_7cc8a18396a1454bad22d3442b7e1980",
              "IPY_MODEL_bfd869dd5d9f4b2c89e45ff23fe09dfc"
            ],
            "layout": "IPY_MODEL_27bcaca28b9f4bb2a9f7311bf3618727"
          }
        },
        "f0488974d0f24a68bb4706804aa1ad51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365599dd3f8e4bdabdcdb26ed1cf6e61",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9b5933f8b80841c8897e1feaa49ee9bf",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a338e4ba4dd24b4d906a6ed1add8f32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_188eff481b7e41158ffc561f3bd23bf2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_74ae48c193e746c989887f1997c7a105",
            "value": ""
          }
        },
        "8325c5becf1e40eda64cbe26421309ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_fae2692fadb548d68a037d06a225168f",
            "style": "IPY_MODEL_a9e2a45e916e417e9207684f2222b535",
            "value": true
          }
        },
        "7ca9241475e040898e7a5aaaaba1a590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5eab90bc386f4795b243ca8c922cf5be",
            "style": "IPY_MODEL_77b75a9e95c24e0f9bc75b24029d82f2",
            "tooltip": ""
          }
        },
        "5b3449f2f26043fb85fc808ab2c64283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1740a880c24898ac64dea6d1421bdd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bb4468a4b77746a28f1dd207b85e6023",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "27bcaca28b9f4bb2a9f7311bf3618727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "365599dd3f8e4bdabdcdb26ed1cf6e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5933f8b80841c8897e1feaa49ee9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "188eff481b7e41158ffc561f3bd23bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74ae48c193e746c989887f1997c7a105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fae2692fadb548d68a037d06a225168f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9e2a45e916e417e9207684f2222b535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eab90bc386f4795b243ca8c922cf5be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b75a9e95c24e0f9bc75b24029d82f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "dc1740a880c24898ac64dea6d1421bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4468a4b77746a28f1dd207b85e6023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c7d1604c9646efbea78e93d4cb179f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf9790904274b84ac0ab7219568c0aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d03020caff9a4664a25a8f5badf5c595",
            "value": "Connecting..."
          }
        },
        "1bf9790904274b84ac0ab7219568c0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03020caff9a4664a25a8f5badf5c595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c59dc85322844769590720972f98b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edab985f152143b896c1760ed19aa500",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a14f75c5377c4e5094b168dd6e1c17a2",
            "value": "Token is valid (permission: write)."
          }
        },
        "a7a448eece6d445596d61bb7a1e91fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ac98650eac473c89d910deab19658d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2e7af7b23d794895800b1cd5e7eb19c6",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "7cc8a18396a1454bad22d3442b7e1980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b898e7ecfdc4dedb31636f697836850",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_315beb8a2f8d438884636747eec2e5ae",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "bfd869dd5d9f4b2c89e45ff23fe09dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1bbca3ff15947a5918f499d7b285131",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b8461d61ad5424c9b33262da9b6bb2e",
            "value": "Login successful"
          }
        },
        "edab985f152143b896c1760ed19aa500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14f75c5377c4e5094b168dd6e1c17a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ac98650eac473c89d910deab19658d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e7af7b23d794895800b1cd5e7eb19c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b898e7ecfdc4dedb31636f697836850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315beb8a2f8d438884636747eec2e5ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1bbca3ff15947a5918f499d7b285131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8461d61ad5424c9b33262da9b6bb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}