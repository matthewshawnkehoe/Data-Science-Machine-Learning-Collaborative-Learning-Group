{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCz0cvWF_wbA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain openai\n",
        "!pip install -q -U faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnISdj8lMqKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f39cb03-a097-4567-ff0c-5a8f11dc0b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open AI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiCAlO7L-VlD"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Blog Post: [Retrieval Augmented Generation (RAG) Using LangChain](https://deci.ai/blog/retrieval-augmented-generation-using-langchain/)\n",
        "\n",
        "Large Language Models (LLMs) are powerful tools for generating human-like text, but they have limitations.\n",
        "\n",
        "Retrieval Augmented Generation (RAG) addresses these challenges, enhancing LLMs by integrating retrieval mechanisms. This approach ensures that the content LLMs produce is both contextually relevant and factually accurate. RAG acts as a bridge, connecting LLMs to vast knowledge sources. As AI becomes increasingly used for diverse tasks, the accuracy and relevance of the generated information are crucial.\n",
        "\n",
        "RAG meets this demand, making AI interactions more informative and context-aware.\n",
        "\n",
        "# What You Need for RAG Implementation\n",
        "\n",
        "Before building out a RAG system, it's essential to familiarize yourself with the tools that make this process possible.\n",
        "\n",
        "Each tool plays a specific role, ensuring that the RAG system operates efficiently and effectively.\n",
        "\n",
        "**LLM**: At the heart of the system is the LLM, the core AI model responsible for generating human-like text responses.\n",
        "\n",
        "**Vector Store**: This is where the magic happens. The Vector Store is a dedicated storage system that houses embeddings and their corresponding textual data, ensuring quick and efficient retrieval.\n",
        "\n",
        "**Vector Store Retriever**: Think of this as the search engine of the system. The Vector Store Retriever fetches relevant documents by comparing vector similarities, ensuring that the most pertinent information is always at hand.\n",
        "\n",
        "**Embedder**: Before storing or retrieving data, we need to convert textual information into a format the system can understand. The Embedder takes on this role, transforming text into vector representations.\n",
        "\n",
        "**Prompt**: Every interaction starts with a user's query or statement. The Prompt captures this initial input, setting the stage for the retrieval and generation processes.\n",
        "\n",
        "**Document Loader**: With vast amounts of data to process, the Document Loader is essential. It imports and reads documents, preparing them for chunking and embedding.\n",
        "\n",
        "**Document Chunker**: To make the data more manageable and efficient for retrieval, the Document Chunker breaks documents into smaller, more digestible pieces.\n",
        "\n",
        "**User Input**: Last but not least, the User Input tool captures the query or statement provided by the end-user, initiating the entire RAG process.\n",
        "\n",
        "\n",
        "# The RAG System and Its Subsystems\n",
        "\n",
        "The primary goal of RAG is to provide LLMs with contextually relevant and factually accurate information, ensuring that the generated content meets the highest standards of quality and relevance.\n",
        "\n",
        "To achieve this, the RAG system is divided into subsystems, each playing a crucial role in the overall process. The tools integral to the RAG system are not standalone entities; they interweave to form the subsystems that drive the RAG process.\n",
        "\n",
        "Each tool fits within one of the following subsystems:\n",
        "\n",
        "1) Index\n",
        "\n",
        "2) Retrieval\n",
        "\n",
        "3) Augment\n",
        "\n",
        "These work together as an orchestrated flow that transforms a user's query into a contextually rich and accurate response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9o1pzeRkXUT"
      },
      "source": [
        "# Index System\n",
        "\n",
        "**Purpose:** This subsystem is responsible for preparing and organizing the data for efficient retrieval.\n",
        "\n",
        "Here are the steps of the Index system\n",
        "\n",
        "**1) Load Documents (Document Loader)**: Imports and reads the vast amounts of data that the system will use.\n",
        "\n",
        "**2) Chunk Documents (Document Chunker):** Breaks down the loaded documents into smaller, more manageable pieces to facilitate efficient retrieval.\n",
        "\n",
        "**3) Embed Documents (Embedder):** Converts these textual chunks into vector representations, making them searchable within the system.\n",
        "\n",
        "**4) Store Embeddings (Vector Store):** Safely stores the generated embeddings alongside their textual counterparts for future retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK3gf7TDH9GS"
      },
      "source": [
        "### Load documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmKgqy5X_vhA"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "yolo_nas_loader = WebBaseLoader(\"https://deci.ai/blog/yolo-nas-object-detection-foundation-model/\").load()\n",
        "\n",
        "decicoder_loader = WebBaseLoader(\"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\").load()\n",
        "\n",
        "yolo_newsletter_loader = WebBaseLoader(\"https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1shOpId_vqb"
      },
      "source": [
        "### Chunk documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuXm06J1IDs7"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "yolo_nas_chunks = text_splitter.transform_documents(yolo_nas_loader)\n",
        "\n",
        "decicoder_chunks = text_splitter.transform_documents(decicoder_loader)\n",
        "\n",
        "yolo_newsletter_chunks = text_splitter.transform_documents(yolo_newsletter_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGzRpURwJqHM"
      },
      "source": [
        "### Create an index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGtfN_hiJzVB"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cachce/\")\n",
        "\n",
        "# create an embedder\n",
        "core_embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model,\n",
        "    store,\n",
        "    namespace = core_embeddings_model.model\n",
        ")\n",
        "\n",
        "# store embeddings in vector store\n",
        "vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)\n",
        "\n",
        "vectorstore.add_documents(decicoder_chunks)\n",
        "\n",
        "vectorstore.add_documents(yolo_newsletter_chunks)\n",
        "\n",
        "# instantiate a retriever\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4HNkh6pmQ61"
      },
      "source": [
        "# Retrieval System\n",
        "\n",
        "**Purpose:** As the name suggests, this subsystem fetches the most relevant information based on the user's query.\n",
        "\n",
        "Here are the steps in the Retrieval system\n",
        "\n",
        "**1) Obtain User Query (User Input):** Captures the user's question or statement.\n",
        "\n",
        "**2) Embed User Query (Embedder):** Transforms the user's query into a vector format, similar to the indexed documents.\n",
        "\n",
        "**3) Vector Search (Vector Store Retriever):** Searches the Vector Store for documents with embeddings that closely match the embedded user query.\n",
        "\n",
        "**4) Return Relevant Documents:** The system then returns the top matches, ensuring that the most pertinent information is always provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq3nKNJ2mRDK"
      },
      "outputs": [],
      "source": [
        "from langchain.llms.openai import OpenAIChat\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9FGhpMWORwa",
        "outputId": "a020b3ca-68fb-4fc2-a8aa-5ce4c881f2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:787: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAIChat()\n",
        "handler =  StdOutCallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBhaA_nkpU_2"
      },
      "outputs": [],
      "source": [
        "# this is the entire retrieval system\n",
        "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    callbacks=[handler],\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAaIa-kpsHHI"
      },
      "source": [
        "# Augment System\n",
        "\n",
        "**Purpose:** This subsystem enhances the LLM's input prompt with the retrieved context, ensuring that the model has all the necessary information to generate a comprehensive response.\n",
        "\n",
        "**1) Create Initial Prompt (Prompt):** Starts with the original user query or statement.\n",
        "\n",
        "**2) Augment Prompt with Retrieved Context:** Merges the initial prompt with the context retrieved from the Vector Store, creating an enriched input for the LLM.\n",
        "\n",
        "**3) Send Augmented Prompt to LLM:** The enhanced prompt is then fed to the LLM.\n",
        "\n",
        "**4) Receive LLM's Response:** After processing the augmented prompt, the LLM generates and returns its response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-NO6i0YS6nY",
        "outputId": "1471855a-4155-4e86-ace9-70a9c1a7596c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# This is the entire augment system!\n",
        "response = qa_with_sources_chain({\"query\":\"What does Neural Architecture Search have to do with how Deci creates its models?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vTXxxh0xBl0"
      },
      "source": [
        "Look at the entire response  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGBkR1ewxHWY",
        "outputId": "780c8083-3f53-4fd7-adb7-70280d8bafae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What does Neural Architecture Search have to do with how Deci creates its models?',\n",
              " 'result': 'Deci uses Neural Architecture Search (NAS) technology, specifically AutoNAC, to create efficient and effective neural network architectures for its models. AutoNAC intelligently searches a large space of possible architectures and zeroes in on the most promising ones. This technology allows Deci to automate the development of superior neural networks and optimize the accuracy and speed of its models.',\n",
              " 'source_documents': [Document(page_content='Neural Architecture Search is define the architecture search space. For YOLO-NAS, our researchers took inspiration from the basic blocks of YOLOv6 and YOLOv8. With the architecture and training regime in place, our researchers harnessed the power of AutoNAC. It intelligently searched a vast space of ~10^14 possible architectures, ultimately zeroing in on three final networks that promised outstanding results. The result is a family of architectures with a novel quantization-friendly basic', metadata={'source': 'https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas', 'title': 'Unleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision', 'description': 'The Future of Computer Vision is Here', 'language': 'en'}),\n",
              "  Document(page_content='Deci’s suite of Large Language Models and text-to-Image models, with DeciCoder leading the charge, is spearheading the movement to address this gap.DeciCoder’s efficiency is evident when compared to other top-tier models. Owing to its innovative architecture, DeciCoder surpasses models like SantaCoder in both accuracy and speed. The innovative elements of DeciCoder’s architecture were generated using Deci’s proprietary Neural Architecture Search technology, AutoNAC™.\\xa0\\nAnother Win for AutoNAC', metadata={'source': \"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\", 'title': 'Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation', 'description': 'Today, we introduce DeciCoder, our 1B-parameter open-source Large Language Model for code generation, equipped with a 2048-context window.', 'language': 'en-US'}),\n",
              "  Document(page_content='The quest for the “optimal” neural network architecture has historically been a labor-intensive manual exploration. While this manual approach often yields results, it is highly time consuming and often falls short in pinpointing the most efficient neural networks. The AI community recognized the promise of Neural Architecture Search (NAS) as a potential game-changer, automating the development of superior neural networks. However, the computational demands of traditional NAS methods limited', metadata={'source': \"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\", 'title': 'Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation', 'description': 'Today, we introduce DeciCoder, our 1B-parameter open-source Large Language Model for code generation, equipped with a 2048-context window.', 'language': 'en-US'}),\n",
              "  Document(page_content='This new model is fast and accurate, offering the best accuracy-latency tradeoff among existing object detection models on the market. This accomplishment was made possible by Deci’s AutoNAC neural architecture search technology, which efficiently constructs deep learning models for any task and hardware.', metadata={'source': 'https://deci.ai/blog/yolo-nas-object-detection-foundation-model/', 'title': 'YOLO-NAS by Deci Achieves State-of-the-Art Performance on Object Detection Using Neural Architecture Search', 'description': 'The new YOLO-NAS architecture sets a new frontier for object detection tasks, offering the best accuracy and latency tradeoff performance.', 'language': 'en-US'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want just the response"
      ],
      "metadata": {
        "id": "2kYiibKxzENe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1JOCbQZzC0R",
        "outputId": "3af839e0-4c74-4e8d-dc43-d78e61cfc034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deci utilizes Neural Architecture Search (NAS) technology, specifically their proprietary AutoNAC technology, to automatically generate and optimize the architecture of their models. Neural Architecture Search helps Deci in efficiently constructing deep learning models for various tasks and hardware.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can get the source like so:"
      ],
      "metadata": {
        "id": "agbQB4A_zZrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV8rFTyST_nD",
        "outputId": "af675b72-ef5e-44ff-b051-547a8626468e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Neural Architecture Search is define the architecture search space. For YOLO-NAS, our researchers took inspiration from the basic blocks of YOLOv6 and YOLOv8. With the architecture and training regime in place, our researchers harnessed the power of AutoNAC. It intelligently searched a vast space of ~10^14 possible architectures, ultimately zeroing in on three final networks that promised outstanding results. The result is a family of architectures with a novel quantization-friendly basic', metadata={'source': 'https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas', 'title': 'Unleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision', 'description': 'The Future of Computer Vision is Here', 'language': 'en'}), Document(page_content='Deci’s suite of Large Language Models and text-to-Image models, with DeciCoder leading the charge, is spearheading the movement to address this gap.DeciCoder’s efficiency is evident when compared to other top-tier models. Owing to its innovative architecture, DeciCoder surpasses models like SantaCoder in both accuracy and speed. The innovative elements of DeciCoder’s architecture were generated using Deci’s proprietary Neural Architecture Search technology, AutoNAC™.\\xa0\\nAnother Win for AutoNAC', metadata={'source': \"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\", 'title': 'Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation', 'description': 'Today, we introduce DeciCoder, our 1B-parameter open-source Large Language Model for code generation, equipped with a 2048-context window.', 'language': 'en-US'}), Document(page_content='The quest for the “optimal” neural network architecture has historically been a labor-intensive manual exploration. While this manual approach often yields results, it is highly time consuming and often falls short in pinpointing the most efficient neural networks. The AI community recognized the promise of Neural Architecture Search (NAS) as a potential game-changer, automating the development of superior neural networks. However, the computational demands of traditional NAS methods limited', metadata={'source': \"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\", 'title': 'Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation', 'description': 'Today, we introduce DeciCoder, our 1B-parameter open-source Large Language Model for code generation, equipped with a 2048-context window.', 'language': 'en-US'}), Document(page_content='This new model is fast and accurate, offering the best accuracy-latency tradeoff among existing object detection models on the market. This accomplishment was made possible by Deci’s AutoNAC neural architecture search technology, which efficiently constructs deep learning models for any task and hardware.', metadata={'source': 'https://deci.ai/blog/yolo-nas-object-detection-foundation-model/', 'title': 'YOLO-NAS by Deci Achieves State-of-the-Art Performance on Object Detection Using Neural Architecture Search', 'description': 'The new YOLO-NAS architecture sets a new frontier for object detection tasks, offering the best accuracy and latency tradeoff performance.', 'language': 'en-US'})]\n"
          ]
        }
      ],
      "source": [
        "print(response['source_documents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-3BqcQJTBu2",
        "outputId": "203856f7-7897-48cc-ad6c-e4330c24b736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = qa_with_sources_chain({\"query\":\"What is DeciCoder\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbsKa0nKu-XP",
        "outputId": "1bddf350-2c2f-4956-bec5-c52f6d62b902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeciCoder is a 1B-parameter open-source Large Language Model (LLM) for code generation. It has a 2048-context window, permissively licensed, delivers a 3.5x increase in throughput, improved accuracy on the HumanEval benchmark, and smaller memory usage compared to widely-used code generation LLMs such as SantaCoder.\n"
          ]
        }
      ],
      "source": [
        "print(response['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RwqP62Uu9Tc",
        "outputId": "2923f8be-c722-4556-a750-cf6be2c837d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = qa_with_sources_chain({\"query\":\"Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qivMMWHMvFqe",
        "outputId": "473a160b-a0c7-4531-b1c5-40b89c611627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deci, a company focused on pushing the boundaries of accuracy and efficiency, has introduced a new architecture called YOLO-NAS. YOLO-NAS is a benchmark for object detection that has the potential to drive innovation and unlock new possibilities across various industries and research domains.\n",
            "\n",
            "Deci has showcased its robust capabilities with the DeciCoder model, which consistently outperforms models like SantaCoder. By leveraging AutoNAC, Deci was able to generate an architecture that is both efficient and powerful.\n",
            "\n",
            "Deci's use of NAS (Neural Architecture Search) played a pivotal role in the development of YOLO-NAS. NAS is a technique that automates the design process of neural networks, allowing for the discovery of optimized architectures. By deploying NAS, Deci was able to achieve state-of-the-art performance on object detection with YOLO-NAS.\n",
            "\n",
            "The integration of NAS in the development of YOLO-NAS and DeciCoder showcases Deci's commitment to pushing the boundaries of AI innovation. With the YOLO-NAS architecture and DeciCoder, Deci aims to provide advanced solutions for various use cases, such as running on edge devices, optimizing generative AI models, reducing cloud costs, shortening development time, and maximizing data center utilization.\n",
            "\n",
            "Deci's focus on accuracy, efficiency, and innovation through the use of NAS sets them apart in the industry. Their dedication to driving progress in object detection and AI research makes them a valuable player in the field.\n"
          ]
        }
      ],
      "source": [
        "print(response['result'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}